<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hive编程概述]]></title>
    <url>%2F2019%2F01%2F22%2FHive%E7%BC%96%E7%A8%8B%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1 基础知识 1.1 概述 Hadoop实现了一个特别的计算模型，也就是MapReduce，可以将计算任务分割成多个处理单元然后分散到一群普通的机器上，从而降低成本并提供水平可伸缩性。计算模型的下面是称为Hadoop分布式文件系统(HDFS)的分布式文件系统。 有一个挑战就是用户如何从一个现有的数据基础架构转移到Hadoop上，而这个基础架构是基于传统RDBMS和结构化查询语句(SQL)的。这是Hive出现的原因，Hive提供了一个称为Hive查询语言(HiveQL/HQL)的SQL方言，来查询存储在Hadoop集群中的数据。 对Java工程师而言，将常见的数据运算对应到底层的MapReduce JAVA API也是非常麻烦的。Hive可以将大多数的查询转化成MapReduce任务(job)。 Hive最适合用于数据仓库应用程序，来进行相关的静态数据分析，然后形成意见和报告。不需要快速响应给出结果，而且数据本身不会频繁变化。Hadoop及HDFS的设计本身约束和局限性限制了Hive的功能，最大的限制就是不支持记录级别的更新、删除和插入操作，只能通过查询生成新表或者将查询结果导入到文件中。因MapReduce启动耗时，Hive查询耗时严重。Hive也不支持事务。 Hive不支持OLTP所需的关键功能，更接近OLAP工具，因查询耗时，其实也不怎么满足OLAP中的”联机“含义。如果用户需要对大规模数据使用OLTP功能，可以选择一些NoSQL数据库，如HBase、Cassandra等。用户也可以和这些数据库结合来使用Hive。 HIVEQL和MySQL的语言最接近。 1.2 MapReduce介绍 Google的两篇MapReduce和GFS论文启发了Doug Cutting开发了Hadoop。 举例WordCount，是MapReduce框架的”hello world”程序。比如10个大文档，每个文档有多行，每行是一句句英语。 10个大文档被拆分成20个文档，20个文档分别触发一个Mapper进程。Mapper进程启动后，会频繁调用来处理文件中的每行文本，传入的键是这行的起始位置(比如1行2行)，值是这行对应的文本。Mapper进程会对传入的值按空格分割成一个个单词，然后每个单词输出一个键值对。键是该单词，值是个数1。 然后按照键值对进行排序sort，然后重新洗牌shuffle，将相同键的键值对分到一个reduce进程中，然后对值进行求和。 优化：如果Mapper进程只是对每个单词计数1，Sort和Shuffle过程会产生网络和磁盘I/O浪费。可以在Mapper进程跟踪单词频数，输出的键值对可以是“单词”-“在这个Mapper进程中出现的次数和”。 MapReduce框架的输入输出数据结构都是键值对。map操作将输入的一个键值对变成0到多个键值对，且输入和输出的键必须不同，值不要求。reduce操作将输入的某个键的所有键值对转换成一个键值对。 Hadoop将提交的job分解成多个独立的map和reduce任务(task)时，一般task所处的位置和要处理的数据的位置一样，减少网络开销。Hadoop会监控每个task，并重启失败的task。 Hadoop每个数据块(block)会复制多份，默认是3份，大小通常是64M的倍数。这么大的数据块可以在硬盘上连续存储，减少磁盘寻址次数。 1.3 Hive等其他高级工具 1.3.1 Hive 和Hive的交互方式，有CLI(命令行界面)和图形用户界面(如Cloudera开源项目Hue)两种。 Hive发行版中附带的模块有CLI、HIVE网络界面(HWI)以及可通过JDBC、ODBC和一个Thrift服务器进行编程访问的几个模块。 所有的命令和查询都会进入到Driver（驱动模块），该模块对输入进行解析编译优化，然后按步骤执行（通常是启动多个MapReduce任务job来执行）。当启动MapReduce任务时，Hive本身不会生成Java MapReduce程序，而是通过一个xml文件驱动执行内置的、原生的Mapper和Reducer模块。 Hive通过和JobTracker通信来初始化MapReduce任务(job)，不需要部署在JobTracker所在的管理节点上执行。大型集群中，通常会有网关机专门部署像Hive这样的工具，可远程和JobTracker通信。 通常数据文件是在HDFS中的，而HDFS是由NameNode进行管理。 Metastore（元数据存储）是一个独立的关系数据库，Hive会在其中保存表模式和其他系统元数据。默认情况下，Hive使用内置的Derby SQL服务器，提供有限的单进程的存储服务，启动Hive会话时，Derby会在当前工作目录下创建metastore_db的目录。集群一般使用MySQL数据库。 1.3.2 Pig 由Yahoo开发，是一种数据流语言，而不是查询语言，其所定制的语言不是基于SQL的。步进式的数据流可以比一组复杂的查询更加直观，常用于ETL(抽取、转换和装载)过程的一部分，也就是将外部数据装载到Hadoop集群中，并转换成所期望的数据格式。 Hadoop团队通常会将Hive和Pig结合使用，对于特定工作选择合适的工具。 1.3.3 HBase HBase是一个分布式的、可伸缩的数据存储，支持行级别的数据更新、快速查询和行级事务（不支持多行事务）。 设计灵感来自于Google的BigTable，HBase支持的一个重要特性就是列存储，其中的列可以组成列族。列族在分布式集群中物理上是存储在一起，当查询场景设计的列是所有列的一个子集时，读写速度会快很多。因为不需要读取所有的行然后丢弃大部分的列，而是只读取所需要的列。 可以像键-值存储一样来使用HBase，每一行都提供了一个唯一键来提供非常快的速度读写这一行的列或者列族。HBase还对每个列保留多个版本的值（按照时间戳进行标记），版本数量可配置。 HBase使用HDFS（或其他某种分布式文件系统）来持久化存储数据，使用了内存缓存技术对数据和本地文件进行追加数据更新操作日志。持久化文件将定期使用日志进行更新等操作。 HBase没有提供类似SQL的查询语言，但是Hive现在已经可以和HBase结合使用了。 1.3.4 Cascading、Crunch及其他 Apache Hadoop生态系统之外还有几个高级语言，当Hive本身没有提供额外的功能时，需要使用JAVA编码来拓展Hive功能，如Cascading、Crunch及其他。 对于一些事件流处理的工作，Spark、Storm、Kfaka等的结合能够取得很好的效果。 2 基础操作 2.1 安装步骤 2.1.1 虚拟机安装 2.1.2 JAVA和HADOOP安装 配置JAVA_HOME和PATH，一种方法是在/etc/profile文件末尾追加；另一种方法是在/etc/profile.d目录下新建bash文件如java.sh,在文件中添加这两项。如果用户不希望影响到系统的其他用户，可以将JAVA_HOME和PATH环境变量的定义加入到用户的$HOME/.bashrc文件中。 配置HADOOP_HOME和PATH，其他同上 2.1.3 Hadoop运行模式 本地模式使用的是本地文件系统，执行Hadoop job时（包含大多数的Hive查询），Map task和Reduce task在同一个进程执行。 分布式模式真实的集群是这种模式，没有完整URL指定的路径都是HDFS的路径，由JobTracker服务来管理job，不同的task在不同的进程执行。即使是在分布式模式下执行，Hive也可以在提交查询前判断是否可以用本地模式来执行查询，当数据量不大时，本地模式查询更快。可以在$HOME/.hiverc文件中添加命令set hive.exec.mode.local.auto=true，来使用这个功能。 伪分布式模式这种模式下的行为和分布式模式一致，引用的文件系统是分布式文件系统，由JobTracker服务来管理job，但只有一台物理机。行为类似于一个节点的集群，比如文件块冗余数限制为1个备份。 2.1.4 测试WordCount例子新建输入文件夹wc_in，下面放几个文本，如 echo “a big apple” &gt; wc_in/a.txt、echo “big apple pen” &gt; wc_in/b.txt。然后执行命令，hadoop jar $HADOOP_HOME/hadoop-0.20.2-examples.jar wordcount wc_in wc_out。输入输出为目录(wc_in,wc_out)，因为一般目录下有很多文件。 2.2 Hive的内部 lib目录下的jar包文件，每个jar文件都实现了Hive功能的某个部分；bin目录下是各种Hive服务可执行文件，包括CLI等；conf目录存放配置文件，hive-default.xml.template包含了Hive提供的配置属性及默认值，一般用户在hive-site.xml修改属性(没有则新建)。 2.3 配置Hadoop环境 2.3.1 本地模式配置 Hive表数据存储位置，修改hive-site.xml文件，配置property属性，hive.metastore.warehouse.dir property属性，hive.metastore.local默认为true，然后使用JDBC直接和一个关系数据库进行通信；为false的话，将通过metastore server来进行通信。 property属性，javax.jdo.option.ConnectionURL，告诉Hive如何连接metastore server，属性值中的databaseName是Derby的metastore_db目录的路径，通过这样设置可以解决每开启一个Hive会话Hive自动删除工作目录下的metastore_db。 2.3.2 分布式模式和伪分布式模式配置 job由JobTracker管理；每个工作节点执行的job task由TaskTracker管理；HDFS由NameNode管理；每个节点上的数据块由DataNode管理。 为方便用户定义自己的数据仓库目录，执行这个语句，set hive.meatstore.warehouse.dir=/user/myname/hive/warehouse。一种是每次启动Hive CLI或在每个Hive脚本前加上这一句；另一种是将这种命令放在$HOME/.hiverc文件中，每一次启动Hive都会执行这个文件。 2.3.3 使用JDBC连接元数据 集群中，Hive的元数据存储一般使用MySQL。 hive-site.xml中，javax.jdo.option.ConnectionURL设置值为jdbc:mysql://machine1/hive.db?createDatabaseIfNotExist=true，其中machine1为服务器名，hive_db为自定义数据库名。 hive-site.xml中，javax.jdo.option.DriverName设置值为com.mysql.jdbc.Driver。 需要将JDBC驱动jar包放在$HIVE_HOME/lib目录下。 2.3.4 Hive命令 hive –service name即可启动相应的服务 cli命令行界面，语句hive即可启动，hive默认为hive –service cli hiveserver监听来自其他进程的Thrift连接的一个守护进程。 hwi可以执行查询语句和其他命令的简单web界面。 jar可以执行需要Hive环境的应用。 –auxpath选项允许用户指定一个以冒号分割的jar包，jar包中包含有用户需要的自定义拓展。 –config，可以覆盖$HIVE-HOME/conf默认的属性配置，指向一个新的配置文件目录。 2.4 命令行界面 查询CLI选项列表 1hive --help --service cli hive使用过程中，有好几种命名空间。hivevar，用户自定义变量；hiveconf，hive相关配置属性；system，java相关配置属性；env，shell（如bash）环境定义的环境变量。 CLI中，set可以显示和修改变量 12345678#等价于 hive --hivevar foo=barhive --define foo=bar#显示变量hive&gt; set hivevar:foo#修改变量hive&gt; set hivevar:foo=bar2# 设置hive属性hive --hiveconf hive.cli.print.current.db=true Hive查询 12345678910111213141516# 查询结果保存，自动退出CLI。-S是静默模式，省略一些不必要的输出，输出是在本地文件系统，不是HDFShive -S -e "select * from table1 limit 5" &gt; /tmp/temp1# 模糊查找某个属性名hive -S -e "set" | grep warehouse# 执行脚本，一般后缀名为.q或.hqlhive -f /path/1.hql# CLI界面执行脚本hive&gt; source /path/1.hql# hive中执行shell命令，命令前加！，以分号结尾hive&gt; ! /bin/echo $JAVA_HOME;hive&gt; ! pwd;# hive中使用hadoop dfs命令，去掉hadoop，以分号结尾# 实际上比在bash中执行hadoop dfs···更高效，因为hadoop dfs每次都会启动一个JVM实例，而hive会在同一个进程执行命令。hive&gt; dfs -ls /# 脚本中，以--表示注释# Hive会将最近的10000行命令记录保存在$HOME/.hivehistory中。 3 数据类型和文件格式 3.1 基本数据类型 常用的数据类型有TINYINT、SMALLINT、INT、BIGINT、BOOLEAN、FLOAT、DOUBLE、STRING、TIMESTAMP和BINARAY(字节数组)等。所有的数据类型都是对JAVA接口的实现，行为细节和JAVA对应的类型是一致的。 很多数据库中，其SQL方言提供限制最大长度的字符串数组，Hive不支持这种。关系型数据库中，出于性能优化的考虑，定长的数组更容易建立索引、数据扫描等。Hadoop和Hive强调优化磁盘的读和写的性能，而限制列的值的长度相对来说没那么重要。 Hive在查询比较不同数据类型的列时，会有隐式转换。如float和double比较，会先将float转化double类型。 3.2 集合数据类型 Hive的列支持三种集合类型，分别是struct、map和array。 struct是指对象，可以包含好几张元素，表示不同性质；array是指数组，同种元素；map是指键值对。 。大多数关系数据库并不支持这些集合数据类型，因为会破坏标准格式。破坏标准格式会增大数据冗余的风险，消耗不必要的磁盘空间，还有可能造成数据不一致，数据改变时冗余的拷贝数据可能没有及时更新。好处就是可以增大吞吐量，当数据量级是T或者P时，以最少的“头部寻址”扫描数据是非常有必要的。按数据集进行封装的话，可以通过减少寻址次数来提高查询速度。如果根据外键关系关联的话，需要进行磁盘间的寻址操作，有非常高的性能损耗。 Hive中没有键的概念，但是用户可以对表建立索引。12345678# suboardinates指有哪些下属；duductions指薪酬有哪些扣费项目及扣费多少；address指街道地址、城市地址和州地址等create table employees(name string,salary float,suboardinates array&lt;STRING&gt;,duductions MAP&lt;STRING,FLOAT&gt;,address STRUCT&lt;street:STRING,city:STRING,state:STRING&gt;); 3.3 文本文件数据编码指定数据存储时用得编码 分隔符 \n ,用于换行 分隔符 ^A ,用于分隔字段，八进制编码\001表示 分隔符 ^B ,用于分隔struct和array的元素，也可以分隔map键值对，八进制编码\002表示 分隔符 ^C ,用于分隔map键值对，八进制编码\003表示12345678910111213141516# suboardinates指有哪些下属；duductions指薪酬有哪些扣费项目及扣费多少；address指街道地址、城市地址和州地址等# 最后两句不需要row format delimited，默认情况下是textline格式# 分隔符 \n ，Hive只支持这种，其他的可以自定义。create table employees(name string,salary float,suboardinates array&lt;STRING&gt;,duductions MAP&lt;STRING,FLOAT&gt;,address STRUCT&lt;street:STRING,city:STRING,state:STRING&gt;);row format delimitedfields terminated by '\001'collection items terminated by '\002'map keys terminated by '\003'lines terminated by '\n'stored as textline; 3.4 读时模式 传统数据库是写时模式，数据写入数据库时对模式进行检查。 Hive是读时模式，数据查询的时候检查，如果没有该字段会有很多的null值、读取数值列时发现非数值的字符串类型会返回null值。Hive尽可能地将各种错误恢复过来。 4 HiveQL:数据定义5 HiveQL:数据操作6 HiveQL:查询7 HiveQL:视图8 HiveQL:索引9 模式设计10 调优11 其他文件格式和压缩方法12 开发13 函数14 Streaming15 自定义Hive文件和记录格式16 Hive的Thrift服务17 存储处理程序和NoSQL18 安全19 锁20 Hive和Onzie整合]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[RNN&LSTM介绍]]></title>
    <url>%2F2018%2F06%2F19%2FRNN-LSTM%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[1 RNN DNN和CNN中，训练样本的输入和输出是确定的。但有一类问题DNN和CNN不好解决，当训练样本输入是连续的序列,且序列的长短不一，比如基于时间的序列：一段段连续的语音，一段段连续的手写文字。这些序列比较长，且长度不一，比较难直接的拆分成一个个独立的样本来通过DNN/CNN进行训练。对于这类问题，RNN比较擅长。 循环神经网络(Recurrent Neural Networks)，它广泛的应用于自然语言处理中的语音识别，手写识别以及机器翻译等领域。 RNN假设样本是基于序列的，比如是从序列索引1到序列索引τ的。对于这其中的任意序列索引号t,它对应的输入是对应的样本序列中的x(t)。而模型在序列索引号t位置的隐藏状态h(t)，则由x(t)和在t−1位置的隐藏状态h(t−1)共同决定。在任意序列索引号t，有对应的模型预测输出o(t),o(t)只由模型当前的隐藏状态h(t)决定。通过预测输出o(t)和训练序列真实输出y(t),以及损失函数L(t)，就可以用DNN类似的方法来训练模型，接着用来预测测试序列中的一些位置的输出，如下图所示为常见的RNN模型结构。 RNN虽然理论上可以很漂亮的解决序列数据的训练，但是它也像DNN一样有梯度消失时的问题，当序列很长的时候问题尤其严重。因此，上面的RNN模型一般不能直接用于应用领域。在语音识别，手写识别以及机器翻译等NLP领域实际应用比较广泛的是基于RNN模型的一个特例LSTM。 2 LSTM LSTM(Long Short-Term Memory),可以避免常规RNN的梯度消失，在工业界广泛应用。LSTM在每个序列索引位置t的门一般包括遗忘门，输入门和输出门三种。和普通的神经网络DNN整体形式差不多，输入神经元-隐藏层-输出神经元，只是隐藏层之间有关联。比如一张图片28*28像素，可以分成28行，第一行有28个像素，即28个神经元，第二行也有28个神经元，第一行的隐藏层和第二行的隐藏层之间有关联。 省略去每层都有的o(t),L(t),y(t)，RNN的模型简化成如下图。 对于序列索引位置t的隐藏结构做了改进，来避免梯度消失的问题，这样的特殊RNN就是LSTM，LSTM的常见结构如下图。 在t时刻除了h(t)，还多了一个隐藏状态，如下图上面的长横线，称为细胞状态(Cell State)，记为C(t)。 遗忘门，在LSTM中以一定的概率控制是否遗忘上一层的隐藏细胞状态C(t-1)。图中输入的有上一序列的隐藏状态h(t−1)和本序列数据x(t)，通过一个激活函数，一般是sigmoid，得到遗忘门的输出f(t)。由于sigmoid的输出f(t)在[0,1]之间，因此这里的输出f(t)代表了遗忘上一层隐藏细胞状态的概率，f(t)再乘以C(t),遗忘门子结构如下图所示。 输入门负责处理当前序列位置的输入。输入门由两部分组成，第一部分使用了sigmoid激活函数，输出为i(t),第二部分使用了tanh激活函数，输出为a(t), 两者相乘再加上传递来的细胞状态，如下图所示。 细胞状态更新，细胞状态C(t)由两部分组成，第一部分是C(t−1)和遗忘门输出f(t)的乘积，第二部分是输入门的i(t)和a(t)的乘积，第一部分加上第二部分，如下图所示。 输出门，隐藏状态h(t)的更新由两部分组成，第一部分是o(t),它由h(t−1)和x(t)以及激活函数sigmoid得到，第二部分由隐藏状态C(t)通过tanh激活函数, 第一部分乘以第二部分，如下图所示。知道了h(t)，进而知道了o(t)，和真实值对比，获得损失函数，进而训练即可。 实际应用中LSTM的难点不在前向反向传播算法，这些有算法库，模型结构和一大堆参数的调参才是问题，理解LSTM模型结构是高效使用的前提。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CNN介绍]]></title>
    <url>%2F2018%2F06%2F18%2FCNN%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[1 简介 卷积神经网络(Convolutional Neural Networks)是最为成功的DNN特例之一,广泛应用于图像识别，也应用于NLP等其他领域。 图像识别任务中，挑一张图像，经一系列卷积层、非线性层、池化(下采样（downsampling))层和完全连接层，最终得到输出，输出描述了图像内容的一个单独分类或分类的概率。 当看到一幅狗的图片时，如果有诸如爪子或四条腿之类的明显特征，便能将它归类为狗。同样地，计算机也可以通过寻找诸如边缘和曲线之类的低级特点来分类图片，继而通过一系列卷积层级建构出更为抽象的概念。 和普通的神经网络DNN不同的是，网络前面的部分不一样，不是隐藏层这种形式，而是过滤器这种，网络后面的部分是一样的，变成输入神经元-隐藏层-输出神经元这种形式。 2 原理 过程 第一层通常是卷积层(Convolutional Layer)，假设卷积层的输入内容是一个32x32x3的像素值数组(3表示是彩色图片)。想象有一束手电筒光正从图像的左上角照过，假设手电筒光可以覆盖5x5的区域。这束手电筒被叫做过滤器(即某个神经元对应的权重矩阵W)，被照过的区域被称为感受野。过滤器是一个数组（3维数组，其中的数字被称作权重或参数）。过滤器的深度必须与输入内容的深度相同（这样才能确保可以进行数学运算），因此过滤器大小为5x5x3。以过滤器所处在的第一个位置为例即图像的左上角，当筛选值在图像上滑动（卷积运算）时，过滤器中的值会与图像中的原始像素值相乘（又称为计算两个数组的点积）。这些乘积被加在一起求和（从数学上来说，一共会有5x5+5x5+5x5=75个乘积），最终得到一个常数数值，表示过滤器位于图片左上角的情况。然后在输入内容上的每一位置重复该过程，下一步将是将过滤器右移1单元，接着再右移1单元，以此类推。输入内容上的每一特定位置都会产生一个数字。过滤器滑过所有位置后将得到一个28x28x1的数组(每一行/列有33-5+1个位置，即28x28x1)，称得到的这个数组为一个特征。使用两个而不是一个5x5x3的过滤器时(两个权重矩阵W)，输出总量将会变成28x28x2，即2个特征，2个神经元。采用的过滤器越多，原始信息保留得越好，特征也就越多。 每个过滤器可以被看成是特征筛选器，特征指的是例如直角边缘、原色、曲线之类的东西。假设第一组过滤器是7x7x3的曲线检测器，如下图所示。将过滤器置于输入内容的左上角时以及其他位置时，它将计算过滤器和这一区域像素值之间的点积。如果输入图像上某个位置某个形状看起来很像过滤器表示的曲线，那么所有点积加在一起将会得出一个很大的值，会被激活；如果不像，则计算点积会得到一个很小的常数值，不会被激活。还有其它检测左弯曲线或直线边缘的过滤器，过滤器越多，激活映射的深度越大，对输入内容的了解也就越多。 第一个卷积层的输出将会是一个28x28x3的数组（假设采用三个5x5x3的过滤器，3个特征，3个神经元）。当进入另一卷积层时，第一个卷积层的输出便是第二个卷积层的输入。第一层的输入是原始图像，而第二卷积层的输入正是第一层输出的激活映射。也就是说，第一层的输入大体描绘了低级特征在原始图片中的位置。在此基础上再采用一组过滤器（让它通过第2个卷积层），输出将是表示了更高级的特征的激活映射。这类特征可以是半圆（曲线和直线的组合）或四边形（几条直线的组合）。随着进入网络越深和经过更多卷积层后，将得到更为复杂特征的激活映射。 完全连接层，这一层处理输入内容（该输入可能是卷积层、ReLU 层或是池化层的输出）后会输出一个N维向量，N是该程序必须选择的分类数量。 其他 CNN的输入不是样本数x784个像素点，而是样本数x(28x28),最后的全连接层需要还原即样本数*神经元个数 计算机通过一个反向传播的训练过程来调整过滤器值（权重W）,初始值随机赋值。反向传播可分为四部分，分别是前向传导、损失函数、后向传导，以及权重更新。 步幅和填充 选择了过滤器的尺寸以后，还需选择步幅(stride)和填充(padding)，步幅即过滤器移动的步长。当把5x5x3的过滤器用在32x32x3的输入上时，输出的大小会是28x28x3，这里空间维度减小了。想让输出量维持为32x32x3,可以对这个层应用大小为2的零填充(zero padding)。零填充在输入内容的边界周围补充零，如果用两个零填充，就会得到一个36x36x3的输入卷，步长不同，0填充的个数也不一样。 超参数 要用多少层、多少卷积层、过滤器尺寸是多少、以及步幅和填充值多大呢？这些问题没有一个固定标准，因为神经网络很大程度上取决于数据类型，图像的大小、复杂度、图像处理任务的类型以及其他更多特征的不同都会造成数据的不同。 ReLU（修正线性单元）层 每个卷积层之后，通常会立即应用一个非线性层（或激活层）。目的是给一个在卷积层中刚经过线性计算操作（只是数组元素依次相乘与求和）的系统引入非线性特征。过去用的是像双曲正切和sigmoid型函数这样的非线性方程，但ReLU层效果好得多，因为神经网络能够在准确度不发生明显改变的情况下把训练速度提高很多（由于计算效率增加），而且能帮助减轻梯度消失的问题。这一层把所有的负激活变为0，这一层会增加模型乃至整个神经网络的非线性特征，而且不会影响卷积层的感受野。 池化层在几个ReLU层之后，一般会选择用一个池化层(pooling layer)，是一个过滤器。也被叫做下采样（downsampling）层，最常用的是最大池化( max-pooling)。采用了一个过滤器（通常是2x2的）和一个同样长度的步幅。如下图所示，输出每个子区域中的最大数字。池化层还有其他选择，比如平均池化(average pooling)和L2-norm池化。这一层背后的原理是：它与其它点的相对位置就比它的绝对位置更重要。有两个主要目的：第一个是权重参数的数目减少到了75%，长度和宽度改变了，但深度没变，因此降低了计算成本；第二是它可以控制过拟合。 分类、定位、检测、分割图像分类任务，获取输入图片，输出类别。目标定位任务，生成一个描述图片中物体所在位置的边界框(生成图片边界框)，如下图。目标检测任务，这需要图片上所有目标的定位任务都已完成，将获得多个边界框和多个类标签(每个边界框对于一个类)，如下图。目标分割任务，输出类标签的同时输出图片中每个目标的轮廓，如下图。 迁移学习迁移学习的思路将帮助我们降低数据需求，指的是利用预训练模型（神经网络的权重和参数都已经被其他人利用更大规模的数据集训练好了）并用自己的数据集将模型「微调」的过程。这种思路中预训练模型扮演着特征提取器的角色。自己将移除网络的最后一层并用自有的分类器置换（取决于你的问题类型）。然后冻结其他所有层的权重并正常训练该网络（冻结这些层意味着在梯度下降/最优化过程中保持权值不变）。 数据增强技术当计算机将图片当作输入时，它将用一个包含一列像素值的数组描述（这幅图）。若是图片左移一个像素。对人来说，这种变化是微不可察的。然而对计算机而已，这种变化非常显著：这幅图的类别和标签保持不变，数组却变化了。这种改变训练数据的数组表征而保持标签不变的方法被称作数据增强技术。这是一种人工扩展数据集的方法。常使用的增强方法包括灰度变化、水平翻转、垂直翻转、随机编组、色值跳变、翻译、旋转等其他多种方法。通过利用这些训练数据的转换方法，将获得两倍甚至三倍于原数据的训练样本。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[DNN介绍]]></title>
    <url>%2F2018%2F06%2F17%2FDNN%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[1 简介 深度神经网络（Deep Neural Networks），DNN可以理解为有很多隐藏层的神经网络。输入神经元个数为1个样本的特征个数，输出神经元个数为1个样本的输出label个数，且各层神经元是全连接的。 最小化DNN的损失函数的过程，即反向传播算法，一层层往前推。 2 损失函数和激活函数 输出是连续值时，使用均方差损失函数和Sigmoid激活函数，函数曲线越来越平缓，最小化损失收敛慢，可以使用交叉熵损失函数(LR的对数损失) 输出是分类值时，使用交叉熵损失函数+softmax函数(对结果归一化) 梯度消失和梯度爆炸，反向传播算法求解过程中，使用了矩阵求导的链式法则，有一大串连乘 如果连乘数字小于1，则越来越小，即梯度消失。这个问题不能完美解决，常用Relu函数max(0,z)，CNN中得到了广泛的应用。 如果连乘数字大于1，则越来越大，即梯度爆炸，一般可以通过调整模型初始化参数来解决。 3 正则化 损失函数加上L1/L2正则化 集成学习，效仿随机森林，有放回采样，5-10个DNN模型 使用dropout的方式（由于dropout会将原始数据分批迭代，因此原始数据集最好较大，否则模型可能会欠拟合。）Dropout 层只能在训练中使用，而不能用于测试过程。 增强数据集，原始的数据集中的图像，我们可以将原始图像稍微的平移或者旋转一点点，则得到了一个新的图像。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理概述]]></title>
    <url>%2F2018%2F06%2F15%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1 相关术语 分词常用的手段是基于词典的正向/反向/双向最大匹配法+统计分词(HMM等)。 词性标注词性一般指动词、名词、形容词等，比如我/r爱/v北京/ns天安门/ns。ns代表名词，v代表动词，都是标注，作用是为分词、命名实体识别和句法分析等做预处理。 命名实体识别从文本中找出人名、地名等。 句法分析往往最终生成的结果是一棵句法分析树。句法分析可以解决传统词袋模型不考虑上下文的问题。“小李是小杨的班长”和“小杨是小李的班长”，这两句话，用词袋模型是相同的，但是句法分析可以分析出其中的主从关系。 指代消解中文中代词出现的频率很高，需要确定代词指向哪个名词。 情感识别本质上是分类问题，常应用在舆情分析等领域。情感一般分为两类，即正面、负面，也可以在前面基础上加上中性类别。电商企业，情感识别可以分析商品评价的好坏，以此作为下一个环节的评判依据。通常可以基于词袋模型+分类器，或者现在流行的词向量模型+RNN。经过测试发现，后者比前者准确率略有提升。 纠错在搜索技术和输入法等领域，可以基于N-Gram进行纠错，也可以通过字典树、有限状态机等方法进行纠错。 问答系统往往需要语音识别、合成，自然语言理解、知识图谱等多项技术的配合才会实现得比较好。 2 自然语言处理层面 词法分析包括汉语的分词和词性标注这两部分。 句法分析业界存在三种比较主流的句法分析方法。 短语结构句法体系，作用是识别出句子中的短语结构以及短语之间的层次句法关系 依存结构句法体系，作用是识别句子中词与词之间的相互依赖关系 深层文法句法分析，利用深层文法，例如词汇化树邻接文法，组合范畴文法等对句子进行深层的句法以及语义分析。复杂度高，不适合大规模数据。 语义分析最终目的是理解句子表达的真实语义。但是，语义应该采用什么表示形式一直困扰着研究者们，没有统一答案。语义角色标注是目前比较成熟的浅层语义分析技术。语义角色标注一般都在句法分析的基础上完成，句法结构对于语义角色标注的性能至关重要。 3 中文分词主要归纳为“规则分词”“统计分词”和“混合分词（规则+统计）”这三个主要流派。规则分词是最早兴起的方法，主要是通过人工设立词库，按照一定方式进行匹配切分，其实现简单高效，但对新词很难进行处理。随后统计机器学习技术的兴起，应用于分词任务上后，就有了统计分词，能够较好应对新词发现等特殊场景。然而实践中，单纯的统计分词也有缺陷，那就是太过于依赖语料的质量，因此实践中多是采用这两种方法的结合，即混合分词。 3.1 规则分词基于规则的分词是一种机械分词方法，主要是通过维护词典，在切分语句时，将语句的每个字符串与词表中的词进行逐一匹配，找到则切分，否则不予切分，按照匹配切分的方式，有三种。基于规则的分词简单高效，但是词典维护工程大，因为新词层出不穷。 3.1.1 正向最大匹配法首先有词典和待切分字符串，假设词典词语最大长度为i，从左向右取字符串前i个字符，然后匹配词典，如果匹配不了，则取前i-1个字符，继续匹配，直至匹配上或者只剩1个字符了。然后原始字符串去掉这个字符，继续重复同样的操作，取前i个字符。 3.1.2 逆向最大匹配法首先字典需要逆序字典，然后从右向左取前i个字符，匹配，匹配不了，取前i-1个字符等，其他和正向最大匹配法原理一样。 3.1.3 双向最大匹配法将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结果进行比较，然后按照最大匹配原则，选取最终切分词的数量最少的作为结果。 3.2 统计分词基于统计的分词，一般两个步骤：(1)建立统计语言模型(2)对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式，用到了统计学习算法，如隐马尔科夫模型(HMM)、条件随机场(CRF)等 3.2.1 语言模型语言模型就是求长度为m的字符串的概率分布P(ω1,ω2,…,ωn),其中ω1到ωn代表文本中的各个字，P(ω1，ω2，…，ωm)=P(ω1)P(ω2|ω1)P(ω3|ω1，ω2)…P(ωi|ω1，ω2，…，ωi-1)…P(ωm|ω1，ω2，…，ωm-1)。当文本过长时，计算难度大。一般用n元模型来降低难度，n元模型忽略距离大于等于n的上文词的影响。当n=2时，称为二元模型，只考虑前面1个词，P(ωi|ω1，ω2，…，ωi-1)=P(ωi|ωi-1)，一般二元模型采用的多。一般使用频率计数的比例来计算n元条件概率,可用拉普拉斯平滑算法解决分子分母为0的问题 3.2.2 HMM模型，通过求解每个词的词位来进行分词。 每个字在构造一个特定的词语时都占据着一个词位，即B（词首）、M（词中）、E（词尾）和S（单独成词）。通过求得每个字的词位，即可分词。 用λ=λ1λ2…λn代表输入的句子，n为句子长度，λi表示字，o=o1o2…on代表输出的标签，那么理想的输出即为：max=maxP(o1o2…on|λ1λ2…λn)，o即为B、M、E、S这4种标记。 期望求解的是max P(o|λ)，通过贝叶斯公式转而求解max P(λ|o)P(o)(分母为常数不需要求)。 假设一，马尔科夫假设。P(λ|o)=P(λ1|o1)P(λ2|o2)…P(λn|on) 假设二，齐次马尔科夫假设，每个输出仅仅与上一个输出有关，即二元模型。P(o)=P(o1)P(o2|o1)P(o3|o2)…P(on|on-1) P(λ|o)P(o)～P(λ1|o1)P(o2|o1)P(λ2|o2)P(o3|o2)…P(on|on-1)P(λn|on) （3.10）。在HMM中，将P(λk|ok)称为发射概率，P(ok|ok-1)称为转移概率。HMM中，求解maxP(λ|o)P(o)的常用方法是Viterbi算法，它是一种动态规划方法。 语料库为分好词的句子，比如这里。实际项目实战中，读者可通过扩充语料、词典补充等手段予以优化。 3.2.3 其他统计分词算法 条件随机场是一种基于马尔可夫思想的统计模型，每个状态不止与他前面的状态有关，还与他后面的状态有关。 神经网络分词算法通常采用CNN、LSTM等深度学习网络自动发现一些模式和特征，然后结合CRF、softmax等分类算法进行分词预测。 3.2.4 优缺点 对比机械分词法，这些统计分词方法不需耗费人力维护词典，能较好地处理歧义和未登录词，是目前分词中非常主流的方法。但其分词的效果很依赖训练语料的质量，且计算量相较于机械分词要大得多。 3.3 混合分词目前不管是基于规则的算法、还是基于HMM、CRF或者deep learning等的方法，其分词效果在具体任务中，其实差距并没有那么明显。实际工程应用中，多是基于一种分词算法，然后用其他分词算法加以辅助。最常用的方式就是先基于词典的方式进行分词，然后再用统计分词方法进行辅助。在保证词典分词准确率的基础上，对未登录词和歧义词有较好的识别，Jieba分词工具便是基于这种方法的实现。 3.4 JieBaJieba不是只有分词这一个功能，其是一个开源框架，提供了很多在分词之上的算法，如关键词提取、词性标注等。提供了很多热门社区项目的扩展插件，如ElasticSearch、solr、lucene等。Jieba分词结合了基于规则和基于统计这两类方法。首先基于前缀词典进行词图扫描，前缀词典是指词典中的词按照前缀包含的顺序排列，例如词典中出现了“上”，之后以“上”开头的词都会出现在这一部分，例如“上海”，进而会出现“上海市”。基于前缀词典可以快速构建包含全部可能分词结果的有向无环图。基于标注语料，使用动态规划的方法可以找出最大概率路径，并将其作为最终的分词结果。对于未登录词，Jieba使用了基于汉字成词的HMM模型，采用了Viterbi算法进行推导。 3.4.1 三种分词模式精确模式：试图将句子最精确地切开，适合文本分析，默认精确模式，jieba.cut(str1,HMM=True)。全模式：把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义，jieba.cut(str1,cut_all=True)。搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词,jieba.cut_for_search(str1)。 3.4.2 去除停用词通用的停用词表，一般实践过程中，需要根据自己的任务，定期更新维护。 3.4.3 提高分词效果需要定制自己的领域词典，用以提升分词的效果。Jieba分词就提供了这样的功能，用户可以加载自定义词典，jieba.load_userdict(‘./data/user_dict.txt’) 4 词性标注与命名实体识别 4.1 词性标注词性，即名词、动词、形容词等。 4.1.1 词性标注最简单的方法是从语料库中统计每个词所对应的高频词性，将其作为默认词性。目前较为主流的方法是如同分词一样，将句子的词性标注作为一个序列标注问题来解决，分词中常用的如隐含马尔可夫模型、条件随机场模型等皆可在词性标注任务中使用。 4.1.2 中文领域中尚无统一的标注标准，较为主流的主要为北大的词性标注集和宾州词性标注集两大类，下图为部分北大词性标注集。 4.1.3 Jieba分词中的词性标注Jieba的词性标注同样是结合规则和统计的方式，具体为在词性标注的过程中，词典匹配和HMM共同作用。 正则表达式进行汉字判断，中文范围 ‘\u4E00-\u9FA5’,前面加上^，表示排除中文re.compile(u’[^\u4E00-\u9FD5]’) 在前缀词典中找出它所分出的词性，若在词典中未找到，则赋予词性为“x”（代表未知）。当然，若在这个过程中，设置使用HMM，且待标注词为未登录词，则会通过HMM方式进行词性标注。 若不符合上面的正则表达式，那么将继续通过正则表达式进行类型判断，分别赋予“x”“m”（数词）和“eng”（英文）。 在词性标注任务中，Jieba分词采用了simultaneous思想的联合模型方法，即将基于字词位标注的分词方法与词性标注结合起来，使用复合标注集。比如，对于名词“人民”，它的词性标注是“n”，而分词的标注序列是“BE”，于是“人”的标注就是“B_n”，“民”的标注就是“E_n”。1234567891011121314import jieba.posseg as psgsent='中文分词是文本处理不可或缺的一步!'sent_list=psg.cut(sent)for w,t in sent_list: print w,t 中文 nz分词 n是 v文本处理 n不可或缺 l的 uj一步 m! x 4.2 命名实体识别 4.2.1 简介 目的是识别语料中人名、地名、组织机构名等命名实体。由于这些命名实体数量不断增加，通常不可能在词典中穷尽列出，且其构成方法具有各自的规律性，因此，通常把对这些词的识别在词汇形态处理（如汉语切分）任务中独立处理，称为命名实体识别（Named Entities Recognition，NER）。数量、时间、日期、货币等实体识别通常可以采用模式匹配的方式获得较好的识别效果。 命名实体识别更侧重高召回率，但在信息检索领域，高准确率更重要。 命名实体识别效果的评判主要看实体的边界是否划分正确以及实体的类型是否标注正确。 相较于实体类别标注子任务，实体边界的识别更加困难。 4.2.2 识别方法分词主要有三种方式，主要有基于规则的方法、基于统计的方法以及二者的混合方法。这在整个NLP的各个子任务基本上也多是同样的划分方式，命名实体识别也不例外。 基于规则的命名实体识别：依赖手工规则的系统，结合命名实体库，对每条规则进行权重赋值，然后通过实体与规则的相符情况来进行类型判断。 基于统计的命名实体识别：主流的基于统计的命名实体识别方法有隐马尔可夫模型、最大熵模型、条件随机场等。基于人工标注的语料，将命名实体识别任务作为序列标注问题来解决。对语料库的依赖比较大，而大规模通用语料库又比较少。 混合方法:在很多情况下是使用混合方法，结合规则和统计方法。借助规则知识提前进行过滤修剪处理，序列标注方式是目前命名实体识别中的主流方法。 4.2.3 条件随机场在大量真实语料中，观察序列更多的是以一种多重的交互特征形式表现出来，观察元素之间广泛存在长程相关性，HMM的效果就受到了制约。条件随机场的思想来源于HMM，每个状态不止与他前面的状态有关，还与他后面的状态有关。 由n个字符构成的NER的句子，每个字符的标签都在已知标签集合（“B”“M”“E”“S”和“O”）中，为每个字符选定标签后，形成了一个随机场(注意，标签是选定的，预先选定很多标签，任务是找出最佳标签)。加一些约束，如所有字符的标签只与相邻的字符的标签相关，那么即马尔可夫随机场问题。 假设马尔可夫随机场中有X和Y两种变量，X一般是给定的，Y是在给定X条件下的输出。如X是字符，Y为标签，P(X|Y)就是条件随机场。通过贝叶斯，最后转化求max P(y|x)。实际中，假设X和Y结构相同，即X=(X1，X2，X3，…，Xn)，Y=(Y1，Y2，Y3，…，Yn) 定义：设X=(X1，X2，X3，…，Xn)和Y=(Y1，Y2，Y3，…，Yn)均为随机变量序列，若在X的条件下，Y的条件概率分布P(Y|X)构成条件随机场，且满足马尔可夫性：P(Yi|X，Y1，Y2，…，Yn)=P(Yi|X，Yi-1，Yi+1) ，则称P(Y|X)为线性链的条件随机场，一般所说的CRF指的就是线性链CRF(也就是只考虑前面一个状态和后面一个状态)。 对句子“我来到牛家村”进行标注，正确标注后的结果应为“我/O来/O到/O牛/B家/M村/E”。采用线性链CRF来进行解决，（O，O，O，B，M，E）是其一种标注序列，（O，O，O，B，B，E）是另一种，选择很多，NER任务中就是找出最靠谱的作为句子标注。 在CRF中，定义一个特征函数集合，然后使用这个特征集合为标注序列进行打分，据此选出最靠谱的标注序列。比如如果标注中出现连续两个“B”结构的标注序列，则给它低分。 在CRF中有两种特征函数，分别为转移函数tk(yi-1，yi，i)和状态函数sl(yi，X，i)。前者依赖于当前和前一个位置，表示从标注序列中位置i-1的标记yi-1转移到位置i上的标记yi的概率。后者依赖当前位置，表示标记序列在位置i上为标记yi的概率。 通过特征函数打分，使得 arg max P(y|x) ，求得合适的标记y，x为特征函数中的x。该问题与HMM求解最大可能序列路径一样，也是采用的Viterbi算法。 解决标注问题时，HMM和CRF效果都比较好，不过CRF能够捕捉全局的信息和灵活的特征设计，因此一般效果要比HMM好，但实现复杂度高。 4.2.4 总结当将分词、词性标注和命名实体识别都作为标注任务时，采用HMM和CRF都是可行的，不同的是标签的区别。在命名实体识别中，在切完词、标注完词性后，再做识别任务，效果要比单纯的字标注要好很多。 4.3 实战 4.3.1 日期识别 背景现有一个基于语音问答的酒店预订系统，其根据用户的每句语音进行解析，识别出用户的酒店预订需求，如入住时间等。用户的语音在发送给后台进行请求时已经转换成中文文本，然而由于语音转换工具的识别问题，许多日期类的数据并不是严格的数字，会出现诸如“六月12”“2016年八月”“20160812”“后天下午”等形式。这里识别出每个请求文本中可能的日期信息，并将其转换成统一的格式进行输出。例如“我要今天住到明天”（假设今天为2017年10月1号），那么通过日期解析后，应该输出为“2017-10-01”和“2017-10-02”。 方法 自定义规则，“今天”“明天”对时间的映射。这里由于是酒店入住，基本不会出现“前天”“昨天”等情况，因此未予添加 利用Jieba词性标注的功能，提取其中“m”（数字）“t”（时间）词性的词 正则表达式匹配 优缺点 相较于基于统计的方法，规则方法无须在系统建设初期为搜集数据标注训练而苦恼，能够快速见效。 采用规则去覆盖所有的语言场景是不太现实的。 4.3.2 地名识别采用基于条件随机场的方法来完成地名识别任务，CRF++的安装，Windows系统用户可去官网https://taku910.github.io/crfpp/ 下载二进制版本，Linux或Mac用户可从Github（ https://github.com/taku910/crfpp ）或官网获取源码进行安装。 1 确定标签体系如同分词和词性标注一样，命名实体识别也有自己的标签体系。可以按照自己的想法自行设计，也采用地理位置标记规范，即针对每个字符标记为“B”“E”“M”“S”“O”中的一个。 2 语料数据处理 CRF++的训练数据要求一定的格式，一般是一行一个token，一句话由多行token组成，多个句子之间用空行分开。其中每行又分成多列，除最后一列以外，其他列表示特征。因此一般至少需要两列，最后一列表示要预测的标签（“B”“E”“M”“S”“O”）。比如只采用字符这一个维度作为特征，以“我去北京饭店。”为例，结果如下（最后一行为空行）： 1234567我 O 去 0北 B京 M饭 M店 E。 O 语料数据：比如1998年人民日报分词数据集，其主要是一个词性标注集。但可以使用其中被标记为“ns”的部分来构造地名识别语料。 3 特征模块设计 CRF有特征函数，它是通过定义一些规则来实现的，而这些规则就对应着CRF++中的特征模板。 CRF++有两种模板类型，第一种是字母U开头，为Unigram template，CRF++会自动为其生成一个特征函数集合。第二种以字母B开头，表示Bigram template，系统会自动产生当前输出与前一个输出token的组合，根据该组合构造特征函数。 特征模板需要自己定义 4 模型训练与调试训练和测试的命令：crf_learn、crf_test。这里考虑的特征维度也少（只考虑了字符本身维度），若采用词性标注后的文本作为语料，将词性作为特征加入训练集中，会使模型效果提升。 5 其他问题该程序针对一些场景能够很好地进行识别，但是在遇到诸如“回龙观”“南锣鼓巷”“北京南站”等词时识别效果并不好。这种情况在实际项目中会经常遇到，通常有以下解决办法： 扩展语料，改进模型。如加入词性特征，调整分词算法等。 整理地理位置词库。在识别时，先通过词库匹配，再采用模型进行发现。 5 关键词提取算法 5.1 概述 算法分为有监督和无监督两类。 有监督通过分类进行，构建一个词表，通过判断每个文档与词表中每个词的匹配程度，以类似打标签的方式，达到关键词提取的效果。有监督较高精度，但缺点是需要大批量的标注数据，人工成本高。固定的词表有时很难将新信息的内容表达出来，人工维护成本高。 无监督不需要人工生成、维护的词表，也不需要人工标准语料辅助进行训练。常用的有TF-IDF算法、TextRank算法和主题模型算法（包括LSA、LSI、LDA等）。 5.2 TF-IDF词频-逆文档频率，词频需要做归一化，整个文档中频次/总词数，BOW是词袋模型，即词的出现次数，未归一化处理。TF-IDF公式如下：文本中还有信息能对关键词提取起作用，比如每个词的词性、出现的位置等。在文本中，名词带有更多关键信息，对名词赋予更高权重，能使提取出来的关键词更合理。某些场景中，文本的起始段落和末尾段落比起其他部分的文本更重要，对出现在这些位置的词赋予更高权重，也能提高关键词提取。 5.3 TextRank算法可以脱离语料库(整个数据集)的背景，仅对单篇文档进行分析就可以提取该文档的关键词。 5.4 LSA/LSI/LDA算法 5.4.1 介绍 有些关键词并不一定会显式地出现在文档当中，如一篇讲动物生存环境的科普文，通篇介绍了狮子老虎鳄鱼等各种动物的情况，但是文中并没有显式地出现动物二字，这种情况下，前面的两种算法不能提取出动物这个隐含的主题信息，这时候就需要用到主题模型。 主题模型认为在词与文档之间没有直接的联系，它们应当还有一个维度将它们串联起来，主题模型将这个维度称为主题。 每个文档对应着一个或多个的主题，而每个主题对应着一个或多个的词语，通过主题，就可以得到每个文档的词分布。 已知数据集中，每个词和文档对应的p(wi|dj)都是已知的。而主题模型就是根据这个已知信息，计算p(wi|tk)和p(tk|dj)的值，含义可以理解为一篇文档中某个词语的重要性=一篇文档中属于某个主题的重要性*某个词语在该主题中的重要性，公式: p(wi|tk)和p(tk|dj)的求解常用方法就是LSA（LSI）和LDA。其中LSA主要是采用SVD（奇异值分解）的方法进行暴力破解，而LDA则是通过贝叶斯学派的方法对分布信息进行拟合。 5.4.2 LSA/LSI算法LSA（Latent Semantic Analysis，潜在语义分析）和LSI（Latent Semantic Index，潜在语义索引），通常被认为是同一种算法。LSA和LSI都是对文档的潜在语义进行分析，但是潜在语义索引在分析后，还会利用分析的结果建立相关的索引。 步骤 使用BOW模型将每个文档表示为向量 将所有的文档词向量拼接起来构成词–文档矩阵（m×n） 对词–文档矩阵进行奇异值分解（SVD）操作（[m×r]·[r×r]·[r×n]） 根据SVD的结果，将词–文档矩阵映射到一个更低维度k（[m×k]·[k×k]·[k×n]，0&lt;k&lt;r）的近似SVD结果，每个词和文档都可以表示为k个主题构成的空间中的一个点。计算每个词和文档的相似度（相似度计算可以通过余弦相似度或者是KL相似度进行，第一个矩阵的每一行(长度k，代表词语)和第三个矩阵的每一列(长度k，代表文档)计算相似度），可以得到每个文档中对每个词的相似度结果，取相似度最高的一个词即为文档的关键词(这个词不一定是构成文档中的词语，可能是通过训练集获得的新词)。 不足1)SVD的计算复杂度高，特征空间维度大时计算效率低下。当新文档进入到已有特征空间时，需要对整个空间重新训练，以得到加入新文档后对应的分布信息。2)改进提出了pLSA算法，通过使用EM算法对分布信息进行拟合替代了使用SVD进行暴力破解，一定程度上解决了LSA的部分缺陷，但是LSA仍有较多不足。3)在pLSA的基础上，引入了贝叶斯模型，实现了现在topic model的主流方法——LDA（Latent Dirichlet Allocation，隐含狄利克雷分布）。 5.4.3 LDA算法 理论基础是贝叶斯理论，拟合出词–文档–主题的分布。 假设文档中主题的先验分布和主题中词的先验分布都服从狄利克雷分布（隐含狄利克雷分布），先验分布+数据（似然）=后验分布。对已有数据集的统计，得到每篇文档中主题的多项式分布和每个主题对应词的多项式分布。 通过先验的狄利克雷分布和观测数据得到的多项式分布，得到一组Dirichlet-multi共轭，并据此来推断文档中主题的后验分布和主题中词的后验分布，就是需要的结果，其中一种主流的方法就是吉布斯采样。 通过这个分布信息计算文档与词的相似性，继而得到文档最相似的词列表。 5.4.4 总结一般情况下，使用词性过滤仅保留名词作为关键词更符合要求，但有些场景对其他词性的词有特殊要求，可以根据场景选择需要过滤的词性。还可以通过调整关键词提取数量、主题模型的主题数量等参数以及增大训练集的方式提高模型表现效果。 6 句法分析 6.1 概述 介绍 句法分析是机器翻译的核心技术，主要任务是识别出句子所包含的句法成分以及这些成分之间的关系，一般以句法树来表示句法分析的结果。 句法分析（Parsing）是从单词串得到句法结构的过程，而实现该过程的工具或程序被称为句法分析器（Parser）。 分为完全句法分析和局部句法分析两种，差别在于：完全句法分析以获取整个句子的句法结构为目的；而局部句法分析只关注于局部的一些成分，例如常用的依存句法分析就是一种局部分析方法。 本质是一套面向候选树的评价方法，会给正确的句法树赋予一个较高的分值，而给不合理的句法树赋予一个较低的分值，这样就可以借用候选句法树的分值进行消歧。 难点 歧义 搜索空间大句法分析是一个极为复杂的任务，候选树个数随句子增多呈指数级增长，搜索空间巨大，因此必须设计出合适的解码器。 方法可以简单地分为基于规则的方法和基于统计的方法两大类。基于规则的方法在处理大规模文本时，语法规则覆盖有限。随着大规模标注树库的建立，基于统计学习模型的句法分析方法开始兴起，当下最流行的是PCFG（Probabilistic Context Free Grammar,概率上下午无关语法）方法。 6.2 句法分析的数据集与评测方法 数据集句法分析的数据集是一种树形的标注结构，称为树库，如下所示：中文树库著名的有中文宾州树库（Chinese TreeBank，CTB）、清华树库（Tsinghua Chinese TreeBank，TCT）和台湾中研院树库，其中CTB是目前绝大多数的中文句法分析研究的基准语料库。不同树库的标记体系不一样，解释不能通用，下图为清华数库部分标记： 评测方法主流句法分析评测方法是PARSEVAL评测体系，指标有准确率、召回率、交叉括号数。交叉括号表示分析得到的某一个短语的覆盖范围与标准句法分析结果的某个短语的覆盖范围存在重叠又不存在包含关系，即构成了一个交叉括号。 6.3 常用方法相较于词法分析（分词、词性标注或命名实体识别等），句法分析成熟度要低上不少。以短语结构树为目标的句法分析器，目前应用最广泛，与很多其他形式语法对应的句法分析器都能通过对短语结构语法（特别是上下文无关文法）的改造而得到，句法分析属于NLP中较为高阶的问题。 基于PCFG的句法分析PCFG是上下文无关文法的扩展，可以计算分析树的概率值。PCFG衍生出了各种形式的算法，包括基于单纯PCFG、基于词汇化和基于子类划分PCFG的句法分析方法等。 基于最大间隔马尔可夫网络的句法分析最大间隔是SVM中的理论，马尔可夫网络是概率图模型中一种具备一定结构处理关系能力的算法。最大间隔马尔可夫网络（Max-Margin Markov Networks）是这两者的结合，能够解决复杂的结构化预测问题，尤为适合用于句法分析任务。这是一种判别式的句法分析方法，通过丰富特征来消解分析过程中产生的歧义。如果要实现多元分类，可以使用多个二分类模型，每个模型分别识别一个短语标记，组合多分类器即可完成句法分析。 基于CRF的句法分析当将句法分析作为序列标注问题来解决时，同样可以采用条件随机场（CRF）模型。可以采用清华树库，设计特征模板，然后训练一个基于CRF++的模型，并进行测试。 基于移进–归约的句法分析模型移动-规约是一种自下而上的方法，从输入串开始，逐步进行“规约”，操作基本数据结构是堆栈。应用于中文时，对词性非常敏感，需要和准确度较高的词性标注工具一块使用。 6.4 使用Stanford Parser的PCFG算法进行句法分析 介绍 Stanford Parser是斯坦福大学自然语言小组开发的开源句法分析器，是基于概率统计句法分析的一个Java实现。以权威的宾州树库作为分析器的训练数据，还支持分词和词性标注、短语结构、依存关系等输出，支持多个语言接口。 安装 需安装JDK，底层JAVA实现 需安装nltk库，因为Python封装是在nltk库中实现的，主要使用nltk.parse中的Stanford模块。 需要下载Stanford Parser的jar包，主要有两个：stanford-parser.jar和stanford-parser-3.8.0-models.jar。 实战对“他骑自行车去了菜市场”这句话进行句法分析以及可视化操作，代码中Stanford Parser的句法分析器接收的输入是分词完后以空格隔开的句子，stanford-parser-3.8.0-models.jar是已经训练好的模型，解压jar包可以看到支持的算法。 6.5 总结 和词法分析（分词、词性标注和命名实体识别等）相比，句法分析算法实际性能离真正实用化有距离，原因在于语言学理论和实际的自然语言应用之间存在很大差距。 实践中，句法分析常结合一定规则来辅助解决一些任务。如模板解析类的任务，通过句法分析进行语义标注，提取其中主谓宾关系，再通过规则模板标出重要的角色信息和行为。 7 文本向量化 7.1 向量化算法word2vec认为上下文相似的词，其语义也相似。利用上下文分布表示词义的方法，这类方法就是有名的词空间模型（word space model）。神经网络词向量模型根据上下文与目标词之间的关系进行建模。 神经网络语言模型 神经网络语言模型（Neural Network Language Model，NNLM），和传统方法估算P(ωi|ωi-(n-1)，…，ωi-1)不同，NNLM模型直接通过一个神经网络结构对n元条件概率进行估计。任务是根据语料库，求解词语向量矩阵即权重矩阵。求解的方法是，从语料库中取出一系列长度为n的样本，然后将这个样本one-hot矩阵表示，通过权重矩阵乘以这个one-hot矩阵，就能获得每个词语的向量，然后将这些词语拼接，则获得了这个长度为n的样本的向量化表达，参考这里。目标函数为使已知前n-1个词语第n个词的概率最大。 NNLM模型使用低维紧凑的词向量对上文进行表示，解决了词袋模型带来的数据稀疏、语义鸿沟等问题；另一方面，在相似的上文语境中，NNLM模型可以预测出相似的目标词。例如，A=“一只小狗躺在地毯上”出现了2000次，B=“一只猫躺在地毯上”出现了1次。根据频率，P(A)要远远大于P(B)，而唯一的区别在于猫和狗，这两个词在词义和语法上都相似，而P(A)远大于P(B)显然是不合理的，采用NNLM计算则得到的P(A)和P(B)是相似的。每个词的输出概率y(ωi)即为所对应的点，将所有词连接，即组成了向量。 NNLM模型的目标是构建一个语言概率模型，最费时的部分当属隐藏层到输出层的权重计算。简单的NNLM模型只有三层，输入层、隐藏层和输出层。 希望能够在得知上文的情况下，知道y(wi)的概率。而y(wi)概率越大，说明上文越有效。则在语料库D中最大化y(ωi)便是NNLM模型的目标函数，如下 C&amp;W模型 C&amp;W是以生成词向量为目标的模型，如果n元短语在语料库中出现过，那么模型会给该短语打高分；如果是未出现在语料库中的短语则会得到较低的评分。 C&amp;W模型的输入层就包含了目标词，其输出层也变为一个节点，该节点输出值的大小代表n元短语的打分高低。相应的，C&amp;W模型的最后一层运算次数为|h|(输出层上一层神经元个数)，远低于NNLM模型的|V|×|h|(词语数目*输出层上一层神经元个数)次。较NNLM模型而言，C&amp;W模型可大大降低运算量。 C&amp;W模型最小化目标函数: 1-(score(ω,c)-score(ω’,c))，其中(ω,c)表示正样本，来自语料库抽取的n元短语，为保证上下文词数一致，n为奇数，ω是目标词，c表示目标词的上下文语境；(ω’,c)表示负样本，负样本是将正样本序列中的中间词替换成其他词得到的。希望(score(ω,c)-score(ω’,c))越大越好，表示正样本得分高负样本得分低，越大，目标函数值越小，因此需要最小化目标函数。 CBOW模型和Skip-gram模型在NNLM和C&amp;W模型的基础上保留其核心部分，得到了CBOW（Continuous Bag of-Words）模型和Skip-gram模型。 CBOW模型(连续词袋模型) 输入层是语义上下文的表示(输出层是这段文本的中间词?)，去掉了隐藏层，大幅提升运算速率，预测下一个词的概率。 使用上下文各词的词向量的平均值替代NNLM模型各个拼接的词向量，如”the cat sat”这句话，首先用固定长度的不同词向量表示上文的三个词语，接着将这三个词向量求平均组成上文的向量化表示。 目标函数和NNLM模型类似，参考上文。 Skip-gram模型同样去掉了隐藏层，但与CBOW模型输入上下文词的平均词向量不同，Skip-gram模型是从目标词ω的上下文中选择一个词，将其词向量组成上下文的表示。 总结Skip-gram和CBOW实际上是word2vec两种不同思想的实现：CBOW的目标是根据上下文来预测当前词语的概率，且上下文所有的词对当前词出现概率的影响的权重是一样的，因此叫continuous bag-of-words模型。Skip-gram刚好相反，其是根据当前词语来预测上下文概率的。在实际使用中，算法本身并无高下之分，可根据最后呈现的效果来进行算法选择。 7.2 向量化算法doc2vec/str2vec 利用word2vec技术计算词语间的相似度效果良好，也可计算句子或者其他长文本间的相似度。对文本分词后提取关键词，用词向量表示这些关键词，接着对关键词向量求平均或者将其拼接，最后利用词向量计算文本间的相似度。但丢失了文本中的语序信息，例如“小王送给小红一个苹果”和“小红送给小王一个苹果”，组成两个句子的词语相同，但表达信息不同。 在word2vec的基础上提出了文本向量化（doc2vec），又称str2vec和para2vec。doc2vec是word2vec的升级，doc2vec不仅提取了文本的语义信息，而且提取了文本的语序信息。在一般的文本处理任务中，会将词向量和段向量相结合使用以期获得更好的效果。 doc2vec技术存在两种模型——Distributed Memory（DM）和Distributed Bag of Words（DBOW），分别对应word2vec技术里的CBOW和Skip-gram模型。与CBOW模型类似，DM模型试图预测给定上下文中某单词出现的概率，只不过DM模型的上下文不仅包括上下文单词而且还包括相应的段落。DBOW则在仅给定段落向量的情况下预测段落中一组随机单词的概率，与Skip-gram模型只给定一个词语预测目标词概率分布类似。 DM模型增加了一个与词向量长度相等的段向量，如段落paragraph ID也是先映射成一个向量，即paragraph vector。paragraph vector与word vector的维数虽然一样，但是代表两个不同的向量空间。如”the cat sat”这句话，首先用固定长度的不同词向量表示上文的三个词语，接着将这三个词向量和段落向量拼接/求平均组成上文的向量化表示，然后将其输入softmax层。在一个句子或者文档的训练过程中，paragraph ID保持不变，共享着同一个paragraph vector，相当于每次在预测单词的概率时，都利用了整个句子的语义。在预测阶段，给待预测的句子新分配一个paragraph ID，词向量和输出层softmax的参数保持训练阶段得到的参数不变，重新利用随机梯度下降算法训练待预测的句子。待误差收敛后，即得到待预测句子的paragraph vector，如下图所示： DBOW模型DM模型通过段落向量和词向量相结合的方式预测目标词的概率分布，而DBOW模型的输入只有段落向量，通过一个段落向量预测段落中某个随机词的概率分布，如下图所示: 7.3 实战 7.3.1 词向量训练 1 中文语料预处理采用维基百科里的中文网页作为训练语料库，最新语料是xml格式的，需要进行处理，如将繁体转简体以及分词等。 2 向量化训练利用gensim模块的Word2Vect读取语料，然后产生词向量，保存模型，需要时调用。 7.3.2 段落向量的训练这里不将每个文档进行分词，而是直接将转换后的简体文本保留。doc2vec在训练时能够采用tag信息来更好地辅助训练（表明是同一类doc），相对word2vec模型，输入文档多了一个tag属性。训练产生段落向量，保存模型，需要时调用。 7.3.3 利用word2vec和doc2vec计算网页相似度 word2vec计算抽取网页新闻中的关键词，接着将关键词向量化，然后将得到的各个词向量相加(而不是拼接)，最后得到的一个词向量总和代表网页新闻的向量化表示，利用这个总的向量计算网页相似度。 doc2vec计算主要包括如下三个步骤：①预处理；②文档向量化；③计算文本相似。 总结doc2vec方法计算的相似度为0.87高于word2vec计算的0.66，通过阅读前两篇新闻，知道这两篇新闻极为相似，因此可以判断doc2vec计算文本相似度的方法更胜一筹。这是因为：doc2vec不仅利用了词语的语义信息而且还综合了上下文语序信息，而word2vec则丢失了语序信息；word2vec方法中的关键词提取算法准确率不高，丢失了很多关键信息。 8 情感分析技术情感分析可以被归类为文本分类问题。 8.1 情感分析的应用 舆情分析(网络数据)来自消费者或者任何第三方机构的正面或者负面的新闻报道，都会影响到公司的发展。 消费者呼声(产品自身数据)消费者呼声是指个体消费者对产品与服务的评价。需要对消费者的评价和反馈进行分析。 市场呼声(网络数据)市场呼声是指消费者使用竞争对手提供的产品与服务的感受。 8.2 情感分析的基本方法对情感分析的研究主要集中在两个方面：识别给定的文本实体是主观的还是客观的，以及识别主观的文本的极性。文本可以划分为积极和消极两类，或者积极、消极和中性（或不相关）的多类。 词法分析词法分析使用了词汇积极性标记字典，将输入文本转换为单词序列后，将每一个新的单词与字典中的词汇进行匹配。如果有一个积极的匹配，分数加到输入文本的分数总池中。例如，如果“戏剧性”在字典中是一个积极的匹配，那么文本的总分会递增。相反，如果有一个消极的匹配，输入文本的总分会减少。文本的分类取决于文本的总得分。 机器学习方法它可以分为三个阶段：数据收集、预处理、训练分类。在训练过程中，需要提供一个标记语料库作为训练数据。分类器使用一系列特征对目标数据进行分类，其中单个短语、两个连续的短语、三个连续的短语、积极词汇的数量、消极词汇的数量、文档长度等都可以作为特征。 混合分析既可以利用机器学习方法的高准确性，又可以利用词法分析快速的特点。 8.3 实战评论情感分析在深度学习出现之前，主流的表示方法有BOW和topic model（主题模型），分类模型主要有SVM和LR。但忽略了语法和文法，例如“这部电影非常难看”和“无聊，无趣，毫无意义”有着类似的意义，但是“这部电影一点也不难看”和前面一句话有着几乎一样的特征表示，但是却代表着截然不同的意思。为了解决这个问题，使用word2vec，可以将文本嵌入到低维空间，并且不丢失文本的顺序信息。分类模型方面，一般会使用如SVM、朴素贝叶斯等，或者CNN、RNN及LSTM等。步骤：(1)训练或者载入一个词向量生成模型，对样本(文本)向量化。(2)创建一个用于训练集的ID矩阵。(3)创建LSTM计算单元。(4)训练。(5)测试。]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[推荐系统概述]]></title>
    <url>%2F2018%2F06%2F11%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1. 概述 长尾效应就在于它的数量上，将所有非流行的市场累加起来就会形成一个比流行市场还大的市场。这也是数据挖掘中，提高预测的关键。用户活跃度和物品流行度均成长尾分布，接近直线。二者关系，一般认为，新用户倾向于浏览热门的物品，因为他们对网站还不熟悉，只能点击首页的热门物品，而老用户会逐渐开始浏览冷门的物品。用户越活跃，越倾向于浏览冷门的物品。 个性化广告投放，发展为一门独立学科，计算广告学。主要三种场景，网页上下文插入广告，搜索广告和个性化展示广告。 隐性反馈数据集，特点是只有正样本（用户喜欢什么物品），而没有负样本（用户对什么物品不感兴趣）。 推荐准确率在有的场景下不是一个好的评测指标，比如在亚马逊书店中，用户迟早会买这本书，推荐只是方便了用户，但并没有带来额外的利润。一方面没有让用户感到惊喜，一方面没有带来潜在的用户。 推荐系统3种评测推荐结果实验方法，离线实验、调查问卷和在线实验，一般都需要完成这3个实验。 推荐算法常用3种方式联系用户兴趣和物品：一，推荐与用户喜欢过的物品相似的物品(通过用户行为计算)，ItemCF；二，推荐与用户类似用户喜欢的物品，UserCF;三，推荐那些具有用户喜欢的特征的物品，隐语义模型等。第四种，则是UGC标签方法，普通用户自己给内容打标签，比如给电影打标签。 在线实验，常用AB测试。将不同用户分组，用不同算法推荐，最后看评测指标。一般周期比较长，所以不能上线所有算法，一般只测试在离线实验和调查问卷中表现比较好的算法。 大型网站需要设计合理的AB测试系统，比如前端网页界面AB测试，后台推荐算法也在做AB测试，结果会互相干扰。因此，切分流量很关键。 推荐系统中，根据现实需求会有不同的推荐任务，也需要不同的特征，而特征数目一般也比较大。如果要在一个系统中把各种特征和任务都统筹考虑，那么系统将会非常复杂，而且很难通过配置文件方便地配置不同特征和任务的权重。因此，推荐系统需要由多个推荐引擎组成，每个推荐引擎负责一类特征和一种任务，而推荐系统的任务只是将推荐引擎的结果按照一定权重或者优先级合并、排序然后返回。 Top N 推荐相关注意事项 如果是评分矩阵，矩阵分解目标函数是RMSE，然后选取评分top N。但这样有个问题，比如1000个评分为1的样本，10个评分为5的样本，对目标函数样本重要性一样，但因为评分为1的样本多，所以算法会倾向于对评分为1的样本预测得更准确些。然而这样没有意义，需要的是top N,需要关注对评分为5的样本预测更准。 还有个情况就是，重要的是预测用户会不会看这个影片，而不是预测是用户看了某部影片给出的评分多少。也许有一部电影用户看了之后会给很高的分数，但用户看的可能性非常小。 离线实验时，用户物品矩阵应该是非0即1，然后选取准确率和召回率等为指标。 2. 评测指标2.1 用户满意度调查问卷、购买率、点击率、用户停留时间等 2.2 预测准确度 评分预测均方根误差RMSE，绝对值误差MAE。RMSE加大了对预测不准的惩罚(平方项的惩罚)，评分系统基于整数指定的，取整的MAE降低了误差(实际误差大) Top N推荐准确率和召回率。为了全面评测，一般会选取不同推荐列表的长度，得到一组准确率和召回率，然后画出准确率和召回率曲线。 覆盖率覆盖率描述一个推荐系统对物品长尾的发掘能力，最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例，常用的指标有信息熵和基尼系数，二者值越小越好。 多样性评测多样性和相似性是相反的，通过计算相似性可以评测多样性。假设用户喜欢动作片和动画片，且用户80%的时间在看动作片，20%的时间在看动画片。C列表中有8部动作片和2部动画片，是一个比较好的推荐。 新颖性评测新颖度的最简单方法是利用推荐结果的平均流行度，因为越不热门的物品越可能让用户觉得新颖。因此，如果推荐结果中物品的平均热门程度较低，那么推荐结果就可能有比较高的新颖性。 惊喜度新颖性和惊喜度区别是，比如给出的推荐影片与用户历史兴趣相关大但用户不了解这个影片，这叫做新颖性；给出的推荐影片与历史兴趣相关性低，但用户看后觉得不错，这叫惊喜度。定义惊喜度需要首先定义推荐结果和用户历史上喜欢的物品的相似度，其次需要定义用户对推荐结果的满意度，目前并没有公认的指标定义。 信任度：度量推荐系统的信任度只能通过问卷调查的方式，询问用户是否信任推荐系统的推荐结果。 实时性一方面，推荐系统需要实时地更新推荐列表来满足用户新的行为变化。比如，当一个用户购买了iPhone，如果推荐系统能够立即给他推荐相关配件，那么肯定比第二天再给用户推荐相关配件更有价值。很多推荐系统都会在离线状态每天计算一次用户推荐列表，然后于在线期间将推荐列表展示给用户。这种设计显然是无法满足实时性的。与用户行为相应的实时性，可以通过推荐列表的变化速率来评测。如果推荐列表在用户有行为后变化不大，或者没有变化，说明推荐系统的实时性不高。另一方面是推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。 健壮性：防攻击 时间多样性推荐系统每天推荐结果的变化程度被定义为推荐系统的时间多样性，如果用户没有行为:(1)在生成推荐结果时加入一定的随机性,比如从推荐列表前20个结果中随机挑选10个结果展示给用户，或者按照推荐物品的权重采样10个结果展示给用户;(2)记录用户每天看到的推荐结果，然后在每天给用户进行推荐时，对他前几天看到过很多次的推荐结果进行适当地降权;(3)每天给用户使用不同的推荐算法。可以设计很多推荐算法，比如协同过滤算法、内容过滤算法等，然后在每天用户访问推荐系统时随机挑选一种算法给他进行推荐。 3. 协同过滤算法3.1 UserCF 原理用户相似度通过余弦相似度来计算，通过建立物品到用户的倒排表来简化计算。然后利用UserCF算法算出用户u和物品i的兴趣度(利用了和用户u最相似的K个用户)，然后对兴趣度排序，推荐 Top N。改进的用户协同过滤，计算用户相似度后，使用UserCF算法可以加上一定的惩罚项，惩罚那些热门商品的权值，冷门物品更能说明用户的相似度，这种算法叫做User-IIF算法。 特点(1)可以让用户发现新奇的物品(2)适用于用户较少的场合，否则计算用户相似度矩阵代价很大(3)时效性较强，用户个性化兴趣不太明显的领域,比如新闻(4)用户行为实时性低,用户有新行为，不一定造成推荐结果的立即变化(5)冷启动，在新用户对很少的物品产生行为后，不能立即对他进行个性化推荐，因为用户相似度表是每隔一段时间离线计算的。有些场景下，比如新闻领域，用户除了推荐列表，总可以通过其他渠道看到其他新的新闻，从而这方面冷启动问题不敏感。其他场景，要根据物品的内容属性，推荐给喜欢类似物品的用户。(6)很难提供令用户信服的推荐解释(7)一般一天计算一次用户相似表(8)利用时间的特性，加上衰减因子 应用新闻推荐使用UserCF算法，一是因为新闻场景的个性化是粗粒度的，二是因为如果使用ItemCF算法，需要不断更新物品相似矩阵，一般一天更新一次，而新闻出来的速度非常快，新闻领域接受不了这个速度，UserCF则更新比较慢。在抓住热点和时效性的同时，UserCF也照顾了一定程度的个性化。除了新闻领域，其他很多场景用ItemCF算法更多一些。 3.2 ItemCF 原理基于物品的协同过滤算法，计算物品相似度不是利用物品的内容属性来计算，而是通过分析所有用户的行为记录计算物品之间的相似度。该算法认为，物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品B。相似度可以计算为，喜欢物品i的用户中有多少比例的用户也喜欢物品j，可以建立倒排表来简化计算。得到了物品相似度后，计算用户u对物品i的兴趣度，通过和(物品i最相似的K个集合和用户喜欢的物品集合交集)来计算，然后根据兴趣度排名。 特点(1)更加个性化，是用户历史兴趣的体现(2)适用于物品数明显小于用户数的场合，否则计算物品相似度矩阵代价很大(3)长尾物品丰富，用户个性化需求强烈的领域(4)实时性，用户有新行为，一定会导致推荐结果的实时变化(5)冷启动，频繁更新物品相似表(半小时，基于物品内容属性)(6)利用用户的历史行为给用户做推荐解释，可以令用户比较信服(7)一般一天计算一次物品相似表(基于用户行为计算)(8)利用时间的特性，加上衰减因子 3.3 内容过滤算法主要是对文本内容向量化及NLP方面的知识，计算TF-IDF，然后再利用余弦相似度等来计算内容相似度。 3.4 隐语义模型 原理隐语义模型LFM(Latent Factor Model),原始矩阵的每一行代表每一个用户u，矩阵的每一列代表着物品i。初始情况下，如果u点击了i，则u,i置1，否则置0。通过最小化损失函数，求得矩阵p(用户u和类别k的兴趣关系)和矩阵q(类别k中具体的物品i所占的比例关系)，p*q则代表用户u和i的兴趣度，然后按照兴趣度排名，取top N即可，相关的名词有LSI、pLSA、LDA和Topic Model。 常见问题隐性反馈数据集上应用LFM解决TopN推荐的第一个关键问题就是如何给每个用户生成负样本，需要采样负样本。一，采样负样本时，要选取那些很热门，而用户却没有行为的物品；二，保证正负样本差不多数量相等。 3.5 基于图的算法基于图的模型，将用户物品用二分图表示，用户在左，物品在右，如果用户点击了物品，则连接左右两点，否则不连接。则此时，求解u和i的兴趣度，则是求两个节点的相关性。相关性可以由改进的PersonalRank算法求解。 3.6 LFM和UserCF/ItemCF的对比 离线计算时，用户数M,物品数N,UserCF的空间复杂度为O(MM),ItemCF空间复杂度为O(NN)。LFM类别个数为K时，空间复杂度为O(F*(N+M)),M和N很大时，LFM节省内存。 离线计算时，总体上LFM、UserCF、ItemCf时间复杂度差不多，LFM稍微高些，主要是有很次迭代。 实时推荐，ItemCF会随着用户实时行为实时更新推荐列表；LFM推荐时，需要计算用户对所有物品的兴趣权重返回topN，物品数很大时复杂度高不适合，同时因为生成推荐列表速度慢，不能实时推荐。 LFM无法提供很好的推荐解释，ItemCF可以 当数据集非常稀疏时，LFM的性能会明显下降，甚至不如UserCF和ItemCF的性能。 4. 冷启动问题 分类：成熟的系统中新用户冷启动(推荐什么)、成熟的系统中新物品冷启动(推荐给谁)、新开发的系统推荐什么 解决方案 非个性化推荐，热门物品 利用用户注册信息，年龄性别等 利用用户社交网络账号授权登录，推荐其好友喜欢的东西 登录时，给出一些物品让用户选择或者直接给出兴趣菜单，收集用户兴趣。给用户选择的物品选择也有算法，决策树的策略。 新加入的物品，利用其自身内容属性，计算相似的物品。常用算法有内容过滤算法ContentItemKNN、LDA等，通过不同话题的相似度计算物品的相似度，不同话题相似度常用指标KL散度。 专家给物品打特征标签，通过这些特征计算物品相似度。 5. 基于用户标签的推荐系统 利用标签做推荐系统，一个简单的算法是找到用户最常用的标签，然后在找这个标签下被用户最多观看的电影，可以利用TF-IDF的思想来改进。 简单算法中新用户和新物品的标签少，因此需要做标签拓展的工作，也就是找和标签相似的标签，即计算相似度。标签i,j的相似度简单计算，在用户行为中同时出现标签i,j的记录数/出现标签i的记录数。 标签系统中并不是所有标签都反应用户兴趣，比如”不喜欢”标签。因此需要做标签清理的工作，比如去除停用词、去除因分隔符不同的同义词等。 标签系统中基于图的推荐算法，一张图，三类顶点，一类是用户顶点，一类是物品顶点，一类是标签顶点。当用户对某个物品i打了某个标签a，连接用户-物品-标签a的边，并将权重设置为1。若边已存在，则权重加一。然后利用PersonalRank算法计算所有物品节点相对于当前用户节点在图上的相关性，然后按照相关性从大到小的排序，给用户推荐排名最高的N个物品。 参考1.项亮《推荐系统实践》]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SQL概述]]></title>
    <url>%2F2018%2F06%2F10%2FSQL%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1 数据库种类关系数据库，如MySQL、Oracle、SQL Server、DB2、PostgreSQL等；键值存储系统；XML数据库；内存数据库；面向列数据库。 2 数据库结构 RDBMS使用客户端/服务器这样的系统结构。 客户端向服务器端发送SQL语句来实现数据库的读写操作。 3 标准SQL 3.1 基本规则 不区分关键字的大小写 字符串和日期常数需要使用单引号’括起来。 标准sql中数据库名称、表名和列名名称必须以半角英文字母开头 3.2 数据类型 INTEGER，不能存储小数。 CHAR，定长字符串，向 CHAR(8) 类型的列中输入’abc’的时候，会以 ‘abc ‘（abc 后面有 5 个半角空格）的形式保存起来。 VARCHAR，可变长字符串，向 VARCHAR(8) 类型的列中输入字符串 ‘abc’ 的时候，保存的就是字符串’abc’。 DATE，该类型，可以直接比较大小，即前后顺序。 3.3 约束 NOT NULL PRIMARY KEY (column) 3.4 注释 1234# 单行-- 内容# 多行/* 内容 */ 3.5 SQL语句执行失败时都不会对表中数据造成影响。 4 基础语句 4.1 DDL(data definiation language) create 1234567891011create database shop;create table product(product_id char(4) not null,product_name varchar(100) not null,product_type varchar(32) not null,sale_price integer,purchase_price integer,regist_date date,primary key(product_id)); drop 1drop table product; alter 1234567# 添加多列alter table product add column (c1 char(3) not null,c2 varchar(10));# 删除多列alter table product drop column c1,drop column c2; rename 1rename table product to products; 4.2 DML(data manipulation language) 4.2.1 select distinct 123# distinct,所有NULL值会显示一个NULL# 只能用在第一个列名之前select distinct product_type,regist_date from products; where 123# where 子句可以使用计算表达式SELECT product_name FROM ProductWHERE sale_price - purchase_price &gt;= 500; null判断是否为NULL值:is NULL、is not NULL。 not 12# NOT运算符where c1&lt;c2等价于where not c1&gt;=c2 select * 和select 所有字段哪个好？ 123#（1）select *，需要数据库先Query Table Metadata For Columns，一定程度上为数据库增加了负担。但是实际上，两者效率差别不大。#（2）考虑到今后的扩展性。因为程序里面你需要使用到的列毕竟是确定的，SELECT * 只是减少了一句 SQL String 的长度，并不能减少其他地方的代码。#（3）使用SELECT *，不能设置列的显示顺序。 别名别名可以使用中文，使用中文时需要用双引号（”）括起来。 可以添加常数（字符串、数字、日期）列 12SELECT '商品' AS string, 38 AS number, '2009-02-24' AS date,product_id, product_name FROM Products; 算术运算符 12# 如果某列某行值是NULL，进行算术运算时，结果全是NULLSELECT sale_price * 2 AS "sale_price_x2" FROM Product; FROM 字句有时不是必须的 1SELECT (100 + 200) * 3 AS calculation; 不等于运算符 &lt;&gt; 字符串类型的数据原则上按照字典顺序进行排序比如在如下字符串中 ‘1’ ‘3’ ‘10’ ‘11’ ‘222’，大于字符’2’的有’3’ ‘222’，字符串内从左往右开始每个字符和2比。 4.2.2 insert 12345# 单行insert into product values ('0001','T恤','衣服',1000,500,'2009-09-20');# 多行INSERT INTO product VALUES ('0002', '打孔器', '办公用品', 500, 320, '2009-09-11'),('0003', '运动T恤', '衣服', 4000, 2800, NULL)# insert ... select ... 语法，select子句 4.2.3 update … set 12345# where子句放在set子句之后UPDATE ProductSET sale_price = sale_price * 10,purchase_price = purchase_price / 2WHERE product_type = '厨房用具'; 4.2.4 delete 12# delete语句只能使用where子句delete from table1 where c1&lt;500; 4.2.5 truncate 12# 只能删除全部数据，但是速度比delete快truncate table1; 4.2.6 事务 4.2.6.1 语法mysql每一条语句都括在事务的开始语句和结束语句之中。 123start transaction;... commit; 4.6.6.2 ACID特性原子性atomicity、一致性consistency、隔离性isolation和持久性durability 4.3 DCL(data controll language)commit、rollback、grant和revoke等。 5 聚合与排序 count 123456# count(*)，计算所有行数，包括全NULL行select count(*) from table1;# 计算某个字段非NULL行数select count(c1) from table1;# 计算某个字段不重复行数select count(distinct c1) from table1; group by 使用group by分组时，select子句只能选择常数、聚合函数以及分组列元素，其他列元素不可选 group by子句结果是随机的 having 只有SELECT子句和HAVING子句（以及ORDER BY子句）中能够使用聚合函数。 HAVING 子句必须写在 GROUP BY 子句之后，having的条件一般是select后的常数、聚合函数，分组列也可以，但分组列更多在where子句后限制 where子句和having子句的执行速度：1，使用count()等聚合函数时，需要排序，排序很耗时，where子句提前减少了数据量；2，where子句可以创建索引，提高速度 order by order by书写顺序放在having后 由于 ASC 和 DESC 这两个关键字是以列为单位指定的，因此可以同时指定一个列为升序，指定其他列为降序。 多个排序键规则是优先使用左侧的键，如果该列存在相同值的话，再接着参考右侧的键。 NULL值在排序时，mysql中全部排在前面 ORDER BY子句中可以使用SELECT子句中未使用的列和聚合函数。 聚合函数会将NULL行排除在外，然后再计算 sum()/avg()只能适用于数值型，但max()/min()可以适用非数值型排序类字段 6 复杂查询 6.1 视图 创建删除 1234# 创建 create view temp1 as select ...# 删除drop view temp1 视图保存的是sql语句，执行时创建临时表优点：1，不用存储在物理设备上，节约空间；2，写复杂sql时，常用的sql保存视图，频繁调用时节约时间。 多重视图会降低 SQL 的性能，尽量避免 定义视图时不能使用ORDER BY子句 更新视图时，有些限制，比如要求一张表、未使用distinct/group by/having等子句 6.2 子查询 6.2.1 就是一段完整的sql语句 6.2.2 尽量避免使用多层嵌套的子查询 6.2.3 标量子查询一次返回一个元素，即1行1列；可以适用于多种关键词子句中。 12345# 查询出销售单价高于平均销售单价的商品# where子句后不能使用聚合函数AVG()，所以只能使用标量子查询SELECT product_name, sale_priceFROM ProductWHERE sale_price &gt; (SELECT AVG(sale_price) FROM Product); 6.2.4 关联子查询 123456789101112# 查询出各商品种类中高于该商品种类的平均销售单价的商品SELECT product_type, product_name, sale_priceFROM Product AS P1WHERE sale_price &gt; (SELECT AVG(sale_price) FROM Product AS P2WHERE P1.product_type = P2.product_typeGROUP BY product_type);\# 添加一列，表示各商品种类平均价格select product_id,product_name,product_type,sale_price, (select avg(sale_price) from product t2 where t1.product_type=t2.product_type GROUP BY product_type) as avg_sale_price from product t1; 7 函数、谓词、CASE表达式 7.1 函数 7.1.1 算数函数 ABS(-10),结果为10 MOD(7,3),结果为1 round(5.555,2),结果为5.55 7.1.2 字符串函数 concat(str1,str2)select str1,str2,concat(str1,str2) from table1; length(str1)mysql以字节为单位，1个汉字两个字节，长度为2select str1,length(str1) from table1; lower(str1) upper(str1) repalce(str1,str2,str3)str1为abc太郎，str2为abc，str3为ABC，结果为ABC太郎 substring(str1 from 2 for 2)substring（字符串 from 起始位置 for 截取的字符个数） 7.1.3 日期函数 current_date返回当前日期2018-06-03select current_date; current_time返回当前时间 14:13:06 current_timestamp返回当前日期时间 2018-06-03 14:14:59 extract(日期元素 from 日期)12select extract(year from current_timestamp)select extract(year from current_time) 7.1.4 转换函数 cast(转换前的值 as 数据类型)select cast(product_id as signed integer) from product; coalesce(str1,’a’)将str1列中的NULL值填充’a’select coalesce(str1,’a’) from table1 7.2 谓词 like前方一致，例如以ddd开头，’ddd%’；中间一致，例如包含ddd，’%ddd%’；后方一致，例如以ddd结尾，’%ddd’。其中，%是代表“0字符以上的任意字符串”，_是代表“任意 1 个字符” between … and …包括临界值 is null、is not null in、not inselect * from table1 where price in (200,300,400)in 后面可以接子查询 exist左侧没有任何参数，右侧书写 1 个参数，该参数通常都会是一个关联子查询。子查询中的SELECT * 是个习惯不重要，exist只关心记录是否存在，因此返回哪些列都没有关系。12345678SELECT product_name, sale_priceFROM Product AS PWHERE NOT EXISTS (SELECT *FROM ShopProduct AS SPWHERE SP.shop_id = '000A'AND SP.product_id = P.product_id); 7.3 CASE表达式 语法 123456789# 求值表达式可以将其看作使用 =、 != 或者 LIKE、 BETWEEN 等谓词编写出来的表达式# 求值表达式为真，执行then后面的表达式，返回一个元素，执行结束；不为真，执行下面的when重复，都不为真，执行else# 使用分支众多的 CASE表达式编写几十行代码的情况也并不少见CASE WHEN &lt;求值表达式&gt; THEN &lt;表达式&gt; WHEN &lt;求值表达式&gt; THEN &lt;表达式&gt; WHEN &lt;求值表达式&gt; THEN &lt;表达式&gt; ···ELSE &lt;表达式&gt;END 示例原表输出结果1代码1为 1234567891011# ELSE 子句也可以省略不写，这时会被默认为 ELSE NULL，但建议写。# end不能省略SELECT product_name, CASE WHEN product_type = '衣服' THEN concat('A ：',product_type) WHEN product_type = '办公用品' THEN concat('B ：',product_type) WHEN product_type = '厨房用具' THEN concat('C ：',product_type) ELSE NULL END AS abc_product_typeFROM Product 输出结果2代码2为 12345678SELECT SUM(CASE WHEN product_type = '衣服' THEN sale_price ELSE 0 END) AS sum_price_clothes, SUM(CASE WHEN product_type = '厨房用具' THEN sale_price ELSE 0 END) AS sum_price_kitchen, SUM(CASE WHEN product_type = '办公用品' THEN sale_price ELSE 0 END) AS sum_price_officeFROM Product; 8 集合运算 union 12345678910111213141516171819# 集合运算符会除去重复的记录# union并列两边选取的字段列数要相同，类型要相同select product_id,product_name from product unionselect product_id,product_name from product2;# 可以使用任何形式的 SELECT 语句，WHERE、 GROUP BY、 HAVING 等子句都可以使用，但ORDER BY子句只需要且只能在最后使用一次select product_id,product_name from product where product_type = '厨房用具'unionselect product_id,product_name from product2where product_type = '厨房用具'order by product_id;# 保留重复行，加上all# 除了保留重复行外，UNION ALL 和UNION的不同之处在于它不会对结果进行排序，因此比UNION 的性能更好。select product_id,product_name from product union allselect product_id,product_name from product2; inner join=joinon子句必须书写在 from和 where之间 left outer join=left join 12345678910# 生成固定行数的单据时，就需要使用外联结。# left 表示左边的表是主表，结果包含主表的全部数据，另一张表没有对应的默认NULLselect a.shop_id,a.shop_name,a.product_id,b.product_name,b.sale_pricefrom shopproduct aleft joinproduct b on a.product_id=b.product_id;# left和right功能一样，常使用left right outer join=right joinright表示右边的表是主表，结果包含主表的全部数据，另一张表没有对应的默认NULL cross join这种联结在实际业务中并不会使用。两个表的行数分别为m和n，则笛卡尔积结果为mn行。集合运算中的除法通常称为关系除法。除法运算是集合运算中最复杂的运算，但是其在实际业务中的应用十分广泛，因此希望大家能在达到中级以上水平时掌握其使用方法。 9 SQL高级处理 9.1 窗口函数 概述MySQL版本5.7 不支持该功能。语法：”窗口函数 OVER (PARTITION BY 某列 ORDER BY 某列)”,其中”PARTITION BY 某列”可省略。窗口函数也称为 OLAP函数 ，online analytical processing，实时分析处理。PARTITION BY 能够设定排序的对象范围,称之为窗口。ORDER BY 能够指定按照哪一列、何种顺序进行排序。窗口函数只能在SELECT子句中使用。能够作为窗口函数的聚合函数（SUM、 AVG、 COUNT、 MAX、 MIN）。专用窗口函数，RANK、 DENSE_RANK、 ROW_NUMBER 等。 rank() 12345# 5,5,5,6,rank排名1,1,1,4，跳过2,3名SELECT product_name, product_type, sale_price,rank OVER (PARTITION BY product_typeORDER BY sale_price) AS rankingFROM Product; dense_rank() 12345# 5,5,5,6,rank排名1,1,1,2，不跳过2名SELECT product_name, product_type, sale_price,dense_rank() OVER (PARTITION BY product_typeORDER BY sale_price) AS rankingFROM Product; row_number() 12345# 5,5,5,6,rank排名1,2,3,4SELECT product_name, product_type, sale_price,row_number() OVER (PARTITION BY product_typeORDER BY sale_price) AS rankingFROM Product; sum()等聚合函数 1234# 累计聚合函数，如累积和、累计平均值、累计最大最小值、累计行数等SELECT product_id, product_name, sale_price,SUM (sale_price) OVER (ORDER BY product_id) AS current_sumFROM Product; 移动平均 123456789101112131415161718# 前2行，加上本行，共计3行SELECT product_id, product_name, sale_price,AVG (sale_price) OVER (ORDER BY product_idROWS 2 PRECEDING) AS moving_avgFROM Product;# 后2行，加上本行，共计3行SELECT product_id, product_name, sale_price,AVG (sale_price) OVER (ORDER BY product_idROWS 2 following) AS moving_avgFROM Product;# 前1行和后1行，假设本行，共计3行SELECT product_id, product_name, sale_price,AVG (sale_price) OVER (ORDER BY product_idROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS moving_avgFROM Product; mysql不分组排序 1234# @c1定义变量，:= 是赋值select product_name, product_type, sale_price,@c1:=@c1+1 as row_numberfrom product,(select @c1:=0) morder by sale_price; mysql分组排序 1234567891011# 按product_type分组排序SELECT t.product_name, t.product_type, t.sale_price, COUNT(*) AS rank FROM product tLEFT JOIN product rON t.product_type = r.product_typeAND t.sale_price &gt;= r.sale_priceGROUP BY t.product_id ; 9.2 grouping运算符 rollup 12345# mysql仅支持rollup# 计算小计和合计值SELECT product_type, SUM(sale_price) AS sum_priceFROM ProductGROUP BY product_type WITH ROLLUP; grouping 12345678910111213141516# 分辨超级分组记录中的 NULL 和原始数据本身的 NULL SELECT GROUPING(product_type) AS product_type,GROUPING(regist_date) AS regist_date, SUM(sale_price) AS sum_priceFROM ProductGROUP BY ROLLUP(product_type, regist_date);# CAST(regist_date AS VARCHAR(16))，防止各个分支会分别返回日期类型和字符串类型的值，执行时就会发生语法错误SELECT CASE WHEN GROUPING(product_type) = 1THEN '商品种类 合计'ELSE product_type END AS product_type,CASE WHEN GROUPING(regist_date) = 1THEN '登记日期 合计'ELSE CAST(regist_date AS VARCHAR(16)) END AS regist_date,SUM(sale_price) AS sum_priceFROM ProductGROUP BY ROLLUP(product_type, regist_date); cube和rollup语法一样，直接替换CUBE，就是将 GROUP BY 子句中聚合键的“所有可能的组合”的汇总结果集中到一个结果中。因此，组合的个数就是 2的n次方（n 是聚合键的个数）。 grouping setsGROUPING SETS可以用于从 ROLLUP 或者 CUBE 的结果中取出部分记录。 参考1.SQL基础教程（第2版）([日]MICK 著)]]></content>
      <categories>
        <category>SQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python 正则表达式]]></title>
    <url>%2F2018%2F06%2F09%2FPython-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1 正则表达式模式1234567891011121314151617^ 匹配字符串的开头$ 匹配字符串的末尾. 匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。* 匹配0个或多个的表达式+ 匹配1个或多个的表达式? +或*后跟？表示非贪婪匹配，即尽可能少的匹配[...] [amk] 匹配 'a'，'m'或'k'[^...] [^abc] 匹配除了a,b,c之外的字符() 表示一个组\w 匹配字母数字及下划线\W 匹配非字母数字及下划线\s 匹配任意空白字符，等价于 [\t\n\r\f].\S 匹配任意非空字符\d 匹配任意数字，等价于 [0-9].\D 匹配任意非数字[^a-zA-Z0-9] 除了任何字母及数字[1-2][0-9]&#123;3&#125; 匹配1000-2999之间的数字，3表示前面重复3次，即[0-9][0-9][0-9] 2 re.match函数re.match(pattern, string, flags=0) pattern,匹配的正则表达式;string,要匹配的字符串;flags标志位，用于控制正则表达式的匹配方式，如re.I|re.M被设置成I和M标志,I是表示匹配对大小写不敏感，M是表示多行匹配、影响^和$ 匹配成功re.match方法返回一个对象，否则返回None。 123import reprint(re.match('www', 'www.runoob.com')) # 在起始位置匹配,输出&lt;_sre.SRE_Match object at 0x00000000020EC510&gt;print(re.match('com', 'www.runoob.com')) # 不在起始位置匹配，输出None group(num)或groups()匹配对象函数来获取匹配表达式 123456import reline = "Cats are smarter than dogs"matchObj = re.match( r'(.*) are (.*?) .*', line, re.M|re.I)# 正则表达式字符串前加上r，r的作用是让编译器忽略反斜杠；加了()表示分组，最后一个.*，没有加分号，所以不算分组。print matchObj.group(0)# 0表示用全部正则表达式去匹配的输出结果(可省略)，输出Cats are smarter than dogsprint matchObj.group(1)# 1表示用第一组正则表达式去匹配的输出结果，对第一组应输出Catsprint matchObj.group(2)# 2表示用第二组正则表达式去匹配的输出结果(需执行前面的即第一组正则表达式),第二组对应输出smarter 3 re.search函数re.search(pattern, string, flags=0) 扫描整个字符串并返回第一个成功的匹配，匹配成功返回一个对象，否则返回None。 12print(re.search('www', 'www.runoob.com'))# 输出&lt;_sre.SRE_Match object at 0x00000000024E9ED0&gt;print(re.search('com', 'www.runoob.com'))# 输出&lt;_sre.SRE_Match object at 0x00000000024E9ED0&gt; re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败返回None；而re.search匹配整个字符串，直到找到一个匹配，找不到返回None。 4 re.sub函数re.sub(pattern, repl, string, count=0, flags=0) repl,替换的字符串，也可为一个函数;count,模式匹配后替换的最大次数，默认0表示替换所有的匹配。12345678910111213phone = "2004-959-559 # 这是一个国外电话号码"# 删除字符串中的 Python注释,即将注释替换成空字符 print re.sub(r'#.*', "", phone)# 输出'2004-959-559 '# 删除非数字(-)的字符串，即将非数字的替换成空字符 print re.sub(r'\D', "", phone)# 输出'2004959559'# repl作为函数def double(matched): value = int(matched.group()) return str(value * 2)s = 'A23G4HFD543'print(re.sub(r'\d+', double, s))# 输出A46G8HFD1086 5 re.compile 函数re.compile(pattern,flags)，其对象可以直接使用match/search等方法 供match()和search()这两个函数使用。123456pattern = re.compile(r'\d+') # 用于匹配至少一个数字m = pattern.match('one12twothree34four') # 查找头部，没有匹配print m # 输出Nonem = pattern.match('one12twothree34four', 3, 10) # 从'1'的位置开始匹配，正好匹配print m # 返回一个 Match 对象，&lt;_sre.SRE_Match object at 0x10a42aac0&gt;print m.group(0),m.start(0),m.end(0),m.span(0) # 输出'12',3,5,(3, 5) 6 re.findall函数re.findall(string, pos, endpos)，返回所有匹配的结果 pos，起始位置；endpos，结束位置123pattern = re.compile(r'\d+') # 查找数字result2 = pattern.findall('run88oob123google456', 0, 10)print(result2)# ['88', '12'] 7 re.split函数re.split(pattern, string, maxsplit=0, flags=0)，将匹配的正则结果，作为分隔符分割字符串 maxsplit，分隔次数，maxsplit=1 分隔一次，默认为 0，不限制次数。1print re.split('\W+', 'runoob, runoob, runoob.')# ['runoob', 'runoob', 'runoob', '']]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas 降低占用内存方法]]></title>
    <url>%2F2018%2F06%2F08%2Fpandas-%E9%99%8D%E4%BD%8E%E5%86%85%E5%AD%98%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[检查DataFrame内存1data.info(memory_usage='deep') 检查每种类型大概用了多少内存 12345for dtype in ['float','int64','object']: selected_dtype = data.select_dtypes(include=[dtype]) mean_usage_b = selected_dtype.memory_usage(deep=True).mean() mean_usage_mb = mean_usage_b / 1024 ** 2 print("Average memory usage for &#123;&#125; columns: &#123;:03.2f&#125; MB".format(dtype,mean_usage_mb)) 定义方法，计算内存 1234567def mem_usage(pandas_obj): if isinstance(pandas_obj,pd.DataFrame): usage_b = pandas_obj.memory_usage(deep=True).sum() else: # we assume if not a df it's a series usage_b = pandas_obj.memory_usage(deep=True) usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes return "&#123;:03.2f&#125; MB".format(usage_mb) 数值类型int/float 12345678# code: raw_int=data.select_dtypes(include=['int64'])print mem_usage(raw_int)# 1658.00 MBdone_int=raw_int.apply(pd.to_numeric,downcast='integer')print mem_usage(done_int)# 362.00 MBcompare= pd.concat([raw_int.dtypes,done_int.dtypes],axis=1)compare.columns = ['before','after']compare.apply(pd.Series.value_counts) 123456# output:index before afterint8 NaN 66.0int16 NaN 8.0int32 NaN 21.0int64 95.0 NaN object类型，如果列取值不多，可以转化为category类型，用于可视化分析]]></content>
      <categories>
        <category>pandas</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[不平衡数据处理方式]]></title>
    <url>%2F2018%2F06%2F04%2F%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[惩罚正负样本的权重很多模型和算法中都有基于类别参数的调整设置，以scikit-learn中的SVM为例，通过在class_weight: {dict, ‘balanced’}中针对不同类别针对不同的权重，来手动指定不同类别的权重。如果使用其默认的方法balanced，那么SVM会将权重设置为与不同类别样本数量呈反比的权重来做自动均衡处理，计算公式为：n_samples / (n_classes * np.bincount(y)).比较简单高效。比如label=0样本数为10，label=1样本数为100,此时可以设置class_weight={0:1,1:0.1}来影响惩罚项系数C，进而影响损失函数的求解，使得权重大样本的惩罚项少，权重小的样本惩罚项多。 特征通过选择显著性特征 采用boosting/bagging等算法 抽样 过采样过采样比较广泛，最直接的方法是简单复制少数类样本形成多条记录，也可使用smote算法 123456789# 过抽样处理库SMOTEfrom imblearn.over_sampling import SMOTE # 建立SMOTE模型对象model_smote = SMOTE() # 输入数据并作过抽样处理x_smote_resampled, y_smote_resampled = model_smote.fit_sample(train,train_y) x_smote_resampled = pd.DataFrame(x_smote_resampled, columns=train.columns)y_smote_resampled = pd.DataFrame(y_smote_resampled,columns=['label']) 欠采样最直接的方法是随机地去掉一些多数类样本来减小多数类的规模 1234567# 欠抽样处理库RandomUnderSamplerfrom imblearn.under_sampling import RandomUnderSampler# 建立RandomUnderSampler模型对象model_RandomUnderSampler = RandomUnderSampler() # 输入数据并作过抽样处理x_RandomUnderSampler_resampled, y_RandomUnderSampler_resampled =model_RandomUnderSampler.fit_sample(train,train_y)]]></content>
      <categories>
        <category>数据预处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[时间序列问题转回归问题简述]]></title>
    <url>%2F2018%2F05%2F19%2F%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%BD%AC%E5%9B%9E%E5%BD%92%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[这种场景，一般是随着时间预测销量，比如销量预测、电量预测等原理参考链接 12345678# 核心函数import pandas as pdfrom pandas import DataFramefrom pandas import concatdir = '../input/'train = pd.read_table(dir + 'train_20171215.txt',engine='python')train.head(10) date day_of_week brand cnt 0 1 3 1 20 1 1 3 5 48 2 2 4 1 16 3 2 4 3 20 4 3 5 1 1411 5 3 5 2 811 6 3 5 3 1005 7 3 5 4 773 8 3 5 5 1565 9 4 6 1 1176 1234567891011121314151617181920212223242526272829303132def series_to_supervised(data, n_in=1, n_out=1, dropnan=True): """ Frame a time series as a supervised learning dataset. Arguments: data: Sequence of observations as a list or NumPy array. n_in: Number of lag observations as input (X). n_out: Number of observations as output (y). dropnan: Boolean whether or not to drop rows with NaN values. Returns: Pandas DataFrame of series framed for supervised learning. """ n_vars = 1 if type(data) is list else data.shape[1] df = DataFrame(data) cols, names = list(), list() # input sequence (t-n, ... t-1) for i in range(n_in, 0, -1): cols.append(df.shift(i)) names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)] # forecast sequence (t, t+1, ... t+n) for i in range(0, n_out): cols.append(df.shift(-i)) if i == 0: names += [('var%d(t)' % (j + 1)) for j in range(n_vars)] else: names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)] # put it all together agg = concat(cols, axis=1) agg.columns = names # drop rows with NaN values if dropnan: agg.dropna(inplace=True) return agg 1234time_cnt = list(train['cnt'].values)# nin 前看 nout后看 这个题目需要前看time2sup = series_to_supervised(data=time_cnt,n_in=276,dropnan=True) time2sup.head(10) var1(t-276) var1(t-275) var1(t-274) var1(t-273) … var1(t-4) var1(t-3) var1(t-2) var1(t-1) var1(t) 276 20 48 16 20 … 407 237 200 535 384 277 48 16 20 1411 … 237 200 535 384 303 278 16 20 1411 811 … 200 535 384 303 314 279 20 1411 811 1005 … 535 384 303 314 176 280 1411 811 1005 773 … 384 303 314 176 310 281 811 1005 773 1565 … 303 314 176 310 283 282 1005 773 1565 1176 … 314 176 310 283 261 283 773 1565 1176 824 … 176 310 283 261 342 284 1565 1176 824 802 … 310 283 261 342 171 285 1176 824 802 1057 … 283 261 342 171 242 12print train.shapeprint time2sup.shape (4773, 4)(4497, 277)4497+276=4773，最后一列var1(t)则为label,表示此刻的值]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[正负样本的定义问题]]></title>
    <url>%2F2018%2F05%2F05%2F%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E7%9A%84%E5%AE%9A%E4%B9%89%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[sklearn与spark ml对于正负样本的定义有区别 sklearn横行为预测值，纵行为真实值 0为负样本、1为正样本,计算精确率、召回率，是计算类别为1的样本 预测值 0 1 真实值 0 x x 1 x x spark ml1为负样本、0为正样本,计算精确率、召回率，是计算类别为0的样本]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[特征重要性计算]]></title>
    <url>%2F2018%2F04%2F20%2F%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[特征重要性可以用来做模型可解释性，这在风控等领域是非常重要的方面。 xgboostxgboost实现中Booster类get_score方法输出特征重要性，其中importance_type参数支持三种特征重要性的计算方法 1.importance_type=weight（默认值），特征重要性使用特征在所有树中作为划分属性的次数 2.importance_type=gain，特征重要性使用特征在作为划分属性时loss平均的降低量 3.importance_type=cover，特征重要性使用特征在作为划分属性时对样本的覆盖度]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征重要性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT生成特征]]></title>
    <url>%2F2018%2F03%2F14%2FGBDT%E7%94%9F%E6%88%90%E7%89%B9%E5%BE%81%2F</url>
    <content type="text"><![CDATA[原理通过GBDT模型训练出很多棵树，每棵树的节点值是每个样本在这棵树上的预测值。同时，将样本在每棵树的节点的位置保存起来，比如[1,3,5]，表示样本在第一棵树节点位置为1，同理，在第二棵树的节点位置为3。对每个节点位置onehot编码，比如第一棵树共有3个节点，则1为001,同理，比如第二棵树有4个节点，则3为0010，假设第三棵树为5个节点，则[1,3,5]为[0 0 1 0 0 1 0 0 0 0 0 1] 代码123456789from sklearn.datasets import make_classificationfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.preprocessing import OneHotEncoderX, y = make_classification(n_samples=10) print yprint X.shapeX 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[0 1 1 0 0 1 1 1 0 0](10L, 20L)array([[-1.46218297e+00, -4.18333502e-01, -3.59274601e-02, 4.51595239e-01, -1.51047633e+00, 5.07112394e-01, 7.38097000e-01, -3.02315469e-01, 4.26175119e-01, 4.01702307e-02, -6.60368519e-01, -1.00253339e+00, 4.55802942e-01, -4.31784177e-01, 5.61925740e-02, -9.87095232e-01, -3.60589437e-01, -8.20339651e-02, -1.51267169e+00, -1.75602670e+00], [-1.38065657e-01, 1.51125229e+00, -1.16786498e+00, 8.96428602e-01, 2.24770779e-02, 2.75832881e-01, 8.84555986e-01, 1.90990870e-01, 1.50974562e+00, 6.32858976e-01, 7.43984848e-01, 6.23124869e-01, 2.75529340e-01, 4.13913270e-01, -5.69452199e-01, 9.92345363e-01, -6.58511532e-01, -1.04902254e+00, 4.17471392e-01, -1.69380460e+00], [ 7.94578797e-01, 6.91286961e-01, -8.36134614e-01, 1.35232255e+00, 8.19800516e-01, -6.63009090e-01, -3.93857189e-01, 1.21970975e+00, 9.45892797e-01, 8.78534488e-01, -6.82525652e-01, -4.65135487e-01, 7.91431577e-01, 3.54382290e-01, 8.96197192e-02, -8.50444786e-01, 5.54008693e-01, -2.22658663e-01, 2.60990234e-01, 5.36795887e-01], [ 2.35619886e-03, 2.00775837e-01, 4.87206937e-02, -4.86582074e-02, -1.93014354e+00, 3.94423848e-01, 8.43146268e-01, -1.18764902e+00, 1.21507109e+00, -5.82489166e-02, 2.91817058e-01, 2.63882786e-01, -8.09303986e-01, 2.13007401e+00, 2.03539722e+00, 2.50978548e-01, -5.79211447e-01, -2.27030861e+00, -1.32198712e+00, 3.58828134e-01], [ 1.76208221e-01, -6.44503388e-01, -1.35005640e-01, 2.57927723e-01, -1.15227919e+00, -1.04384378e+00, -5.25599357e-01, 2.16430369e+00, 1.14631323e+00, -4.86738371e-01, 3.56177943e-01, 4.25940193e-01, -2.46271932e-01, 8.02966687e-01, -9.16757374e-01, 3.92150341e-01, 8.89340812e-01, 8.89122664e-01, -6.91321300e-01, 3.60691085e-01], [-9.17667327e-01, -2.19718742e-01, 3.08231099e-01, -1.20865300e+00, 9.52723078e-02, -2.13289317e+00, 8.23448746e-01, 1.84966961e-01, 2.35626420e+00, -5.90061668e-01, -9.91453561e-01, 2.62985532e-01, -9.96616193e-01, -3.90777590e-01, 3.89293149e-01, -1.31349623e+00, 2.72401261e-02, -2.54414046e-01, -4.60378536e-01, 1.23080237e+00], [-8.28192448e-01, 1.15639869e+00, 2.78606672e-02, 9.65889005e-02, 1.68886757e+00, -3.25775102e-01, 1.75717716e-01, 1.59902401e-01, -6.55490597e-01, 2.95479360e-01, 6.60409198e-01, -2.70659454e+00, 7.21371599e-01, 5.61757431e-01, 1.22378486e+00, 9.99869289e-01, 2.21082939e+00, 1.31353776e-01, 1.64938642e+00, 3.81579172e-02], [-1.18357533e+00, 1.47944412e+00, 2.82847558e-01, 1.01023062e+00, 1.54087073e+00, 1.88003713e-02, -2.54038797e-01, -1.24068833e+00, -3.31394808e-02, 1.10512704e+00, -7.21359030e-01, 5.02536950e-01, 4.86264732e-01, -2.48485472e+00, 7.36671531e-01, -8.50743359e-01, 1.42773431e+00, -3.85276831e-01, 7.92621022e-01, 8.04302650e-01], [ 5.00298505e-01, 4.99864304e-01, -4.43128177e-01, -6.80877761e-01, -2.30634433e+00, -4.18106882e-01, -8.34288871e-01, 1.68655058e+00, -2.32944124e+00, -3.10646642e-01, -3.88910822e-01, -9.13217680e-01, 1.01294499e+00, 1.13638795e-01, 3.46336445e-01, -6.82351046e-01, -3.25986042e-01, -1.87458285e-01, -1.97646950e+00, -1.29381917e+00], [ 2.39415433e-01, 1.07290006e+00, 1.15550474e+00, -1.15201307e+00, -5.78108383e-01, -3.80187570e-01, -1.28861486e+00, -1.32918430e+00, -5.28881163e-01, 1.96556881e-01, -8.61984192e-01, -2.17046046e+00, -8.82487188e-02, 5.60887804e-01, 1.31724634e+00, -1.18909996e+00, -1.67108757e+00, 8.37225262e-01, -9.06706808e-01, -6.15740923e-01]]) 123456X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)gbc = GradientBoostingClassifier(n_estimators=2)one_hot = OneHotEncoder()gbc.fit(X_train, y_train)X_train_new = one_hot.fit_transform(gbc.apply(X_train)[:, :, 0])print (X_train_new.todense()) 12345[[0. 1. 0. 1.] [0. 1. 0. 1.] [1. 0. 1. 0.] [1. 0. 1. 0.] [1. 0. 1. 0.]]]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
      <tags>
        <tag>GBDT生成特征</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程简述]]></title>
    <url>%2F2018%2F03%2F07%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1 数据变换 1.1 简单函数变换:将不正态分布的数据变为正态分布，如平方、根号、log和差分等，log和差分有时可以将时间序列非平稳序列转化为平稳序列 1.2 规范化:归一化、标准化、范数化、排序、小数定标化等 1.3 连续变量离散化:等值划分、等量划分、聚类划分 1.4 构造特征:找到特征之间的关系，构造和label相关性更强的特征 1.5 小波变换，提取特征产生新的向量 2 特征选择 2.1 策略特征选择本质上是一个特征组合优化的问题，主要分三种策略，分别是完全搜索、启发式搜索和随机搜索等。 2.1.1 完全搜索，如广度优先搜索。 2.1.2 启发式搜索，如序列前向选择、序列后向选择、增L去R选择和双向搜索等等。 2.1.3 随机搜索:随机产生序列选择算法，即随机产生一个特征子集，然后在这个子集上执行SFS(sequential forward selection)和SBS算法;模拟退火算法，每次以一定概率接受比当前解更差的解，这个概率会随着时间逐渐降低;遗传算法，通过交叉、突变等操作繁殖出下一代特征子集，评分越高的特征子集被选中繁殖的概率越大。缺点，随机搜索算法依赖随机因素，有实验结果难重现。 2.2 类型特种选择算法有三种类型，分别是嵌入式特征选择、过滤式特征选择和封装式特征选择。 2.2.1 嵌入式特征选择，如决策树 2.2.2 过滤式特征选择，主要有四类，距离度量、信息度量、关联度度量以及一致性度量，具体来说有相关系数法、卡方分布，互信息法和信息增益等。 2.2.3 封装式特征选择，如LR、RF和XGBoost等 3 构造特征 特征组合(交叉)A特征需要做离散化处理，然后通过字符串拼接B特征，然后再做OneHot编码 GBDT构造特征通过GBDT模型训练出很多棵树，每棵树的节点值是每个样本在这棵树上的预测值。同时，将样本在每棵树的节点的位置保存起来，比如[1,3,5]，表示样本在第一棵树节点位置为1，同理，在第二棵树的节点位置为3。对每个节点位置onehot编码，比如第一棵树共有3个节点，则1为001,同理，比如第二棵树有4个节点，则3为0010，假设第三棵树为5个节点，则[1,3,5]为[0 0 1 0 0 1 0 0 0 0 0 1] 4 其他 不同模型对数据的依赖度LR适合拟合离散特征，GBDT适合拟合连续特征 连续性特征离散化等值划分(按照值域平分区间)和等量划分(按照样本个数每个区间样本数量一样)两种。 树模型不用归一化，标准化 特征监控对于重要的特征需要进行监控，主要看特征最大最小值是否发生变化、特征稳定性是否发生明显变化、特征的对于样本的覆盖率等等 PCA算法比如时速和秒速度，PCA可以去掉。通过求解协方差矩阵，然后分解，选择前p个特征向量，则进行了降维 VC维理论维度越高，可容许的复杂度越高。但是，一，容易过拟合;二，特征数量增加带来的训练、测试和存储开销很大;三，有些基于距离的算法，如KNN/Kmeans等，会影响算法精度;四，可视化分析的需要，一般二维三维展示]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python文本处理]]></title>
    <url>%2F2018%2F03%2F05%2Fpython%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B9%8Bgensim%2F</url>
    <content type="text"><![CDATA[LDA参考123456789101112131415161718192021222324252627282930313233from gensim import corpora,similarities,models import jieba # 训练样本 raw_documents = [ '0南京江心洲污泥偷排”等污泥偷排或处置不当而造成的污染问题，不断被媒体曝光', '1面对美国金融危机冲击与国内经济增速下滑形势，中国政府在2008年11月初快速推出“4万亿”投资十项措施', '2全国大面积出现的雾霾，使解决我国环境质量恶化问题的紧迫性得到全社会的广泛关注', '3大约是1962年的夏天吧，潘文突然出现在我们居住的安宁巷中，她旁边走着40号王孃孃家的大儿子，一看就知道，他们是一对恋人。那时候，潘文梳着一条长长的独辫', '4坐落在美国科罗拉多州的小镇蒙特苏马有一座4200平方英尺(约合390平方米)的房子，该建筑外表上与普通民居毫无区别，但其内在构造却别有洞天', '5据英国《每日邮报》报道，美国威斯康辛州的非营利组织“占领麦迪逊建筑公司”(OMBuild)在华盛顿和俄勒冈州打造了99平方英尺(约9平方米)的迷你房屋', '6长沙市公安局官方微博@长沙警事发布消息称，3月14日上午10时15分许，长沙市开福区伍家岭沙湖桥菜市场内，两名摊贩因纠纷引发互殴，其中一人被对方砍死', '7乌克兰克里米亚就留在乌克兰还是加入俄罗斯举行全民公投，全部选票的统计结果表明，96.6%的选民赞成克里米亚加入俄罗斯，但未获得乌克兰和国际社会的普遍承认', '8京津冀的大气污染，造成了巨大的综合负面效应，显性的是空气污染、水质变差、交通拥堵、食品不安全等，隐性的是各种恶性疾病的患者增加，生存环境越来越差', '9 1954年2月19日，苏联最高苏维埃主席团，在“兄弟的乌克兰与俄罗斯结盟300周年之际”通过决议，将俄罗斯联邦的克里米亚州，划归乌克兰加盟共和国', '10北京市昌平区一航空训练基地，演练人员身穿训练服，从机舱逃生门滑降到地面', '11腾讯入股京东的公告如期而至，与三周前的传闻吻合。毫无疑问，仅仅是传闻阶段的“联姻”，已经改变了京东赴美上市的舆论氛围', '12国防部网站消息，3月8日凌晨，马来西亚航空公司MH370航班起飞后与地面失去联系，西安卫星测控中心在第一时间启动应急机制，配合地面搜救人员开展对失联航班的搜索救援行动', '13新华社昆明3月2日电，记者从昆明市政府新闻办获悉，昆明“3·01”事件事发现场证据表明，这是一起由新疆分裂势力一手策划组织的严重暴力恐怖事件', '14在即将召开的全国“两会”上，中国政府将提出2014年GDP增长7.5%左右、CPI通胀率控制在3.5%的目标', '15中共中央总书记、国家主席、中央军委主席习近平看望出席全国政协十二届二次会议的委员并参加分组讨论时强调，团结稳定是福，分裂动乱是祸。全国各族人民都要珍惜民族大团结的政治局面，都要坚决反对一切危害各民族大团结的言行' ] corpora_documents = [] #分词处理 for item_text in raw_documents: item_seg = list(jieba.cut(item_text)) corpora_documents.append(item_seg) # 生成字典和向量语料 dictionary = corpora.Dictionary(corpora_documents)print type(dictionary)print(dictionary) 12&lt;class 'gensim.corpora.dictionary.Dictionary'&gt;Dictionary(384 unique tokens: [u'\u8981', u'CPI', u'\u5931\u8054', u'\u901a\u8fc7', u'\u73af\u5883\u8d28\u91cf']...) 12345# 稀疏表达方式，实际上产生的是16*384的词频矩阵，16是文档数目，384是词语数目# 通过下面一句得到语料中每一篇文档对应的稀疏向量（这里是bow向量） corpus = [dictionary.doc2bow(text) for text in corpora_documents] # 向量的每一个元素代表了一个word在这篇文档中出现的次数 print(corpus) 1[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)], [(1, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1)], [(13, 3), (18, 1), (19, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1)], [(13, 3), (19, 5), (32, 1), (34, 1), (50, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 2), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 2), (87, 1), (88, 2), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1)], [(13, 2), (19, 2), (23, 1), (27, 1), (32, 2), (42, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1)], [(1, 1), (13, 2), (19, 1), (24, 1), (32, 1), (42, 1), (97, 2), (98, 2), (110, 1), (111, 1), (112, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1)], [(16, 1), (19, 4), (63, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 2)], [(13, 3), (19, 3), (57, 1), (79, 1), (103, 1), (134, 1), (184, 1), (185, 1), (186, 1), (187, 3), (188, 2), (189, 2), (190, 1), (191, 1), (192, 1), (193, 2), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1)], [(13, 5), (14, 1), (17, 1), (19, 4), (86, 2), (129, 1), (206, 1), (207, 3), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1)], [(1, 1), (13, 2), (19, 4), (24, 1), (27, 1), (32, 1), (34, 1), (45, 1), (124, 1), (136, 1), (171, 1), (173, 1), (187, 2), (188, 1), (189, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1)], [(19, 2), (149, 1), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 1), (260, 2), (261, 1), (262, 1)], [(1, 1), (13, 4), (19, 3), (21, 1), (24, 1), (27, 1), (65, 1), (86, 1), (129, 1), (263, 1), (264, 1), (265, 2), (266, 1), (267, 2), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1), (280, 1), (281, 1)], [(13, 1), (19, 4), (27, 1), (32, 1), (63, 1), (171, 1), (173, 1), (176, 1), (206, 1), (248, 1), (252, 2), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), (295, 1), (296, 1), (297, 1), (298, 1), (299, 1), (300, 1), (301, 2), (302, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1)], [(1, 1), (13, 1), (19, 3), (24, 1), (45, 1), (63, 2), (143, 1), (173, 1), (249, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1), (313, 1), (314, 1), (315, 1), (316, 1), (317, 1), (318, 1), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (324, 2), (325, 1), (326, 1), (327, 1), (328, 1), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1), (334, 1)], [(1, 1), (13, 2), (19, 1), (24, 1), (28, 1), (32, 2), (34, 1), (48, 1), (102, 1), (150, 1), (207, 1), (243, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 1), (340, 1), (341, 1), (342, 1), (343, 1), (344, 1), (345, 1), (346, 1), (347, 1), (348, 1)], [(13, 3), (19, 3), (48, 1), (65, 1), (86, 2), (151, 1), (172, 1), (207, 2), (316, 1), (349, 1), (350, 1), (351, 1), (352, 2), (353, 1), (354, 1), (355, 1), (356, 1), (357, 1), (358, 1), (359, 1), (360, 1), (361, 1), (362, 1), (363, 1), (364, 1), (365, 1), (366, 1), (367, 1), (368, 1), (369, 2), (370, 1), (371, 1), (372, 1), (373, 1), (374, 1), (375, 2), (376, 1), (377, 1), (378, 1), (379, 1), (380, 1), (381, 2), (382, 1), (383, 2)]] 1234# corpus是一个返回bow向量的迭代器。下面代码将完成对corpus中出现的每一个特征的IDF值的统计工作 tfidf_model = models.TfidfModel(corpus) corpus_tfidf = tfidf_model[corpus] corpus_tfidf 1&lt;gensim.interfaces.TransformedCorpus at 0xaa15eb8&gt; 12345678similarity = similarities.Similarity('Similarity-tfidf-index', corpus_tfidf, num_features=600) test_data_1 = '北京雾霾红色预警' test_cut_raw_1 = list(jieba.cut(test_data_1)) # ['北京', '雾', '霾', '红色', '预警'] test_corpus_1 = dictionary.doc2bow(test_cut_raw_1) # [(51, 1), (59, 1)]，即在字典的56和60的地方出现重复的字段，这个值可能会变化 similarity.num_best = 5 test_corpus_tfidf_1=tfidf_model[test_corpus_1] # 根据之前训练生成的model，生成query的IFIDF值，然后进行相似度计算 # [(51, 0.7071067811865475), (59, 0.7071067811865475)] print(similarity[test_corpus_tfidf_1]) # 返回最相似的样本材料,(index_of_document, similarity) tuples 1[(2, 0.3595932722091675)] 123456test_data_2 = '长沙街头发生砍人事件致6人死亡' test_cut_raw_2 = list(jieba.cut(test_data_2)) test_corpus_2 = dictionary.doc2bow(test_cut_raw_2) test_corpus_tfidf_2=tfidf_model[test_corpus_2] similarity.num_best = 3 print(similarity[test_corpus_tfidf_2]) # 返回最相似的样本材料,(index_of_document, similarity) tuples 1[(6, 0.19451555609703064), (13, 0.10124017298221588)]]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>gensim</tag>
        <tag>jieba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark DataFrame 常用函数及常见问题]]></title>
    <url>%2F2018%2F02%2F26%2FSpark%20DataFrame%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[函数123456# python:df['age'].value_counts()df.groupBy("age").count().show()# python:df.describe()df.describe().show()# python:df.fillna(df.mean())# spark:https://stackoverflow.com/questions/40057563/replace-missing-values-with-mean-spark-dataframe 问题 parquet格式错误 12//需注意Hive版本需要低于2.1.0SparkSession().config("spark.sql.hive.convertMetastoreParquet","false") 列字段存在但找不到 1SparkSession().config("spark.sql.parquet.filterPushdown", "false") 表字段过多不显示 SparkSession().config(&quot;spark.debug.maxToStringFields&quot;, &quot;100&quot;) spark ml算法管道predict产生的df字段，其中label对应predictedLabel(int类型)，indexedLabel对应prediction(double类型) spark ml模型评估，和sklearn不同，需要将我们关注的样本正样本定义为0，负样本为1，这样在计算精确率和召回率时才正确，一般所说的正样本和sklearn一样（正样本为1，负样本为0）。 spark 批量读取DataFrame文件时，有时候会出错，此时可以一个个文件读入，然后合并，即使这样还不行，那也找到了出现问题的文件在哪里。 命令行spark-shell –conf spark.sql.hive.convertMetastoreParquet=false 命令行启动]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[将jar包打包到集群运行]]></title>
    <url>%2F2018%2F01%2F06%2F%E5%B0%86jar%E5%8C%85%E6%89%93%E5%8C%85%E5%88%B0%E9%9B%86%E7%BE%A4%E8%BF%90%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[规范化，一般有4个文件夹，分别为bin、conf、jars和logs 运行 ./run.sh，不能直接使用run.sh，需指定当前目录 bin job-env.sh 12SPARK_HOME=/opt/cloudera/parcels/sparkHADOOP_CONF_DIR=/etc/hadoop/conf run.sh 1234567891011121314151617181920212223#!/usr/bin/env bashsource /etc/profileif [ -z "$&#123;JOB_HOME&#125;" ]; then export JOB_HOME="$(cd "`dirname "$0"`"/; pwd)"ficd $JOB_HOMEcd ..source bin/job-env.shexport HADOOP_CONF_DIR=/etc/hadoop/confnohup $SPARK_HOME/bin/spark-submit --master yarn \--deploy-mode client \--conf spark.yarn.maxAppAttempts=1 \--conf spark.default.parallelism=30 \--num-executors 3 \--executor-memory 1G \--driver-memory 1G \--executor-cores 1 \--driver-class-path /etc/hive/conf \--files /etc/hive/conf/hive-site.xml,/etc/hadoop/conf/hdfs-site.xml,/etc/hadoop/conf/core-site.xml,conf/log4j.properties \--class ScgdUserLoss \jars/1-1.0-SNAPSHOT.jar \ &gt; logs/nohup.out 2&gt;&amp;1 &amp; conflog4j.properties，默认为 12345678910111213141516171819202122232425262728293031323334353637383940## Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the "License"); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.## Set everything to be logged to the consolelog4j.rootCategory=INFO, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n# Set the default spark-shell log level to WARN. When running the spark-shell, the# log level for this class is used to overwrite the root logger's log level, so that# the user can have different defaults for the shell and regular Spark apps.log4j.logger.org.apache.spark.repl.Main=WARN# Settings to quiet third party logs that are too verboselog4j.logger.org.spark_project.jetty=WARNlog4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERRORlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFOlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFOlog4j.logger.org.apache.parquet=ERRORlog4j.logger.parquet=ERROR# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive supportlog4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATALlog4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR jars自己提交的jar包 logs输出日志]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python连接hive/mysql]]></title>
    <url>%2F2017%2F12%2F14%2Fpython%E8%BF%9E%E6%8E%A5hive-mysql%2F</url>
    <content type="text"><![CDATA[注意 windows环境，sasl安装不上，报各种错误解决办法：在whl库中，找到所需要的whl中，安装即可 whl文件是什么？whl格式本质上是一个压缩包，里面包含了py文件，以及经过编译的pyd文件。使得可以在不具备编译环境的情况下，选择合适自己的python环境进行安装。 首先集群需要启动hiveserver2 1hive --service hiveserver2 ip映射，可以在xshell对应集群窗口属性中设置隧道，访问某个ip的端口映射到访问本地ip某个端口 hive 12345678910import pandas as pdimport pymysqlfrom hdfs3 import HDFileSystemfrom pyhive import hivecursor = hive.connect("192.168.90.44",port = 10000).cursor()cursor.execute('select day ,mediaid,aid ,clickthrough from ad.terminalclick_log where day between "20180409" and "20180523" ')columns = ["day","mediaid","aid","clickthrougt"]data = pd.DataFrame(data=cursor.fetchall(),columns=columns)data.head() day mediaid aid clickthrougt 0 20180409 agltb3B1Yi1 id1 starcorcom 1 20180409 agltb3B1Yi1 id1 starcorcom 2 20180409 agltb3B1Yi1 id1 starcorcom 3 20180409 agltb3B1Yi1 id1 starcorcom 4 20180409 agltb3B1Yi1 id1 starcorcom mysql123456conn= pymysql.connect(host="192.168.90.44",user="root", password="starcor",db="test",port=3306,charset='utf8')# 创建游标cursor = conn.cursor()cursor.execute("select * from item_index")item_index_df=pd.DataFrame(data=list(cursor.fetchall()))item_index_df.head() 0 1 2 3 4 5 6 7 8 0 (22-23) 0.833254 0.166746 0.131273 0.245796 0.281922 0.162764 0.11543 0.0629 1 (26-27) 0.83121 0.16879 0.113689 0.233952 0.280988 0.187385 0.0982592 0.0857 2 (28-29) 0.766236 0.233764 0.105216 0.228691 0.280427 0.235434 0.104194 0.0459 3 -14 0.828478 0.171522 0.0940754 0.146784 0.28686 0.258316 0.116409 0.0977 4 007大战皇家赌场 0.709068 0.290932 0.279817 0.150034 0.180251 0.204246 0.103626 0.0822]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
      <tags>
        <tag>python连接hive/mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提交本地项目到github及更新项目]]></title>
    <url>%2F2017%2F11%2F26%2F%E6%8F%90%E4%BA%A4%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE%E5%88%B0github%E5%8F%8A%E6%9B%B4%E6%96%B0%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[github在线上传通过git工具 上传 cd 项目文件夹 git init git add . git commit -m update github主页创建同名项目，不要勾选add README,并复制仓库地址 git remote add origin https://github.com/dcexist/study_4_userLoss.git git push -u origin master 更新 cd 项目文件夹123git add .git commit -m updategit push origin master]]></content>
      <categories>
        <category>github</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2017%2F11%2F23%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux命令 打包 1tar cvf data0.tar data0/ 解压 1tar xvf data0.tar ../data1 集群配置文件夹 1cd /etc/hive/conf 查看集群其他主机名和对应ip 1cat /etc/hosts 查找指定进程是否在运行 1ps aux | grep hiveser 写内容到文件中 1echo "bla la la" &gt; ./a.txt 追加内容到文件中 1echo "ba la" &gt;&gt; ./a.txt 显示变量 1echo $JAVA_HOME 下载 12wget http://curl -o http://]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Shell命令]]></title>
    <url>%2F2017%2F11%2F13%2FHadoop%20Shell%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Hadoop命令 查询有哪些目录 1hdfs dfs -ls / 用管道查看文件(more/head/tail等) 1hdfs dfs -cat wc_out/* | more 查询目录文件大小 1hdfs dfs -du -h / 查询某个文件夹大小 1hdfs dfs -dus -h / 从hdfs取文件到linux本地 1hdfs dfs -get /user/portrait/scgd/data0 ./output/ 上传文件到hdfs上 1hdfs dfs -put ./output/ /user/portrait/scgd/data0 给命令起别名 1alias hfs= hdfs dfs]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
      <tags>
        <tag>Hadoop命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地IDEA连接集群]]></title>
    <url>%2F2017%2F11%2F05%2F%E6%9C%AC%E5%9C%B0IDEA%E8%BF%9E%E6%8E%A5%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[读取hdfs数据 建立resource文件夹,将集群中 /etc/hive/conf 中的xml文件复制到这个文件夹,并将该文件夹设置为resource属性，同时将代码文件夹设置为source属性 将集群 /etc/hosts文件中的映射关系添加到c:/System32/drivers/etc/hosts文件 spark.read.format(“csv”).load(“/user/test/test2.csv”).show() 读取hive表数据 除了上述之外，还需配置pom.xml的依赖，比如spark-sql/spark-hive/tez引擎等 spark.sql(“select * from starcor_test.play_log where day=20180509 limit 20”).show() #表名需要是库名.表名 读取mysql表数据加入mysql-connector···依赖 val jdbcDF = spark.read .format(&quot;jdbc&quot;) .option(&quot;url&quot;, &quot;jdbc:mysql://master02:3306&quot;) .option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;) .option(&quot;dbtable&quot;, &quot;test.item_index&quot;) .option(&quot;user&quot;, &quot;root&quot;) .option(&quot;password&quot;, &quot;starcor&quot;) .load() 本地文件 spark.read.format(“csv”).load(“file:/c:/user/test/test2.csv”).show()]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sql 操作案例]]></title>
    <url>%2F2017%2F10%2F20%2Fsql-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[用户行为记录表，计算相邻两次的差值(比如时间戳之差，作为用户观看时间) 1234spark.sql("select user_id,video_type,server_time,row_number() over(partition by user_id order by server_time) rank from playlog ").createOrReplaceTempView("a")spark.sql("select user_id,video_type,server_time,rank-1 as rank from a order by user_id,server_time").createOrReplaceTempView("b")spark.sql("select a.user_id,a.video_type,( b.server_time-a.server_time) watch_time from a left join b on a.user_id=b.user_id and a.rank=b.rank order by a.server_time").createOrReplaceTempView("c")spark.sql("select user_id,video_type, watch_time/60000.0 watch_time from c where watch_time is not null").createOrReplaceTempView("d")]]></content>
      <categories>
        <category>SQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas 常用函数]]></title>
    <url>%2F2017%2F10%2F17%2Fpandas-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[保存文件,含header1data.to_csv('F:\\MyDocuments\\starcor\\one_day.csv',header=True,index=False,encoding='utf-8') 读取文件，含header 1data=pd.read_csv('F:\\MyDocuments\\starcor\\one_day.csv',encoding='utf-8') 用户行为记录，按照用户分组，计算分组后server_time.max-server_time.min 123456index user_id client_type video_id video_name playbill_length server_time0 8230003234837184 stb d216440bff6dadf9813b067d6c42bdc0 御姐归来 0.0 1.529424e+121 8230004001768545 stb 34acd1a54645f3a9761173ddc62154f0 宝宝巴士儿歌 0.0 1.529424e+122 8230004001768545 stb 34acd1a54645f3a9761173ddc62154f0 宝宝巴士儿歌 0.0 1.529424e+123 8230004001768545 stb 34acd1a54645f3a9761173ddc62154f0 宝宝巴士儿歌 0.0 1.529424e+124 8230002289053523 stb 5b1fa285380f7f1fbf3d68665be05d24 女医明妃传 DVD版 0.0 1.529424e+12 123temp=data.groupby('user_id',as_index=False)['server_time'].agg(&#123;'server_time_max':max,'server_time_min':min&#125;)temp['playbill_length1']=temp.server_time_max-temp.server_time_mintemp.head() 123456index user_id server_time_min server_time_max playbill_length10 8230002560236474 1.529508e+12 1.529510e+12 2773122.01 8230002560283518 1.529508e+12 1.529510e+12 1383261.02 8230002560346901 1.529508e+12 1.529510e+12 2516150.03 8230002560367956 1.529510e+12 1.529510e+12 1326.04 8230002560463268 1.529509e+12 1.529509e+12 1832.0 DataFrame有些字段，比如影片类型特征列，往往不止一个类型，有些情况下需要将影片类型拆分并新建特征列(类型1，类型2) 1234# 提取多值标签-影片类型len=data2.evs_content_type.map(lambda x:len(str(x).split('/'))).max()for i in range(len): data2['film_category'+str(i)]=data2['evs_content_type'].map(lambda x:str(str(x).split('/')[i]) if len(str(x).split('/')) &gt; i else '') 用户行为记录，记录用户相邻两次操作时间差 1234567891011121314151617181920# 原始表 user_id timestamp0 8230003138952154 01 8230003138952154 12 8230003138952154 53 8230003138952154 94 8230003138952154 95 8230003138952154 106 8230003138952154 117 8230003138952154 158 8230004134835377 79 9950000002391827 510 9950000002391827 611 9950000002391827 912 9950000002391827 1213 9950000002391827 1414 9950000002391827 1415 9950000002391827 1516 9950000002391827 1617 9950000002391827 17 12345df['pre'] = df['timestamp'].shift(1)df['uid'] = df['user_id'].shift(1)df['interval'] = (df['timestamp'] - df['pre'])df1 = df[(df['user_id'] == df['uid'])]df1[['user_id','timestamp','interval']] 1234567891011121314151617# 输出 user_id server_time_min interval1 8230003138952154 1 1.02 8230003138952154 5 4.03 8230003138952154 9 4.04 8230003138952154 9 0.05 8230003138952154 10 1.06 8230003138952154 11 1.07 8230003138952154 15 4.010 9950000002391827 6 1.011 9950000002391827 9 3.012 9950000002391827 12 3.013 9950000002391827 14 2.014 9950000002391827 14 0.015 9950000002391827 15 1.016 9950000002391827 16 1.017 9950000002391827 17 1.0]]></content>
      <categories>
        <category>pandas</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Jupyter notebook 隐藏代码]]></title>
    <url>%2F2017%2F10%2F16%2Fjupyter-notebook-%E9%9A%90%E8%97%8F%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[代码如下：123456789101112131415from IPython.display import HTMLHTML('''&lt;script&gt;code_show=true; function code_toggle() &#123; if (code_show)&#123; $('div.input').hide(); &#125; else &#123; $('div.input').show(); &#125; code_show = !code_show&#125; $( document ).ready(code_toggle);&lt;/script&gt;&lt;form action="javascript:code_toggle()"&gt;&lt;input type="submit" value="Click here to toggle on/off the raw code."&gt;&lt;/form&gt;''')]]></content>
      <categories>
        <category>Jypyter</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[makedown 表格自动生成]]></title>
    <url>%2F2017%2F10%2F15%2Fmakedown-%E8%A1%A8%E6%A0%BC%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%2F</url>
    <content type="text"><![CDATA[参考链接1234567891011exceltk用例整个表格： exceltk.exe -t md -xls xxx.xls exceltk.exe -t md -xls xxx.xlsx指定sheet： exceltk.exe -t md -xls xx.xls -sheet sheetname exceltk.exe -t md -xls xx.xlsx -sheet sheetnameexceltk特性： 转换Excel表格到MarkDown表格 支持Excel单元格带超链接 如果Excel里有合并的跨行单元格，在转换后的MarkDown里是分开的单元格，这是因为MarkDown本身不支持跨行单元格 如果Excel表格右侧有大量的空列，则会被自动裁剪，算法是根据前100行来检测并计算 exceltk下载地址 1./exceltk.exe -t md -xls 1.xlsx 注意事项在执行命令时，需关闭要处理的xls文件窗口，不能在Excel打开]]></content>
      <categories>
        <category>makedown</category>
      </categories>
      <tags>
        <tag>makedown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最小值寻优算法推导]]></title>
    <url>%2F2017%2F10%2F14%2F%E6%9C%80%E5%B0%8F%E5%80%BC%E5%AF%BB%E4%BC%98%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[梯度下降法+最小二乘法+牛顿法+拟牛顿法 推导]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[L1&L2正则化介绍及推导]]></title>
    <url>%2F2017%2F10%2F13%2FL1-L2%E6%AD%A3%E5%88%99%E5%8C%96%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[介绍L1范数，表示向量X中元素的绝对值之和；L2范数，表示向量X中元素平方的和然后开根号。 推导]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[XGBoost原理及推导]]></title>
    <url>%2F2017%2F10%2F12%2FXGBoost%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[原理利用前向分布算法，在优化目标函数的时候，泰勒展开到二阶，然后优化求解。 推导图片拖到新标签页，可仔细查看图片。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost&GBDT原理及推导]]></title>
    <url>%2F2017%2F10%2F10%2FAdaBoost-GDBT%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[原理 AdaBoost对于存在强依赖关系的弱学习器，需要串行生成。先从初试训练集训练一个学习器，然后根据该学习器的表现对训练样本的分布进行调整，对于先前学习器犯错的样本给予更多的关注，继续生成学习器，重复上述步骤，直至训练了指定数目的学习器，然后将这些学习器进行加权结合，每个权重代表的是其对应分类器在上一轮迭代中的成功度。 GBDT是一种Boosting算法，但又与传统的boost算法思路不同，每次迭代不是对犯错的样本施以更高的权重。回归树在使用前项分步算法迭代中，下一轮是拟合上一轮的模型的负梯度在上一轮模型的值。 推导图片拖到新标签页，可仔细查看图片。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[决策树原理及推导]]></title>
    <url>%2F2017%2F10%2F09%2F%E5%86%B3%E7%AD%96%E6%A0%91%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[原理主要是利用树的形式，每次按照一定的准则选择特征，然后根据特征划分子树或者叶结点，直到满足停止条件位置。 推导图片拖到新标签页，可仔细查看图片。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯原理及推导]]></title>
    <url>%2F2017%2F10%2F08%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[原理判断某个记录属于哪一类，可以认为在该记录出现的情况下哪种类别概率最大，最大的就属于该类（比如判断一个黑人来自哪里，可以认为在黑人记录出现的情况下非洲概率最大，会判断他来自非洲）。但这个问题有时候不好直接得到答案P(C=0|X)，利用朴素贝叶斯的话就是求（P(C=0，X)）除以（训练集中该记录出现的概率强P(X)），分母一般不需要计算，分子用条件概率就可以求得，分子，即P(C=0，X)=P(X|C=0)P(C=0)=P(X1|C=0)*P(X2|C=0)P(C=0) 推导图片拖到新标签页，可仔细查看图片。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SVM原理及推导]]></title>
    <url>%2F2017%2F10%2F07%2FSVM%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[原理寻找一个超平面，使得距离超平面最近的点与超平面间隔最大化，然后通过对其对偶问题的求解求得模型参数。 推导图片拖到新标签页，可仔细查看图片。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LR原理及推导]]></title>
    <url>%2F2017%2F10%2F06%2FLR%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[原理利用sigmoid函数对线性回归预测的结果进行映射，规定当sigmoid自变量&gt;0时，输出y为1代表正例，当sigmoid变量&lt;0时，输出y为0代表负例，利用极大似然法求解模型参数（极大似然法公式中也代表着使预测为正的概率最大和预测为负的概率最大）。 推导图片拖到新标签页，可仔细查看图片。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[《沉思录》节选]]></title>
    <url>%2F2017%2F01%2F08%2F%E6%B2%89%E6%80%9D%E5%BD%95%2F</url>
    <content type="text"><![CDATA[序言 作者是马可奥勒留，古罗马皇帝，自幼被皇帝收为养子后被推上帝位。哲学史上，属于斯多葛派。晚期的斯多葛派哲学家思考具体的生活伦理，关注的是在一个个体难以把握自己命运的广大世界里，如何让自己有限的一生发出善的光辉。 奥勒留坦言受惠于埃比克太德的《对话录》甚多，埃比克太德早年曾是奴隶，后获释为自由人。 哲学原来并非如后世的哲学教科书那般呆板枯燥，如一堆殿堂上的木偶，而是一潭活水。 卷一本卷为奥勒留自述从身边的人或者历史人物中学到了哪些东西，有的不只是某些品格如谦和，还有的具体到某个具体的事情值得学习。很多方面都是大家都知道的大道理，但知易行难。 从阿珀洛尼厄斯那里学到，凡是除了依赖理性，别无他途。 养父（古罗马皇帝安东尼.派厄斯）身上，他也善于享受但不沉溺，对这些东西很多人是软弱地既不能放弃、也做不到有节制地享受的。 感谢神明，虽然我曾一度陷入情欲，最终还是得到了解脱。感谢神明，在我对哲学产生兴趣时，没有让我拜倒在任何诡辩家脚下。没有一味读死书或沉迷于逻辑推理，也没有过度专注于探究天国的奥秘。 卷二 不论地位如何，我的存在，不过是一具尸体、一阵呼吸、一份能支配自己的能力。假设你现在濒临死亡，好好想想：你已经垂垂老矣，不要让自己成为奴隶，听凭操纵，像木偶一样受各种冲动胡乱驱使，也不要再抱怨目前的处境，或者为将来忧虑。 抛开你对书本的渴望，这样你就不必在喋喋不休的抱怨中死去，而是怀着真正的欢欣和对神明的虔诚感激离开人世。 记住你已经拖延得很久了，神明赋予你的恩典已经够多了，你却没有好好把握。你的生命有限，如果不能用来消除思想上的困惑，这样的机会就再也没有了，再也不属于你，随着你的离世而永远地丧失了。 人的生命如此短暂，你的甚至已经接近尾声，却没有让自己获得荣耀，而将幸福寄托在别人的灵魂身上。 在生活中被自己的行为弄得筋疲力尽的人也同样是愚蠢的，他们的一切冲动没有目标，他们的思想是懵懂的。 如果一个人不能够看清别人灵魂，那也没有什么不妥；而很少留心自己内心动机的人，却注定不幸。 由欲望引起的过错比愤怒引起的过错更应该受到谴责。愤怒时，由于某种痛苦或内心的煎熬失去了理智；因欲望犯错是由于经不起快乐的诱惑，过错也就更不道德，缺乏男人应有的自制力。 让你的每一个行为、每一句话、每一种想法都像是即将辞世的人所做的最后一次吧。 万事万物在飞逝而去，那些以快乐诱惑我们、以痛苦恐吓我们的事物，以及浮华的名声，这一切都毫无价值，转瞬即逝，需要我们的理性来甄别。死亡不过是自然的一种转换，若是还害怕这种自然的转换，那就像小孩一样无知了。 没有比这更值得同情了：有的人对什么都要弄个究竟，连地下的事物都要刨根问底，还要琢磨邻人的想法，却不知道只要一心一意地供奉内心的神明就足够了，这样就能保持心灵的纯洁。不因激情和轻率而犯错，也不会对神灵的安排和人的作为感到不满。 人所失去的，只是他此刻所拥有的生活；人所拥有的，也只是他此刻正在失去的生活，因此，生命的长短没有什么不同。 属于身体的一切如同流水易逝，属于灵魂的宛若泡影，人生是一场战场，一段旅途，身后的名声也只会渐渐湮灭。那靠什么来保护指引我们呢？只有一样东西————这便是哲学，它能保护我们的灵魂始终纯洁，免受伤害，不被痛苦和快乐所驱使，能让我们做事不再漫无目的，也不会存心欺瞒，别人做什么或者不做什么都影响不了我们。更进一步，我们因此能够对一切发生或者注定的事情欣然领受，好像一切自有源头，理所应当。最重要的是，我们能够以一种愉快的心情等待死亡，因为这只是组成一切生物的元素的分解。如果元素本身在不断变化，变成其他事物的过程中并没有什么令人不快的，我们又何必恐惧这种变化和分解呢？ 卷三 我们应当考虑的，不仅仅是生命正在消耗，时日无多，还应当想到，即使能够活得长久些，怎见得我们就有把握保持领悟事物的能力，仍然能够不断思考有关神和人的事情？如果一个人的思想开始迟钝，虽然他的呼吸消化和体力各项功能还有，但完美的支配自身，明确自己的义务，分析感受到的各种现象，对于结束生命的时机作出明确判断诸如此类的行为他却不再能够做到，因为这绝对需要一种良好的理性能力，而他的这些能力早已经衰退了。所以我们必须抓住时机，不只是因为我们在一天天接近死亡，也因为我们对事物的理解力和观察力会在死亡之前逐渐衰退。 如果一个人具有敏锐的感知能力和深刻的观察力，那么宇宙万物在他看来无一不使他愉悦，即使其中有的事物只是顺带出现的。这样的人在亲眼观察野兽张开血盆大口时，得到的快乐并不比观赏画家和雕刻家以此为题材的作品要少。只有真正熟稔自然及其造化的人，才能深刻领会到其中的迷人之处。 与理智和神性相比，这具臭皮囊实在不值一提，如同粪土。 如果任由一切身外之物——例如众人的赞美、权势、财富、或者享乐——来同这种合乎理性的群体的利益抗衡，那是不对的。那些东西似乎能暂时满足我们，但会控制我们，将我们带入歧途。 那些极为看重自己的智慧并且相信自己有判断力的人，从不装腔作势、乱发牢骚，既不刻意独处也不一定要混迹于众人之中；最重要的是，他一生从不殚精竭虑去追求什么，也不企图躲避什么，灵魂在他身体内停留的时间里长一些还是短一些，他完全不在意。即便必须马上离开人世，他也会高高兴兴的上路，就好像做别的事情一样体面自然；他一生看重的事情只有一件：永远做一个合乎理性的人，社会的一份子。 每个人的生命都只存在于此刻，此刻也在飞快地消逝，其他大部分的时间不是已经过去、永不复返，就是尚未到来，不可预料。 一心一意，不急不躁，不随便分心在别的事情上，保持心内的纯净正直，即使你随时可能放弃生命————如果能做到这些，不奢求什么，也无所畏惧。 感觉属于肉体，欲望属于灵魂，判断属于理智。 卷四 当内心的力量顺应自然，就能运用天生的能力来应对任何事，就像火焰吞没落进它里面的东西一样。若是小小火苗，可能会被这些东西压灭，但若是熊熊烈火，则瞬间就能将堆在它上面的东西吞噬，燃烧殆尽，借助这些别的物质让火势更旺。 人总是想退隐乡间、海滨、山林，你也曾经全心向往这种生活，但这完全是一种庸俗的想法，因为你尽可以随时退隐到自己的内心去，没有任何地方能比自己的心灵更为宁静、更无烦忧。如果这个人的内心海阔天空，他只消静心敛神，立刻就可以获得完全的宁静。内心的原则也只需简明扼要，运用起来足以澄净一切纷扰。 第一，外在事物与心灵无关，一切的纷扰都来自内心；第二，你眼前所见的一切都瞬息万变，终将归于湮灭。 你和别人都会很快死去，不久之后你们的名字也将被后人忘记。 不要有受到伤害的想法，想法不见了，伤害也就消失了。 你是作为这世界的一部分出现的，你从哪里来就到哪里去，或者可以说经过一个转化的过程，你将回到那创造你的宇宙理性之中。 不要窥探别人的内心的黑暗，不要左顾右盼，而只是笔直地一路奔向终点。 如果躯体死了，而灵魂却继续存在，大气怎么容纳得了这无数的灵魂呢？同样，大地又怎么容纳得了那些自古以来被埋葬的尸体呢？实际上，埋在地下的尸体一段时间之后就会转化分解，为别的尸体腾出空间。而灵魂也是如此，它在大气中存在一段时间之后便要转化消散，通过燃烧重新回到宇宙造物的法则中去，因此别的灵魂也就有了存在的空间。 若要保持心灵宁静，只做必要的事情。这样不仅能获得行事适当而带来的心灵宁静，也能获得由于少做事而产生的心灵宁静。因为我们的大部分言行都是不必要的，如果有所节制，会有更多的闲暇，更少的烦恼。我们应该减少不必要的行为，也要抛弃不必要的想法。 回想一下以前的时代，你会发现那时人们的经历和现在没有什么不同：结婚、生育、生病、死亡、吃喝、买卖、阿谀奉承、自吹自擂、猜疑、算计、抱怨命运、积累钱财、梦想着做高官，到如今这些人已经死去，不复存在。最重要的是要记住，你做每件事都要以事情本身的价值来判断。 什么才是值得我们追求的呢？只有一件：思想公正，行为无私，为人诚实，对于所经历的一切都坦然愉快地接受，从不大惊小怪，因为这一切都源于同一个命运的安排。 总之，牢牢记住吧：人生是何等短促，何等卑微。昨天像是一滩黏液，明天也不过是一具木乃伊、一堆灰尘。所以，在这短暂的有生之年，让自己过得合乎自然吧，恰然地走向人生的终点，就像一枚熟透之后即将坠地的橄榄，感激承托它的大地，感激生养它的枝干。 要像那岸边的礁石，任凭海浪不断地向它击打，也巍然不动，直到暴怒的海浪变得驯服。“我多么不幸啊，这样的事情落到我头上！”相反，要这样对自己说：“我是多么幸运啊，虽然发生了这样的事，但我仍旧泰然自若，既没有被现在压垮，也不为将来感到恐惧。” 卷五 休息是必要的，但也要有限度，就像吃喝不能过多吃饱一样，可你已经超出限度了，超出足够的量了。 摈弃那些搅乱你心灵的无益的念头，恢复内心的完全平静，这并不难做到。 每个人的遭遇都是事先安排好的，以适合他的命运。这里所说的适合，就像工匠把石头凿得方方正正，严丝合缝地嵌进墙壁或金字塔里，让它们适合特定的建筑一样。如果不是对宇宙有用，他绝不会让这件事降临到你头上，自然绝不会让任何不符合自然之道的东西任意胡为。 你平时是怎样想的，你的灵魂便也是怎样的，因为灵魂受思想的熏陶。 敬畏那宇宙中最强大的力量吧，正是这力量在利用、掌控着世间万物。同样，也要尊重你内部那个最强大的力量，因为它也具有类似的能力。它支配着你的一切，你的生命受它统治。 过去的一切转瞬即逝，未来的一切也是一个无底的深渊，没有什么能留存下来。那么，那些自鸣得意、怨天尤人、伤心痛苦的人不是傻子又是什么呢？难道困扰他们的这些事情能永世长存么吗？ 快了快了，你很快就将化为灰尘、一具骷髅，只剩下一个名字，甚至连名字也不存在了。你所能支配的只是自身的肉体和呼吸之内的一切，在此之外的，非你所能控制。 不因自己的私欲伤害他人。 因为人总是以为自己受到了伤害，那不是一个好习惯。 卷六 只要你是在尽自己的责任，那么无论是忍饥受冻还是衣食饱暖，无论你得到的是唾骂还是赞美，无论你即将离世还是忙于其他的工作都毫无差别。因为死亡也是人生当中的一件事情，所以在离开人世之前，把手里的工作做好就足够了。 要观察事物的本质，不要忽略任何事物的特质和它的价值。 如果宇宙是一片混沌，为什么我还愿意留在这个混乱无序的世界里呢？无论我做什么，最终都是要死的，那还有什么可烦忧的呢？如果宇宙有秩序有主宰，我便虔诚地、坚定不移地信奉那主宰万物的力量，更加无需烦忧了。 有时当你为环境所迫、烦躁不安的时候，要赶快克制自己，不要一直处于那种失衡状态。因为如果你能不断地观照内心，便能更好地控制自己。 对于摆在面前的佳肴美味，如果你能这样想，就能看到它们的本质：这是一条鱼的尸体，这是一只鸟、一头猪的尸体。这酒呢，只是葡萄的浆汁，而身上的紫袍不过是在血里染过的羊毛。而性交，也就是器官的摩擦，伴随着一阵抽搐、喷出的一点黏液而已。这样就能看穿事物的形态，直达本质，看清他们到底是什么。 到底什么才是有价值的？是众人对我们的欢呼赞美吗？当然不是，这样的赞美不过是舌头在嘴里摇动，有什么价值呢？要顺应生命的要求来取舍自己的行为。 对于那些与自己同一个时代，和他们生活在一起的人，他们从来不肯说任何好话。却又格外看重后世的人会如何称颂自己，尽管那些人他们从未见过、甚至永远也见不到。 不要以为你自己难以完成的事情，别人也不可能做到。同样，如果有什么事情是别人能够做到的，那么要相信你自己也能做到。 你的每一项义务都是由一系列小事构成的，只管去有条不紊地完成手里的工作。 死亡是一种休憩，不再受感官快乐的引诱，不再如木偶般被欲望所操控。各种飘摇不定的想法不再出现，肉体的劳作也就此停歇。 多么令人羞愧呵！你的身体还没有放弃战斗，灵魂倒先屈服了。 像安东尼一样不为虚名所累，像他一样专注于理解事物的本质，在仔细考察并且有了清楚的认识以前，他绝不会放过任何一件小事。由于饮食简单，除了惯常的休息之外，他不需要任何别的放松。 赶快醒来恢复神智吧，这样你就能发现不过是做了一场梦，明白让你困扰的只是梦幻。你清醒过来之后，再看看你面对的现实，也正如同你梦中面对的景象。 我是由身体和灵魂组成的，一切事物都与身体无关，因为身体不能感觉种种事物的差别。但对于心灵，只有那些不属于它自身活动的事物才与它无关，而它自身所有的行为，都在它控制之下。 适应你命中注定的环境，爱那些注定与你生活在一起的人，要真诚地爱他们。 如果你的体重不足300磅，我猜你也不至于因此而懊恼吧，那为什么要因为只活了这么些年而不是更长而感到难过呢？你既然能对分配给你的身体重量感到满足，那就应该也满足于分配给你的时间。 追逐虚名的人把幸福寄托在别人的言辞上，贪图享乐的人把幸福寄托在自己的感官上，而有理智的人则把幸福安置在自己的行动之中。 要养成这样的习惯，认真倾听别人说话，尽可能地深入对方的内心。 和我一起进入这世界的人，有多少已经撒手离去了。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
  </entry>
</search>
