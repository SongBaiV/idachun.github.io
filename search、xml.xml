<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[makedown 表格自动生成]]></title>
    <url>%2F2018%2F06%2F15%2Fmakedown-%E8%A1%A8%E6%A0%BC%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%2F</url>
    <content type="text"><![CDATA[参考链接 exceltk用例 整个表格： exceltk.exe -t md -xls xxx.xls exceltk.exe -t md -xls xxx.xlsx 指定sheet： exceltk.exe -t md -xls xx.xls -sheet sheetname exceltk.exe -t md -xls xx.xlsx -sheet sheetnameexceltk 特性： 转换Excel表格到MarkDown表格 支持Excel单元格带超链接 如果Excel里有合并的跨行单元格，在转换后的MarkDown里是分开的单元格，这是因为MarkDown本身不支持跨行单元格 如果Excel表格右侧有大量的空列，则会被自动裁剪，算法是根据前100行来检测并计算 exceltk下载地址 ./exceltk.exe -t md -xls 1.xlsx]]></content>
      <categories>
        <category>makedown</category>
      </categories>
      <tags>
        <tag>makedown 表格自动生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn 注意事项]]></title>
    <url>%2F2018%2F06%2F15%2Fsklearn-%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[正负样本规范sklearn 横行为预测值，纵行为真实值sklearn中0为负样本、1为正样本,计算精确率、召回率，也是计算类别为1的正样本 预测值 0 1 真实值 0 1]]></content>
      <categories>
        <category>sklearn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python连接hive/mysql]]></title>
    <url>%2F2018%2F06%2F14%2Fpython%E8%BF%9E%E6%8E%A5hive-mysql%2F</url>
    <content type="text"><![CDATA[import pandas as pd import pymysql from hdfs3 import HDFileSystem from pyhive import hive cursor = hive.connect(&quot;192.168.90.44&quot;,port = 10000).cursor() cursor.execute(&#39;select day ,mediaid,aid ,clickthrough from ad.terminalclick_log where day between &quot;20180409&quot; and &quot;20180523&quot; &#39;) #cursor.fetchall()外面是个大元组，里面是个小元组，小元组代表每一行元素 exposure_pv = list(cursor.fetchall())#变成外面是个列表，里面是个小元组 exposure_list = [] for r in exposure_pv: exposure_list.append(r)#变成外面是个大列表，里面是个小列表 columns = [&quot;day&quot;,&quot;mediaid&quot;,&quot;aid&quot;,&quot;clickthrougt&quot;] impression_Data = pd.DataFrame(data=exposure_list,columns=columns) impression_Data.head() conn= pymysql.connect(host=&quot;192.168.90.44&quot;,user=&quot;root&quot;, password=&quot;starcor&quot;,db=&quot;test&quot;,port=3306,charset=&#39;utf8&#39;) # 创建游标 cursor = conn.cursor() cursor.execute(&quot;select * from item_index&quot;) item_index_df=pd.DataFrame(data=list(cursor.fetchall())) item_index_df.head() 0 1 2 3 4 5 6 7 8 0 (22-23) 0.833254 0.166746 0.131273 0.245796 0.281922 0.162764 0.11543 0.0629008 1 (26-27) 0.83121 0.16879 0.113689 0.233952 0.280988 0.187385 0.0982592 0.0857842 2 (28-29) 0.766236 0.233764 0.105216 0.228691 0.280427 0.235434 0.104194 0.0459612 3 -14 0.828478 0.171522 0.0940754 0.146784 0.28686 0.258316 0.116409 0.0977354 4 007大战皇家赌场 0.709068 0.290932 0.279817 0.150034 0.180251 0.204246 0.103626 0.0822047]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
      <tags>
        <tag>python连接hive/mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征重要性计算]]></title>
    <url>%2F2018%2F06%2F14%2F%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[特征重要性可以用来做模型可解释性，这在风控等领域是非常重要的方面。xgboostxgboost实现中Booster类get_score方法输出特征重要性，其中importance_type参数支持三种特征重要性的计算方法 1.importance_type=weight（默认值），特征重要性使用特征在所有树中作为划分属性的次数 2.importance_type=gain，特征重要性使用特征在作为划分属性时loss平均的降低量 3.importance_type=cover，特征重要性使用特征在作为划分属性时对样本的覆盖度]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征重要性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT生成特征]]></title>
    <url>%2F2018%2F06%2F14%2FGBDT%E7%94%9F%E6%88%90%E7%89%B9%E5%BE%81%2F</url>
    <content type="text"><![CDATA[from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingClassifier from sklearn.preprocessing import OneHotEncoder X, y = make_classification(n_samples=10) print y print X.shape X [0 1 1 0 0 1 1 1 0 0] (10L, 20L) array([[-1.46218297e+00, -4.18333502e-01, -3.59274601e-02, 4.51595239e-01, -1.51047633e+00, 5.07112394e-01, 7.38097000e-01, -3.02315469e-01, 4.26175119e-01, 4.01702307e-02, -6.60368519e-01, -1.00253339e+00, 4.55802942e-01, -4.31784177e-01, 5.61925740e-02, -9.87095232e-01, -3.60589437e-01, -8.20339651e-02, -1.51267169e+00, -1.75602670e+00], [-1.38065657e-01, 1.51125229e+00, -1.16786498e+00, 8.96428602e-01, 2.24770779e-02, 2.75832881e-01, 8.84555986e-01, 1.90990870e-01, 1.50974562e+00, 6.32858976e-01, 7.43984848e-01, 6.23124869e-01, 2.75529340e-01, 4.13913270e-01, -5.69452199e-01, 9.92345363e-01, -6.58511532e-01, -1.04902254e+00, 4.17471392e-01, -1.69380460e+00], [ 7.94578797e-01, 6.91286961e-01, -8.36134614e-01, 1.35232255e+00, 8.19800516e-01, -6.63009090e-01, -3.93857189e-01, 1.21970975e+00, 9.45892797e-01, 8.78534488e-01, -6.82525652e-01, -4.65135487e-01, 7.91431577e-01, 3.54382290e-01, 8.96197192e-02, -8.50444786e-01, 5.54008693e-01, -2.22658663e-01, 2.60990234e-01, 5.36795887e-01], [ 2.35619886e-03, 2.00775837e-01, 4.87206937e-02, -4.86582074e-02, -1.93014354e+00, 3.94423848e-01, 8.43146268e-01, -1.18764902e+00, 1.21507109e+00, -5.82489166e-02, 2.91817058e-01, 2.63882786e-01, -8.09303986e-01, 2.13007401e+00, 2.03539722e+00, 2.50978548e-01, -5.79211447e-01, -2.27030861e+00, -1.32198712e+00, 3.58828134e-01], [ 1.76208221e-01, -6.44503388e-01, -1.35005640e-01, 2.57927723e-01, -1.15227919e+00, -1.04384378e+00, -5.25599357e-01, 2.16430369e+00, 1.14631323e+00, -4.86738371e-01, 3.56177943e-01, 4.25940193e-01, -2.46271932e-01, 8.02966687e-01, -9.16757374e-01, 3.92150341e-01, 8.89340812e-01, 8.89122664e-01, -6.91321300e-01, 3.60691085e-01], [-9.17667327e-01, -2.19718742e-01, 3.08231099e-01, -1.20865300e+00, 9.52723078e-02, -2.13289317e+00, 8.23448746e-01, 1.84966961e-01, 2.35626420e+00, -5.90061668e-01, -9.91453561e-01, 2.62985532e-01, -9.96616193e-01, -3.90777590e-01, 3.89293149e-01, -1.31349623e+00, 2.72401261e-02, -2.54414046e-01, -4.60378536e-01, 1.23080237e+00], [-8.28192448e-01, 1.15639869e+00, 2.78606672e-02, 9.65889005e-02, 1.68886757e+00, -3.25775102e-01, 1.75717716e-01, 1.59902401e-01, -6.55490597e-01, 2.95479360e-01, 6.60409198e-01, -2.70659454e+00, 7.21371599e-01, 5.61757431e-01, 1.22378486e+00, 9.99869289e-01, 2.21082939e+00, 1.31353776e-01, 1.64938642e+00, 3.81579172e-02], [-1.18357533e+00, 1.47944412e+00, 2.82847558e-01, 1.01023062e+00, 1.54087073e+00, 1.88003713e-02, -2.54038797e-01, -1.24068833e+00, -3.31394808e-02, 1.10512704e+00, -7.21359030e-01, 5.02536950e-01, 4.86264732e-01, -2.48485472e+00, 7.36671531e-01, -8.50743359e-01, 1.42773431e+00, -3.85276831e-01, 7.92621022e-01, 8.04302650e-01], [ 5.00298505e-01, 4.99864304e-01, -4.43128177e-01, -6.80877761e-01, -2.30634433e+00, -4.18106882e-01, -8.34288871e-01, 1.68655058e+00, -2.32944124e+00, -3.10646642e-01, -3.88910822e-01, -9.13217680e-01, 1.01294499e+00, 1.13638795e-01, 3.46336445e-01, -6.82351046e-01, -3.25986042e-01, -1.87458285e-01, -1.97646950e+00, -1.29381917e+00], [ 2.39415433e-01, 1.07290006e+00, 1.15550474e+00, -1.15201307e+00, -5.78108383e-01, -3.80187570e-01, -1.28861486e+00, -1.32918430e+00, -5.28881163e-01, 1.96556881e-01, -8.61984192e-01, -2.17046046e+00, -8.82487188e-02, 5.60887804e-01, 1.31724634e+00, -1.18909996e+00, -1.67108757e+00, 8.37225262e-01, -9.06706808e-01, -6.15740923e-01]]) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5) gbc = GradientBoostingClassifier(n_estimators=2) one_hot = OneHotEncoder() gbc.fit(X_train, y_train) X_train_new = one_hot.fit_transform(gbc.apply(X_train)[:, :, 0]) print (X_train_new.todense()) [[0. 1. 0. 1.] [0. 1. 0. 1.] [1. 0. 1. 0.] [1. 0. 1. 0.] [1. 0. 1. 0.]]]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
      <tags>
        <tag>GBDT生成特征</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python文本处理之gensim]]></title>
    <url>%2F2018%2F06%2F14%2Fpython%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B9%8Bgensim%2F</url>
    <content type="text"><![CDATA[from gensim import corpora,similarities,models import jieba # 训练样本 raw_documents = [ &#39;0南京江心洲污泥偷排”等污泥偷排或处置不当而造成的污染问题，不断被媒体曝光&#39;, &#39;1面对美国金融危机冲击与国内经济增速下滑形势，中国政府在2008年11月初快速推出“4万亿”投资十项措施&#39;, &#39;2全国大面积出现的雾霾，使解决我国环境质量恶化问题的紧迫性得到全社会的广泛关注&#39;, &#39;3大约是1962年的夏天吧，潘文突然出现在我们居住的安宁巷中，她旁边走着40号王孃孃家的大儿子，一看就知道，他们是一对恋人。那时候，潘文梳着一条长长的独辫&#39;, &#39;4坐落在美国科罗拉多州的小镇蒙特苏马有一座4200平方英尺(约合390平方米)的房子，该建筑外表上与普通民居毫无区别，但其内在构造却别有洞天&#39;, &#39;5据英国《每日邮报》报道，美国威斯康辛州的非营利组织“占领麦迪逊建筑公司”(OMBuild)在华盛顿和俄勒冈州打造了99平方英尺(约9平方米)的迷你房屋&#39;, &#39;6长沙市公安局官方微博@长沙警事发布消息称，3月14日上午10时15分许，长沙市开福区伍家岭沙湖桥菜市场内，两名摊贩因纠纷引发互殴，其中一人被对方砍死&#39;, &#39;7乌克兰克里米亚就留在乌克兰还是加入俄罗斯举行全民公投，全部选票的统计结果表明，96.6%的选民赞成克里米亚加入俄罗斯，但未获得乌克兰和国际社会的普遍承认&#39;, &#39;8京津冀的大气污染，造成了巨大的综合负面效应，显性的是空气污染、水质变差、交通拥堵、食品不安全等，隐性的是各种恶性疾病的患者增加，生存环境越来越差&#39;, &#39;9 1954年2月19日，苏联最高苏维埃主席团，在“兄弟的乌克兰与俄罗斯结盟300周年之际”通过决议，将俄罗斯联邦的克里米亚州，划归乌克兰加盟共和国&#39;, &#39;10北京市昌平区一航空训练基地，演练人员身穿训练服，从机舱逃生门滑降到地面&#39;, &#39;11腾讯入股京东的公告如期而至，与三周前的传闻吻合。毫无疑问，仅仅是传闻阶段的“联姻”，已经改变了京东赴美上市的舆论氛围&#39;, &#39;12国防部网站消息，3月8日凌晨，马来西亚航空公司MH370航班起飞后与地面失去联系，西安卫星测控中心在第一时间启动应急机制，配合地面搜救人员开展对失联航班的搜索救援行动&#39;, &#39;13新华社昆明3月2日电，记者从昆明市政府新闻办获悉，昆明“3·01”事件事发现场证据表明，这是一起由新疆分裂势力一手策划组织的严重暴力恐怖事件&#39;, &#39;14在即将召开的全国“两会”上，中国政府将提出2014年GDP增长7.5%左右、CPI通胀率控制在3.5%的目标&#39;, &#39;15中共中央总书记、国家主席、中央军委主席习近平看望出席全国政协十二届二次会议的委员并参加分组讨论时强调，团结稳定是福，分裂动乱是祸。全国各族人民都要珍惜民族大团结的政治局面，都要坚决反对一切危害各民族大团结的言行&#39; ] corpora_documents = [] #分词处理 for item_text in raw_documents: item_seg = list(jieba.cut(item_text)) corpora_documents.append(item_seg) # 生成字典和向量语料 dictionary = corpora.Dictionary(corpora_documents) print type(dictionary) print(dictionary) &lt;class &#39;gensim.corpora.dictionary.Dictionary&#39;&gt; Dictionary(384 unique tokens: [u&#39;\u8981&#39;, u&#39;CPI&#39;, u&#39;\u5931\u8054&#39;, u&#39;\u901a\u8fc7&#39;, u&#39;\u73af\u5883\u8d28\u91cf&#39;]...) # 稀疏表达方式，实际上产生的是16*384的词频矩阵，16是文档数目，384是词语数目 # 通过下面一句得到语料中每一篇文档对应的稀疏向量（这里是bow向量） corpus = [dictionary.doc2bow(text) for text in corpora_documents] # 向量的每一个元素代表了一个word在这篇文档中出现的次数 print(corpus) [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)], [(1, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1)], [(13, 3), (18, 1), (19, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1)], [(13, 3), (19, 5), (32, 1), (34, 1), (50, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 2), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 2), (87, 1), (88, 2), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1)], [(13, 2), (19, 2), (23, 1), (27, 1), (32, 2), (42, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1)], [(1, 1), (13, 2), (19, 1), (24, 1), (32, 1), (42, 1), (97, 2), (98, 2), (110, 1), (111, 1), (112, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1)], [(16, 1), (19, 4), (63, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 2)], [(13, 3), (19, 3), (57, 1), (79, 1), (103, 1), (134, 1), (184, 1), (185, 1), (186, 1), (187, 3), (188, 2), (189, 2), (190, 1), (191, 1), (192, 1), (193, 2), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1)], [(13, 5), (14, 1), (17, 1), (19, 4), (86, 2), (129, 1), (206, 1), (207, 3), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1)], [(1, 1), (13, 2), (19, 4), (24, 1), (27, 1), (32, 1), (34, 1), (45, 1), (124, 1), (136, 1), (171, 1), (173, 1), (187, 2), (188, 1), (189, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1)], [(19, 2), (149, 1), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 1), (260, 2), (261, 1), (262, 1)], [(1, 1), (13, 4), (19, 3), (21, 1), (24, 1), (27, 1), (65, 1), (86, 1), (129, 1), (263, 1), (264, 1), (265, 2), (266, 1), (267, 2), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1), (280, 1), (281, 1)], [(13, 1), (19, 4), (27, 1), (32, 1), (63, 1), (171, 1), (173, 1), (176, 1), (206, 1), (248, 1), (252, 2), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), (295, 1), (296, 1), (297, 1), (298, 1), (299, 1), (300, 1), (301, 2), (302, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1)], [(1, 1), (13, 1), (19, 3), (24, 1), (45, 1), (63, 2), (143, 1), (173, 1), (249, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1), (313, 1), (314, 1), (315, 1), (316, 1), (317, 1), (318, 1), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (324, 2), (325, 1), (326, 1), (327, 1), (328, 1), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1), (334, 1)], [(1, 1), (13, 2), (19, 1), (24, 1), (28, 1), (32, 2), (34, 1), (48, 1), (102, 1), (150, 1), (207, 1), (243, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 1), (340, 1), (341, 1), (342, 1), (343, 1), (344, 1), (345, 1), (346, 1), (347, 1), (348, 1)], [(13, 3), (19, 3), (48, 1), (65, 1), (86, 2), (151, 1), (172, 1), (207, 2), (316, 1), (349, 1), (350, 1), (351, 1), (352, 2), (353, 1), (354, 1), (355, 1), (356, 1), (357, 1), (358, 1), (359, 1), (360, 1), (361, 1), (362, 1), (363, 1), (364, 1), (365, 1), (366, 1), (367, 1), (368, 1), (369, 2), (370, 1), (371, 1), (372, 1), (373, 1), (374, 1), (375, 2), (376, 1), (377, 1), (378, 1), (379, 1), (380, 1), (381, 2), (382, 1), (383, 2)]] # corpus是一个返回bow向量的迭代器。下面代码将完成对corpus中出现的每一个特征的IDF值的统计工作 tfidf_model = models.TfidfModel(corpus) corpus_tfidf = tfidf_model[corpus] corpus_tfidf &lt;gensim.interfaces.TransformedCorpus at 0xaa15eb8&gt; similarity = similarities.Similarity(&#39;Similarity-tfidf-index&#39;, corpus_tfidf, num_features=600) test_data_1 = &#39;北京雾霾红色预警&#39; test_cut_raw_1 = list(jieba.cut(test_data_1)) # [&#39;北京&#39;, &#39;雾&#39;, &#39;霾&#39;, &#39;红色&#39;, &#39;预警&#39;] test_corpus_1 = dictionary.doc2bow(test_cut_raw_1) # [(51, 1), (59, 1)]，即在字典的56和60的地方出现重复的字段，这个值可能会变化 similarity.num_best = 5 test_corpus_tfidf_1=tfidf_model[test_corpus_1] # 根据之前训练生成的model，生成query的IFIDF值，然后进行相似度计算 # [(51, 0.7071067811865475), (59, 0.7071067811865475)] print(similarity[test_corpus_tfidf_1]) # 返回最相似的样本材料,(index_of_document, similarity) tuples [(2, 0.3595932722091675)] test_data_2 = &#39;长沙街头发生砍人事件致6人死亡&#39; test_cut_raw_2 = list(jieba.cut(test_data_2)) test_corpus_2 = dictionary.doc2bow(test_cut_raw_2) test_corpus_tfidf_2=tfidf_model[test_corpus_2] similarity.num_best = 3 print(similarity[test_corpus_tfidf_2]) # 返回最相似的样本材料,(index_of_document, similarity) tuples [(6, 0.19451555609703064), (13, 0.10124017298221588)]]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>gensim</tag>
        <tag>jieba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2018%2F06%2F13%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux命令 打包 tar cvf data0.tar data0/ 解压 tar xvf data0.tar ../data1]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark DataFrame 常用函数及常见问题]]></title>
    <url>%2F2018%2F06%2F13%2FSpark-DataFrame-%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[问题 parquet格式错误 //需注意Hive版本需要低于2.1.0 SparkSession().config(&quot;spark.sql.hive.convertMetastoreParquet&quot;,&quot;false&quot;) 列字段存在但找不到 SparkSession().config(&quot;spark.sql.parquet.filterPushdown&quot;, &quot;false&quot;) 表字段过多不显示 SparkSession().config(&quot;spark.debug.maxToStringFields&quot;, &quot;100&quot;) spark ml算法管道predict产生的df字段，其中label对应predictedLabel(int类型)，indexedLabel对应prediction(double类型)]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
      <tags>
        <tag>spark df函数</tag>
        <tag>spark df问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Shell命令]]></title>
    <url>%2F2018%2F06%2F13%2FHadoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Hadoop命令 查询目录文件大小 hdfs dfs -du -h / 从hdfs取文件到linux本地 hdfs dfs -get /user/portrait/scgd/data0 ./output/ 上传文件到hdfs上 hdfs dfs -put ./output/ /user/portrait/scgd/data0]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
      <tags>
        <tag>Hadoop命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘领域的机会和挑战]]></title>
    <url>%2F2018%2F06%2F10%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%2F</url>
    <content type="text"><![CDATA[机会新搜索 面向知识工作者 IT工程师、记者、律师、销售、猎头、医生、公务员等等 组织内外数据相结合 以领域知识图谱为核心数据处理平民化 数据、算法和代码共享和交易 基于非表格数据的BI 业务人员处理大数据 挑战多源:整合多种来源的数据 网页数据 API数据 企业内部数据异构:多格式、非表格 通用网页提取算法 算法工厂，算法市场 标注工场，标注众包知识计算 构建实体，寻找关系 基于规则 端到端机器学习机器学习 用于知识计算 用于目标数据处理 用于数据库系统索引优化 用于数据库系统查询计划优化硬件加速 GPU、众核、内存、高速网络云+端 算法超市 如针对不同数据集的数据提取算法 数据超市 原始数据、标注、表格、知识均可交易 公有云、私有云和边缘计算[/cp]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机会和挑战</tag>
      </tags>
  </entry>
</search>
