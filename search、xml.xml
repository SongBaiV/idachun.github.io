<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[自然语言处理概述]]></title>
    <url>%2F2018%2F06%2F15%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1 相关术语 分词常用的手段是基于字典的最长串匹配。 词性标注词性一般指动词、名词、形容词等，比如我/r爱/v北京/ns天安门/ns。ns代表名词，v代表动词，都是标注。 命名实体识别指从文本中识别具有特定类别的实体（通常是名词），例如人名、地名等。 句法分析往往最终生成的结果是一棵句法分析树。句法分析可以解决传统词袋模型不考虑上下文的问题。“小李是小杨的班长”和“小杨是小李的班长”，这两句话，用词袋模型是相同的，但是句法分析可以分析出其中的主从关系。 指代消解中文中代词出现的频率很高，它的作用的是用来表征前文出现过的人名、地名等。 情感识别本质上是分类问题，常应用在舆情分析等领域。情感一般分为两类，即正面、负面，也可以在前面基础上加上中性类别。电商企业，情感识别可以分析商品评价的好坏，以此作为下一个环节的评判依据。通常可以基于词袋模型+分类器，或者现在流行的词向量模型+RNN。经过测试发现，后者比前者准确率略有提升。 纠错在搜索技术和输入法等领域，可以基于N-Gram进行纠错，也可以通过字典树、有限状态机等方法进行纠错。 问答系统往往需要语音识别、合成，自然语言理解、知识图谱等多项技术的配合才会实现得比较好。 2 自然语言处理层面 词法分析包括汉语的分词和词性标注这两部分。 句法分析业界存在三种比较主流的句法分析方法。 短语结构句法体系，作用是识别出句子中的短语结构以及短语之间的层次句法关系 依存结构句法体系，作用是识别句子中词与词之间的相互依赖关系 深层文法句法分析，利用深层文法，例如词汇化树邻接文法，组合范畴文法等对句子进行深层的句法以及语义分析。复杂度高，不适合大规模数据。 语义分析最终目的是理解句子表达的真实语义。但是，语义应该采用什么表示形式一直困扰着研究者们，没有统一答案。语义角色标注是目前比较成熟的浅层语义分析技术。语义角色标注一般都在句法分析的基础上完成，句法结构对于语义角色标注的性能至关重要。 3 正则表达式在NLP中的应用正则表达式的作用之一是将这些文档内容从非结构化转为结构化以便后续的文本挖掘，另一个作用就是去除“噪声”。 3.1 匹配字符串 1234567re.search(regex,string)#表示字符串是否有能够匹配，返回值True或者Dalsere.findall(regex,string)#返回所有被匹配到的字符或字符串list1=str1.split('。')regex='^爬.'for line in list1: if re.search(regex,line): print line 3.2 特殊符号 123456'.'， 匹配任意一个字符'^a'， 匹配所有以字母a开头的字符串'a$'， 匹配所有以字母a结尾的字符串'[bcr]at'， 匹配“bat”“cat”以及“rat”。如果是要匹配的是[]，需要前面加上转义字符\，即\[\]r'\\'， 匹配\'[1-2][0-9]&#123;3&#125;'，匹配1000-2999之间的数字，3表示前面重复3次，即[0-9][0-9][0-9] 4 中文分词主要归纳为“规则分词”“统计分词”和“混合分词（规则+统计）”这三个主要流派。规则分词是最早兴起的方法，主要是通过人工设立词库，按照一定方式进行匹配切分，其实现简单高效，但对新词很难进行处理。随后统计机器学习技术的兴起，应用于分词任务上后，就有了统计分词，能够较好应对新词发现等特殊场景。然而实践中，单纯的统计分词也有缺陷，那就是太过于依赖语料的质量，因此实践中多是采用这两种方法的结合，即混合分词。 4.1 规则分词基于规则的分词是一种机械分词方法，主要是通过维护词典，在切分语句时，将语句的每个字符串与词表中的词进行逐一匹配，找到则切分，否则不予切分，按照匹配切分的方式，有三种。基于规则的分词简单高效，但是词典维护工程大，因为新词层出不穷。 4.1.1 正向最大匹配法首先有词典和待切分字符串，假设词典词语最大长度为i，从左向右取字符串前i个字符，然后匹配词典，如果匹配不了，则取前i-1个字符，继续匹配，直至匹配上或者只剩1个字符了。然后原始字符串去掉这个字符，继续重复同样的操作，取前i个字符。 4.1.2 逆向最大匹配法首先字典需要逆序字典，然后从右向左取前i个字符，匹配，匹配不了，取前i-1个字符等，其他和正向最大匹配法原理一样。 4.1.3 双向最大匹配法将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结果进行比较，然后按照最大匹配原则，选取最终切分词的数量最少的作为结果。 4.2 统计分词基于统计的分词，一般两个步骤：(1)建立统计语言模型(2)对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式，用到了统计学习算法，如隐马尔科夫模型(HMM)、条件随机场(CRF)等 4.2.1 语言模型语言模型就是求长度为m的字符串的概率分布P(ω1,ω2,…,ωn),其中ω1到ωn代表文本中的各个字，P(ω1，ω2，…，ωm)=P(ω1)P(ω2|ω1)P(ω3|ω1，ω2)…P(ωi|ω1，ω2，…，ωi-1)…P(ωm|ω1，ω2，…，ωm-1)。当文本过长时，计算难度大。一般用n元模型来降低难度，n元模型忽略距离大于等于n的上文词的影响。当n=2时，称为二元模型，只考虑前面1个词，P(ωi|ω1，ω2，…，ωi-1)=P(ωi|ωi-1)，一般二元模型采用的多。一般使用频率计数的比例来计算n元条件概率,可用拉普拉斯平滑算法解决分子分母为0的问题 4.2.2 HMM模型 每个字在构造一个特定的词语时都占据着一个词位，即B（词首）、M（词中）、E（词尾）和S（单独成词）。通过求得每个字的词位，即可分词。 用λ=λ1λ2…λn代表输入的句子，n为句子长度，λi表示字，o=o1o2…on代表输出的标签，那么理想的输出即为：max=maxP(o1o2…on|λ1λ2…λn)，o即为B、M、E、S这4种标记。 期望求解的是max P(o|λ)，通过贝叶斯公式转而求解max P(λ|o)P(o)(分母为常数不需要求)。 假设一，马尔科夫假设。P(λ|o)=P(λ1|o1)P(λ2|o2)…P(λn|on) 假设二，齐次马尔科夫假设，每个输出仅仅与上一个输出有关，即二元模型。P(o)=P(o1)P(o2|o1)P(o3|o2)…P(on|on-1) P(λ|o)P(o)～P(λ1|o1)P(o2|o1)P(λ2|o2)P(o3|o2)…P(on|on-1)P(λn|on) （3.10）。在HMM中，将P(λk|ok)称为发射概率，P(ok|ok-1)称为转移概率。HMM中，求解maxP(λ|o)P(o)的常用方法是Veterbi算法，它是一种动态规划方法。 语料库为分好词的句子，比如这里。实际项目实战中，读者可通过扩充语料、词典补充等手段予以优化。 4.2.3 其他统计分词算法 条件随机场是一种基于马尔可夫思想的统计模型，每个状态不止与他前面的状态有关，还与他后面的状态有关。 神经网络分词算法通常采用CNN、LSTM等深度学习网络自动发现一些模式和特征，然后结合CRF、softmax等分类算法进行分词预测。 4.2.4 优缺点 对比机械分词法，这些统计分词方法不需耗费人力维护词典，能较好地处理歧义和未登录词，是目前分词中非常主流的方法。但其分词的效果很依赖训练语料的质量，且计算量相较于机械分词要大得多。 4.3 混合分词目前不管是基于规则的算法、还是基于HMM、CRF或者deep learning等的方法，其分词效果在具体任务中，其实差距并没有那么明显。实际工程应用中，多是基于一种分词算法，然后用其他分词算法加以辅助。最常用的方式就是先基于词典的方式进行分词，然后再用统计分词方法进行辅助。在保证词典分词准确率的基础上，对未登录词和歧义词有较好的识别，Jieba分词工具便是基于这种方法的实现。 4.4 JieBaJieba不是只有分词这一个功能，其是一个开源框架，提供了很多在分词之上的算法，如关键词提取、词性标注等。提供了很多热门社区项目的扩展插件，如ElasticSearch、solr、lucene等。Jieba分词结合了基于规则和基于统计这两类方法。首先基于前缀词典进行词图扫描，前缀词典是指词典中的词按照前缀包含的顺序排列，例如词典中出现了“上”，之后以“上”开头的词都会出现在这一部分，例如“上海”，进而会出现“上海市”。基于前缀词典可以快速构建包含全部可能分词结果的有向无环图。基于标注语料，使用动态规划的方法可以找出最大概率路径，并将其作为最终的分词结果。对于未登录词，Jieba使用了基于汉字成词的HMM模型，采用了Viterbi算法进行推导。 4.4.1 三种分词模式精确模式：试图将句子最精确地切开，适合文本分析，默认精准模式，jieba.cut(str1,HMM=True)。全模式：把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义，jieba.cut(str1,cut_all=True)。搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词,jieba.cut_for_search(str1)。 4.4.2 去除停用词通用的停用词表,一般实践过程中，需要根据自己的任务，定期更新维护。 4.4.3 提高分词效果需要定制自己的领域词典，用以提升分词的效果。Jieba分词就提供了这样的功能，用户可以加载自定义词典，jieba.load_userdict(‘./data/user_dict.txt’) 5 词性标注与命名实体识别 5.1 词性标注词性，即名词、动词、形容词等 5.1.1 词性标注最简单的方法是从语料库中统计每个词所对应的高频词性，将其作为默认词性。目前较为主流的方法是如同分词一样，将句子的词性标注作为一个序列标注问题来解决，分词中常用的如隐含马尔可夫模型、条件随机场模型等皆可在词性标注任务中使用。 5.1.2 中文领域中尚无统一的标注标准，较为主流的主要为北大的词性标注集和宾州词性标注集两大类，下图为部分北大词性标注集。 5.1.3 Jieba分词中的词性标注Jieba的词性标注同样是结合规则和统计的方式，具体为在词性标注的过程中，词典匹配和HMM共同作用。 正则表达式进行汉字判断，中文范围 ‘\u4E00-\u9FA5’,前面加上^，表示排除中文re.compile(u’[^\u4E00-\u9FD5]’).sub(r’ ‘, str1) 在前缀词典中找出它所分出的词性，若在词典中未找到，则赋予词性为“x”（代表未知）。当然，若在这个过程中，设置使用HMM，且待标注词为未登录词，则会通过HMM方式进行词性标注。 若不符合上面的正则表达式，那么将继续通过正则表达式进行类型判断，分别赋予“x”“m”（数词）和“eng”（英文）。 在词性标注任务中，Jieba分词采用了simultaneous思想的联合模型方法，即将基于字标注的分词方法与词性标注结合起来，使用复合标注集。比如，对于名词“人民”，它的词性标注是“n”，而分词的标注序列是“BE”，于是“人”的标注就是“B_n”，“民”的标注就是“E_n”。1234567891011121314import jieba.posseg as psgsent='中文分词是文本处理不可或缺的一步!'sent_list=psg.cut(sent)for w,t in sent_list: print w,t 中文 nz分词 n是 v文本处理 n不可或缺 l的 uj一步 m! x 5.2 命名实体识别 5.2.1 简介 目的是识别语料中人名、地名、组织机构名等命名实体。由于这些命名实体数量不断增加，通常不可能在词典中穷尽列出，且其构成方法具有各自的规律性，因此，通常把对这些词的识别在词汇形态处理（如汉语切分）任务中独立处理，称为命名实体识别（Named Entities Recognition，NER）。数量、时间、日期、货币等实体识别通常可以采用模式匹配的方式获得较好的识别效果。 命名实体识别更侧重高召回率，但在信息检索领域，高准确率更重要。 命名实体识别效果的评判主要看实体的边界是否划分正确以及实体的类型是否标注正确。 相较于实体类别标注子任务，实体边界的识别更加困难。 5.2.2 识别方法分词主要有三种方式，主要有基于规则的方法、基于统计的方法以及二者的混合方法。这在整个NLP的各个子任务基本上也多是同样的划分方式，命名实体识别也不例外。 基于规则的命名实体识别：依赖手工规则的系统，结合命名实体库，对每条规则进行权重赋值，然后通过实体与规则的相符情况来进行类型判断。 基于统计的命名实体识别：主流的基于统计的命名实体识别方法有隐马尔可夫模型、最大熵模型、条件随机场等。基于人工标注的语料，将命名实体识别任务作为序列标注问题来解决。对语料库的依赖比较大，而大规模通用语料库又比较少。 混合方法:在很多情况下是使用混合方法，结合规则和统计方法。借助规则知识提前进行过滤修剪处理，序列标注方式是目前命名实体识别中的主流方法。 5.2.3 条件随机场在大量真实语料中，观察序列更多的是以一种多重的交互特征形式表现出来，观察元素之间广泛存在长程相关性，HMM的效果就受到了制约。条件随机场的思想来源于HMM，每个状态不止与他前面的状态有关，还与他后面的状态有关。 由n个字符构成的NER的句子，每个字符的标签都在已知标签集合（“B”“M”“E”“S”和“O”）中，为每个字符选定标签后，形成了一个随机场(注意，标签是选定的)。加一些约束，如所有字符的标签只与相邻的字符的标签相关，那么即马尔可夫随机场问题。 假设马尔可夫随机场中有X和Y两种变量，X一般是给定的，Y是在给定X条件下的输出。如X是字符，Y为标签，P(X|Y)就是条件随机场。通过贝叶斯，最后转化求max P(y|x)。实际中，假设X和Y结构相同，即X=(X1，X2，X3，…，Xn)，Y=(Y1，Y2，Y3，…，Yn) 定义：设X=(X1，X2，X3，…，Xn)和Y=(Y1，Y2，Y3，…，Yn)均为随机变量序列，若在X的条件下，Y的条件概率分布P(Y|X)构成条件随机场，且满足马尔可夫性：P(Yi|X，Y1，Y2，…，Yn)=P(Yi|X，Yi-1，Yi+1) ，则称P(Y|X)为线性链的条件随机场，一般所说的CRF指的就是线性链CRF。 对句子“我来到牛家村”进行标注，正确标注后的结果应为“我/O来/O到/O牛/B家/M村/E”。采用线性链CRF来进行解决，（O，O，O，B，M，E）是其一种标注序列，（O，O，O，B，B，E）是另一种，选择很多，NER任务中就是找出最靠谱的作为句子标注。 在CRF中，定义一个特征函数集合，然后使用这个特征集合为标注序列进行打分，据此选出最靠谱的标注序列。比如如果标注中出现连续两个“B”结构的标注序列，则给它低分。 在CRF中有两种特征函数，分别为转移函数tk(yi-1，yi，i)和状态函数sl(yi，X，i)。前者依赖于当前和前一个位置，表示从标注序列中位置i-1的标记yi-1转移到位置i上的标记yi的概率。后者依赖当前位置，表示标记序列在位置i上为标记yi的概率。 通过特征函数打分，使得 arg max P(y|x) ，求得合适的标记y，x为特征函数中的x。该问题与HMM求解最大可能序列路径一样，也是采用的Veterbi算法。 解决标注问题时，HMM和CRF效果都比较好，不过CRF能够捕捉全局的信息和灵活的特征设计，因此一般效果要比HMM好，但实现复杂度高。 5.2.4 总结当将分词、词性标注和命名实体识别都作为标注任务时，采用HMM和CRF都是可行的，不同的是标签的区别。在命名实体识别中，在切完词、标注完词性后，再做识别任务，效果要比单纯的字标注要好很多。 5.3 实战 5.3.1 日期识别 背景现有一个基于语音问答的酒店预订系统，其根据用户的每句语音进行解析，识别出用户的酒店预订需求，如入住时间等。用户的语音在发送给后台进行请求时已经转换成中文文本，然而由于语音转换工具的识别问题，许多日期类的数据并不是严格的数字，会出现诸如“六月12”“2016年八月”“20160812”“后天下午”等形式。这里识别出每个请求文本中可能的日期信息，并将其转换成统一的格式进行输出。例如“我要今天住到明天”（假设今天为2017年10月1号），那么通过日期解析后，应该输出为“2017-10-01”和“2017-10-02”。 方法 自定义规则，“今天”“明天”对时间的映射。这里由于是酒店入住，基本不会出现“前天”“昨天”等情况，因此未予添加 利用Jieba词性标注的功能，提取其中“m”（数字）“t”（时间）词性的词 正则表达式匹配 优缺点 相较于基于统计的方法，规则方法无须在系统建设初期为搜集数据标注训练而苦恼，能够快速见效。 采用规则去覆盖所有的语言场景是不太现实的。 5.3.2 地名识别采用基于条件随机场的方法来完成地名识别任务，CRF++的安装，Windows系统用户可去官网https://taku910.github.io/crfpp/ 下载二进制版本，Linux或Mac用户可从Github（ https://github.com/taku910/crfpp ）或官网获取源码进行安装。 1 确定标签体系如同分词和词性标注一样，命名实体识别也有自己的标签体系。可以按照自己的想法自行设计，也采用地理位置标记规范，即针对每个字符标记为“B”“E”“M”“S”“O”中的一个。 2 语料数据处理 CRF++的训练数据要求一定的格式，一般是一行一个token，一句话由多行token组成，多个句子之间用空行分开。其中每行又分成多列，除最后一列以外，其他列表示特征。因此一般至少需要两列，最后一列表示要预测的标签（“B”“E”“M”“S”“O”）。比如只采用字符这一个维度作为特征，以“我去北京饭店。”为例，结果如下（最后一行为空行）： 1234567我 O 去 0北 B京 M饭 M店 E。 O 语料数据：比如1998年人民日报分词数据集，其主要是一个词性标注集。但可以使用其中被标记为“ns”的部分来构造地名识别语料。 3 特征模块设计 CRF有特征函数，它是通过定义一些规则来实现的，而这些规则就对应着CRF++中的特征模板。 CRF++有两种模板类型，第一种是字母U开头，为Unigram template，CRF++会自动为其生成一个特征函数集合。第二种以字母B开头，表示Bigram template，系统会自动产生当前输出与前一个输出token的组合，根据该组合构造特征函数。 特征模板需要自己定义 4 模型训练与调试训练和测试的命令：crf_learn、crf_test。这里考虑的特征维度也少（只考虑了字符本身维度），若采用词性标注后的文本作为语料，将词性作为特征加入训练集中，会使模型效果提升。 5 其他问题该程序针对一些场景能够很好地进行识别，但是在遇到诸如“回龙观”“南锣鼓巷”“北京南站”等词时识别效果并不好。这种情况在实际项目中会经常遇到，通常有以下解决办法： 扩展语料，改进模型。如加入词性特征，调整分词算法等。 整理地理位置词库。在识别时，先通过词库匹配，再采用模型进行发现。 6 关键词提取算法 6.1 概述 算法分为有监督和无监督两类。 有监督通过分类进行，构建一个词表，通过判断每个文档与词表中每个词的匹配程度，以类似打标签的方式，达到关键词提取的效果。有监督较高精度，但缺点是需要大批量的标注数据，人工成本高。固定的词表有时很难将新信息的内容表达出来，人工维护成本高。 无监督不需要人工生成、维护的词表，也不需要人工标准语料辅助进行训练。常用的有TF-IDF算法、TextRank算法和主题模型算法（包括LSA、LSI、LDA等）。 6.2 TF-IDF词频-逆文档频率，词频需要做归一化，整个文档中频次/总词数，BOW是词袋模型，即词的出现次数，未归一化处理。TF-IDF公式如下：文本中还有信息能对关键词提取起作用，比如每个词的词性、出现的位置等。在文本中，名词带有更多关键信息，对名词赋予更高权重，能使提取出来的关键词更合理。某些场景中，文本的起始段落和末尾段落比起其他部分的文本更重要，对出现在这些位置的词赋予更高权重，也能提高关键词提取。 6.3 TextRank算法可以脱离语料库(整个数据集)的背景，仅对单篇文档进行分析就可以提取该文档的关键词。 6.4 LSA/LSI/LDA算法 6.4.1 介绍 有些关键词并不一定会显式地出现在文档当中，如一篇讲动物生存环境的科普文，通篇介绍了狮子老虎鳄鱼等各种动物的情况，但是文中并没有显式地出现动物二字，这种情况下，前面的两种算法不能提取出动物这个隐含的主题信息，这时候就需要用到主题模型。 主题模型认为在词与文档之间没有直接的联系，它们应当还有一个维度将它们串联起来，主题模型将这个维度称为主题。 每个文档对应着一个或多个的主题，而每个主题对应着一个或多个的词语，通过主题，就可以得到每个文档的词分布。 已知数据集中，每个词和文档对应的p(wi|dj)都是已知的。而主题模型就是根据这个已知信息，计算p(wi|tk)和p(tk|dj)的值，含义可以理解为一篇文档中某个词语的重要性=一篇文档中属于某个主题的重要性*某个词语在该主题中的重要性，公式: p(wi|tk)和p(tk|dj)的求解常用方法就是LSA（LSI）和LDA。其中LSA主要是采用SVD（奇异值分解）的方法进行暴力破解，而LDA则是通过贝叶斯学派的方法对分布信息进行拟合。 6.4.2 LSA/LSI算法LSA（Latent Semantic Analysis，潜在语义分析）和LSI（Latent Semantic Index，潜在语义索引），通常被认为是同一种算法。LSA和LSI都是对文档的潜在语义进行分析，但是潜在语义索引在分析后，还会利用分析的结果建立相关的索引。 步骤 使用BOW模型将每个文档表示为向量 将所有的文档词向量拼接起来构成词–文档矩阵（m×n） 对词–文档矩阵进行奇异值分解（SVD）操作（[m×r]·[r×r]·[r×n]） 根据SVD的结果，将词–文档矩阵映射到一个更低维度k（[m×k]·[k×k]·[k×n]，0&lt;k&lt;r）的近似SVD结果，每个词和文档都可以表示为k个主题构成的空间中的一个点。计算每个词和文档的相似度（相似度计算可以通过余弦相似度或者是KL相似度进行，第一个矩阵的每一行(长度k，代表词语)和第三个矩阵的每一列(长度k，代表文档)计算相似度），可以得到每个文档中对每个词的相似度结果，取相似度最高的一个词即为文档的关键词(这个词不一定是构成文档中的词语，可能是通过训练集获得的新词)。 不足1)SVD的计算复杂度高，特征空间维度大时计算效率低下。当新文档进入到已有特征空间时，需要对整个空间重新训练，以得到加入新文档后对应的分布信息。2)改进提出了pLSA算法，通过使用EM算法对分布信息进行拟合替代了使用SVD进行暴力破解，一定程度上解决了LSA的部分缺陷，但是LSA仍有较多不足。3)在pLSA的基础上，引入了贝叶斯模型，实现了现在topic model的主流方法——LDA（Latent Dirichlet Allocation，隐含狄利克雷分布）。 6.4.3 LDA算法 理论基础是贝叶斯理论，拟合出词–文档–主题的分布。 假设文档中主题的先验分布和主题中词的先验分布都服从狄利克雷分布（隐含狄利克雷分布），先验分布+数据（似然）=后验分布。对已有数据集的统计，得到每篇文档中主题的多项式分布和每个主题对应词的多项式分布。 通过先验的狄利克雷分布和观测数据得到的多项式分布，得到一组Dirichlet-multi共轭，并据此来推断文档中主题的后验分布和主题中词的后验分布，就是需要的结果，其中一种主流的方法就是吉布斯采样。 通过这个分布信息计算文档与词的相似性，继而得到文档最相似的词列表。 6.4.4 总结一般情况下，使用词性过滤仅保留名词作为关键词更符合要求，但有些场景对其他词性的词有特殊要求，可以根据场景选择需要过滤的词性。还可以通过调整关键词提取数量、主题模型的主题数量等参数以及增大训练集的方式提高模型表现效果。 7 句法分析 7.1 概述 介绍 句法分析是机器翻译的核心技术，主要任务是识别出句子所包含的句法成分以及这些成分之间的关系，一般以句法树来表示句法分析的结果。 句法分析（Parsing）是从单词串得到句法结构的过程，而实现该过程的工具或程序被称为句法分析器（Parser）。 分为完全句法分析和局部句法分析两种，差别在于：完全句法分析以获取整个句子的句法结构为目的；而局部句法分析只关注于局部的一些成分，例如常用的依存句法分析就是一种局部分析方法。 本质是一套面向候选树的评价方法，会给正确的句法树赋予一个较高的分值，而给不合理的句法树赋予一个较低的分值，这样就可以借用候选句法树的分值进行消歧。 难点 歧义 搜索空间大句法分析是一个极为复杂的任务，候选树个数随句子增多呈指数级增长，搜索空间巨大，因此必须设计出合适的解码器。 方法可以简单地分为基于规则的方法和基于统计的方法两大类。基于规则的方法在处理大规模文本时，语法规则覆盖有限。随着大规模标注树库的建立，基于统计学习模型的句法分析方法开始兴起，当下最流行的是PCFG（Probabilistic Context Free Grammar）方法。 7.2 句法分析的数据集与评测方法 数据集句法分析的数据集是一种树形的标注结构，称为树库，如下所示：中文树库著名的有中文宾州树库（Chinese TreeBank，CTB）、清华树库（Tsinghua Chinese TreeBank，TCT）和台湾中研院树库，其中CTB是目前绝大多数的中文句法分析研究的基准语料库。不同树库的标记体系不一样，解释不能通用，下图为清华数库部分标记： 评测方法主流句法分析评测方法是PARSEVAL评测体系，指标有准确率、召回率、交叉括号数。交叉括号表示分析得到的某一个短语的覆盖范围与标准句法分析结果的某个短语的覆盖范围存在重叠又不存在包含关系，即构成了一个交叉括号。 7.3 常用方法相较于词法分析（分词、词性标注或命名实体识别等），句法分析成熟度要低上不少。以短语结构树为目标的句法分析器，目前应用最广泛，与很多其他形式语法对应的句法分析器都能通过对短语结构语法（特别是上下文无关文法）的改造而得到，句法分析属于NLP中较为高阶的问题。 基于PCFG的句法分析PCFG是上下文无关文法的扩展，可以计算分析树的概率值。PCFG衍生出了各种形式的算法，包括基于单纯PCFG、基于词汇化和基于子类划分PCFG的句法分析方法等。 基于最大间隔马尔可夫网络的句法分析最大间隔是SVM中的理论，马尔可夫网络是概率图模型中一种具备一定结构处理关系能力的算法。最大间隔马尔可夫网络（Max-Margin Markov Networks）是这两者的结合，能够解决复杂的结构化预测问题，尤为适合用于句法分析任务。这是一种判别式的句法分析方法，通过丰富特征来消解分析过程中产生的歧义。如果要实现多元分类，可以使用多个二分类模型，每个模型分别识别一个短语标记，组合多分类器即可完成句法分析。 基于CRF的句法分析当将句法分析作为序列标注问题来解决时，同样可以采用条件随机场（CRF）模型。可以采用清华树库，设计特征模板，然后训练一个基于CRF++的模型，并进行测试。 基于移进–归约的句法分析模型移动-规约是一种自下而上的方法，从输入串开始，逐步进行“规约”，操作基本数据结构是堆栈。应用于中文时，对词性非常敏感，需要和准确度较高的词性标注工具一块使用。 7.4 使用Stanford Parser的PCFG算法进行句法分析 介绍 Stanford Parser是斯坦福大学自然语言小组开发的开源句法分析器，是基于概率统计句法分析的一个Java实现。以权威的宾州树库作为分析器的训练数据，还支持分词和词性标注、短语结构、依存关系等输出，支持多个语言接口。 安装 需安装JDK，底层JAVA实现 需安装nltk库，因为Python封装是在nltk库中实现的，主要使用nltk.parse中的Stanford模块。 需要下载Stanford Parser的jar包，主要有两个：stanford-parser.jar和stanford-parser-3.8.0-models.jar。 实战对“他骑自行车去了菜市场”这句话进行句法分析以及可视化操作，代码中Stanford Parser的句法分析器接收的输入是分词完后以空格隔开的句子，stanford-parser-3.8.0-models.jar是已经训练好的模型，解压jar包可以看到支持的算法。 7.5 总结 和词法分析（分词、词性标注和命名实体识别等）相比，句法分析算法实际性能离真正实用化有距离，原因在于语言学理论和实际的自然语言应用之间存在很大差距。 实践中，句法分析常结合一定规则来辅助解决一些任务。如模板解析类的任务，通过句法分析进行语义标注，提取其中主谓宾关系，再通过规则模板标出重要的角色信息和行为。 8 文本向量化]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[推荐系统概述]]></title>
    <url>%2F2018%2F06%2F11%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1. 概述 长尾效应就在于它的数量上，将所有非流行的市场累加起来就会形成一个比流行市场还大的市场。这也是数据挖掘中，提高预测的关键。用户活跃度和物品流行度均成长尾分布，接近直线。二者关系，一般认为，新用户倾向于浏览热门的物品，因为他们对网站还不熟悉，只能点击首页的热门物品，而老用户会逐渐开始浏览冷门的物品。用户越活跃，越倾向于浏览冷门的物品。 个性化广告投放，发展为一门独立学科，计算广告学。主要三种场景，网页上下文插入广告，搜索广告和个性化展示广告。 隐性反馈数据集，特点是只有正样本（用户喜欢什么物品），而没有负样本（用户对什么物品不感兴趣）。 推荐准确率在有的场景下不是一个好的评测指标，比如在亚马逊书店中，用户迟早会买这本书，推荐只是方便了用户，但并没有带来额外的利润。一方面没有让用户感到惊喜，一方面没有带来潜在的用户。 推荐系统3种评测推荐结果实验方法，离线实验、调查问卷和在线实验，一般都需要完成这3个实验。 推荐算法常用3种方式联系用户兴趣和物品：一，推荐与用户喜欢过的物品相似的物品(通过用户行为计算)，ItemCF；二，推荐与用户类似用户喜欢的物品，UserCF;三，推荐那些具有用户喜欢的特征的物品，隐语义模型等。第四种，则是UGC标签方法，普通用户自己给内容打标签，比如给电影打标签。 在线实验，常用AB测试。将不同用户分组，用不同算法推荐，最后看评测指标。一般周期比较长，所以不能上线所有算法，一般只测试在离线实验和调查问卷中表现比较好的算法。 大型网站需要设计合理的AB测试系统，比如前端网页界面AB测试，后台推荐算法也在做AB测试，结果会互相干扰。因此，切分流量很关键。 推荐系统中，根据现实需求会有不同的推荐任务，也需要不同的特征，而特征数目一般也比较大。如果要在一个系统中把各种特征和任务都统筹考虑，那么系统将会非常复杂，而且很难通过配置文件方便地配置不同特征和任务的权重。因此，推荐系统需要由多个推荐引擎组成，每个推荐引擎负责一类特征和一种任务，而推荐系统的任务只是将推荐引擎的结果按照一定权重或者优先级合并、排序然后返回。 2. 评测指标2.1 用户满意度调查问卷、购买率、点击率、用户停留时间等 2.2 预测准确度 评分预测均方根误差RMSE，绝对值误差MAE。RMSE加大了对预测不准的惩罚(平方项的惩罚)，评分系统基于整数指定的，取整的MAE降低了误差(实际误差大) Top N推荐准确率和召回率。为了全面评测，一般会选取不同推荐列表的长度，得到一组准确率和召回率，然后画出准确率和召回率曲线。 覆盖率覆盖率描述一个推荐系统对物品长尾的发掘能力，最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例，常用的指标有信息熵和基尼系数，二者值越小越好。 多样性评测多样性和相似性是相反的，通过计算相似性可以评测多样性。假设用户喜欢动作片和动画片，且用户80%的时间在看动作片，20%的时间在看动画片。C列表中有8部动作片和2部动画片，是一个比较好的推荐。 新颖性评测新颖度的最简单方法是利用推荐结果的平均流行度，因为越不热门的物品越可能让用户觉得新颖。因此，如果推荐结果中物品的平均热门程度较低，那么推荐结果就可能有比较高的新颖性。 惊喜度新颖性和惊喜度区别是，比如给出的推荐影片与用户历史兴趣相关大但用户不了解这个影片，这叫做新颖性；给出的推荐影片与历史兴趣相关性低，但用户看后觉得不错，这叫惊喜度。定义惊喜度需要首先定义推荐结果和用户历史上喜欢的物品的相似度，其次需要定义用户对推荐结果的满意度，目前并没有公认的指标定义。 信任度：度量推荐系统的信任度只能通过问卷调查的方式，询问用户是否信任推荐系统的推荐结果。 实时性一方面，推荐系统需要实时地更新推荐列表来满足用户新的行为变化。比如，当一个用户购买了iPhone，如果推荐系统能够立即给他推荐相关配件，那么肯定比第二天再给用户推荐相关配件更有价值。很多推荐系统都会在离线状态每天计算一次用户推荐列表，然后于在线期间将推荐列表展示给用户。这种设计显然是无法满足实时性的。与用户行为相应的实时性，可以通过推荐列表的变化速率来评测。如果推荐列表在用户有行为后变化不大，或者没有变化，说明推荐系统的实时性不高。另一方面是推荐系统需要能够将新加入系统的物品推荐给用户。这主要考验了推荐系统处理物品冷启动的能力。 健壮性：防攻击 时间多样性推荐系统每天推荐结果的变化程度被定义为推荐系统的时间多样性，如果用户没有行为:(1)在生成推荐结果时加入一定的随机性,比如从推荐列表前20个结果中随机挑选10个结果展示给用户，或者按照推荐物品的权重采样10个结果展示给用户;(2)记录用户每天看到的推荐结果，然后在每天给用户进行推荐时，对他前几天看到过很多次的推荐结果进行适当地降权;(3)每天给用户使用不同的推荐算法。可以设计很多推荐算法，比如协同过滤算法、内容过滤算法等，然后在每天用户访问推荐系统时随机挑选一种算法给他进行推荐。 3. 协同过滤算法3.1 UserCF 原理用户相似度通过余弦相似度来计算，通过建立物品到用户的倒排表来简化计算。然后利用UserCF算法算出用户u和物品i的兴趣度(利用了和用户u最相似的K个用户)，然后对兴趣度排序，推荐 Top N。改进的用户协同过滤，计算用户相似度后，使用UserCF算法可以加上一定的惩罚项，惩罚那些热门商品的权值，冷门物品更能说明用户的相似度，这种算法叫做User-IIF算法。 特点(1)可以让用户发现新奇的物品(2)适用于用户较少的场合，否则计算用户相似度矩阵代价很大(3)时效性较强，用户个性化兴趣不太明显的领域,比如新闻(4)用户行为实时性低,用户有新行为，不一定造成推荐结果的立即变化(5)冷启动，在新用户对很少的物品产生行为后，不能立即对他进行个性化推荐，因为用户相似度表是每隔一段时间离线计算的。有些场景下，比如新闻领域，用户除了推荐列表，总可以通过其他渠道看到其他新的新闻，从而这方面冷启动问题不敏感。其他场景，要根据物品的内容属性，推荐给喜欢类似物品的用户。(6)很难提供令用户信服的推荐解释(7)一般一天计算一次用户相似表(8)利用时间的特性，加上衰减因子 应用新闻推荐使用UserCF算法，一是因为新闻场景的个性化是粗粒度的，二是因为如果使用ItemCF算法，需要不断更新物品相似矩阵，一般一天更新一次，而新闻出来的速度非常快，新闻领域接受不了这个速度，UserCF则更新比较慢。在抓住热点和时效性的同时，UserCF也照顾了一定程度的个性化。除了新闻领域，其他很多场景用ItemCF算法更多一些。 3.2 ItemCF 原理基于物品的协同过滤算法，计算物品相似度不是利用物品的内容属性来计算，而是通过分析所有用户的行为记录计算物品之间的相似度。该算法认为，物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品B。相似度可以计算为，喜欢物品i的用户中有多少比例的用户也喜欢物品j，可以建立倒排表来简化计算。得到了物品相似度后，计算用户u对物品i的兴趣度，通过和(物品i最相似的K个集合和用户喜欢的物品集合交集)来计算，然后根据兴趣度排名。 特点(1)更加个性化，是用户历史兴趣的体现(2)适用于物品数明显小于用户数的场合，否则计算物品相似度矩阵代价很大(3)长尾物品丰富，用户个性化需求强烈的领域(4)实时性，用户有新行为，一定会导致推荐结果的实时变化(5)冷启动，频繁更新物品相似表(半小时，基于物品内容属性)(6)利用用户的历史行为给用户做推荐解释，可以令用户比较信服(7)一般一天计算一次物品相似表(基于用户行为计算)(8)利用时间的特性，加上衰减因子 3.3 内容过滤算法主要是对文本内容向量化及NLP方面的知识，计算TF-IDF，然后再利用余弦相似度等来计算内容相似度。 3.4 隐语义模型 原理隐语义模型LFM(Latent Factor Model),原始矩阵的每一行代表每一个用户u，矩阵的每一列代表着物品i。初始情况下，如果u点击了i，则u,i置1，否则置0。通过最小化损失函数，求得矩阵p(用户u和类别k的兴趣关系)和矩阵q(类别k中具体的物品i所占的比例关系)，p*q则代表用户u和i的兴趣度，然后按照兴趣度排名，取top N即可。 常见问题隐性反馈数据集上应用LFM解决TopN推荐的第一个关键问题就是如何给每个用户生成负样本，需要采样负样本。一，采样负样本时，要选取那些很热门，而用户却没有行为的物品；二，保证正负样本差不多数量相等。 3.5 基于图的算法基于图的模型，将用户物品用二分图表示，用户在左，物品在右，如果用户点击了物品，则连接左右两点，否则不连接。则此时，求解u和i的兴趣度，则是求两个节点的相关性。相关性可以由改进的PersonalRank算法求解。 3.6 LFM和UserCF/ItemCF的对比 离线计算时，用户数M,物品数N,UserCF的空间复杂度为O(MM),ItemCF空间复杂度为O(NN)。LFM类别个数为K时，空间复杂度为O(F*(N+M)),M和N很大时，LFM节省内存。 离线计算时，总体上LFM、UserCF、ItemCf时间复杂度差不多，LFM稍微高些，主要是有很次迭代。 实时推荐，ItemCF会随着用户实时行为实时更新推荐列表；LFM推荐时，需要计算用户对所有物品的兴趣权重返回topN，物品数很大时复杂度高不适合，同时因为生成推荐列表速度慢，不能实时推荐。 LFM无法提供很好的推荐解释，ItemCF可以 当数据集非常稀疏时，LFM的性能会明显下降，甚至不如UserCF和ItemCF的性能。 4. 冷启动问题 分类：成熟的系统中新用户冷启动(推荐什么)、成熟的系统中新物品冷启动(推荐给谁)、新开发的系统推荐什么 解决方案 非个性化推荐，热门物品 利用用户注册信息，年龄性别等 利用用户社交网络账号授权登录，推荐其好友喜欢的东西 登录时，给出一些物品让用户选择或者直接给出兴趣菜单，收集用户兴趣。给用户选择的物品选择也有算法，决策树的策略。 新加入的物品，利用其自身内容属性，计算相似的物品。常用算法有内容过滤算法ContentItemKNN、LDA等，通过不同话题的相似度计算物品的相似度，不同话题相似度常用指标KL散度。 专家给物品打特征标签，通过这些特征计算物品相似度。 5. 基于用户标签的推荐系统 利用标签做推荐系统，一个简单的算法是找到用户最常用的标签，然后在找这个标签下被用户最多观看的电影，可以利用TF-IDF的思想来改进。 简单算法中新用户和新物品的标签少，因此需要做标签拓展的工作，也就是找和标签相似的标签，即计算相似度。标签i,j的相似度简单计算，在用户行为中同时出现标签i,j的记录数/出现标签i的记录数。 标签系统中并不是所有标签都反应用户兴趣，比如”不喜欢”标签。因此需要做标签清理的工作，比如去除停用词、去除因分隔符不同的同义词等。 标签系统中基于图的推荐算法，一张图，三类顶点，一类是用户顶点，一类是物品顶点，一类是标签顶点。当用户对某个物品i打了某个标签a，连接用户-物品-标签a的边，并将权重设置为1。若边已存在，则权重加一。然后利用PersonalRank算法计算所有物品节点相对于当前用户节点在图上的相关性，然后按照相关性从大到小的排序，给用户推荐排名最高的N个物品。 参考1.项亮《推荐系统实践》]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas 降低占用内存方法]]></title>
    <url>%2F2018%2F06%2F08%2Fpandas-%E9%99%8D%E4%BD%8E%E5%86%85%E5%AD%98%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[检查DataFrame内存1data.info(memory_usage='deep') 检查每种类型大概用了多少内存 12345for dtype in ['float','int64','object']: selected_dtype = data.select_dtypes(include=[dtype]) mean_usage_b = selected_dtype.memory_usage(deep=True).mean() mean_usage_mb = mean_usage_b / 1024 ** 2 print("Average memory usage for &#123;&#125; columns: &#123;:03.2f&#125; MB".format(dtype,mean_usage_mb)) 定义方法，计算内存 1234567def mem_usage(pandas_obj): if isinstance(pandas_obj,pd.DataFrame): usage_b = pandas_obj.memory_usage(deep=True).sum() else: # we assume if not a df it's a series usage_b = pandas_obj.memory_usage(deep=True) usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes return "&#123;:03.2f&#125; MB".format(usage_mb) 数值类型int/float 12345678# code: raw_int=data.select_dtypes(include=['int64'])print mem_usage(raw_int)# 1658.00 MBdone_int=raw_int.apply(pd.to_numeric,downcast='integer')print mem_usage(done_int)# 362.00 MBcompare= pd.concat([raw_int.dtypes,done_int.dtypes],axis=1)compare.columns = ['before','after']compare.apply(pd.Series.value_counts) 123456# output:index before afterint8 NaN 66.0int16 NaN 8.0int32 NaN 21.0int64 95.0 NaN object类型，如果列取值不多，可以转化为category类型，用于可视化分析]]></content>
      <categories>
        <category>pandas</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[不平衡数据处理方式]]></title>
    <url>%2F2018%2F06%2F04%2F%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[惩罚正负样本的权重很多模型和算法中都有基于类别参数的调整设置，以scikit-learn中的SVM为例，通过在class_weight: {dict, ‘balanced’}中针对不同类别针对不同的权重，来手动指定不同类别的权重。如果使用其默认的方法balanced，那么SVM会将权重设置为与不同类别样本数量呈反比的权重来做自动均衡处理，计算公式为：n_samples / (n_classes * np.bincount(y)).比较简单高效。比如label=0样本数为10，label=1样本数为100,此时可以设置class_weight={0:1,1:0.1}来影响惩罚项系数C，进而影响损失函数的求解，使得权重大样本的惩罚项少，权重小的样本惩罚项多。 特征通过选择显著性特征 采用boosting/bagging等算法 抽样 过采样过采样比较广泛，最直接的方法是简单复制少数类样本形成多条记录，也可使用smote算法 123456789# 过抽样处理库SMOTEfrom imblearn.over_sampling import SMOTE # 建立SMOTE模型对象model_smote = SMOTE() # 输入数据并作过抽样处理x_smote_resampled, y_smote_resampled = model_smote.fit_sample(train,train_y) x_smote_resampled = pd.DataFrame(x_smote_resampled, columns=train.columns)y_smote_resampled = pd.DataFrame(y_smote_resampled,columns=['label']) 欠采样最直接的方法是随机地去掉一些多数类样本来减小多数类的规模 1234567# 欠抽样处理库RandomUnderSamplerfrom imblearn.under_sampling import RandomUnderSampler# 建立RandomUnderSampler模型对象model_RandomUnderSampler = RandomUnderSampler() # 输入数据并作过抽样处理x_RandomUnderSampler_resampled, y_RandomUnderSampler_resampled =model_RandomUnderSampler.fit_sample(train,train_y)]]></content>
      <categories>
        <category>数据预处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[时间序列问题转回归问题简述]]></title>
    <url>%2F2018%2F05%2F19%2F%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E8%BD%AC%E5%9B%9E%E5%BD%92%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[这种场景，一般是随着时间预测销量，比如销量预测、电量预测等原理参考链接 12345678# 核心函数import pandas as pdfrom pandas import DataFramefrom pandas import concatdir = '../input/'train = pd.read_table(dir + 'train_20171215.txt',engine='python')train.head(10) date day_of_week brand cnt 0 1 3 1 20 1 1 3 5 48 2 2 4 1 16 3 2 4 3 20 4 3 5 1 1411 5 3 5 2 811 6 3 5 3 1005 7 3 5 4 773 8 3 5 5 1565 9 4 6 1 1176 1234567891011121314151617181920212223242526272829303132def series_to_supervised(data, n_in=1, n_out=1, dropnan=True): """ Frame a time series as a supervised learning dataset. Arguments: data: Sequence of observations as a list or NumPy array. n_in: Number of lag observations as input (X). n_out: Number of observations as output (y). dropnan: Boolean whether or not to drop rows with NaN values. Returns: Pandas DataFrame of series framed for supervised learning. """ n_vars = 1 if type(data) is list else data.shape[1] df = DataFrame(data) cols, names = list(), list() # input sequence (t-n, ... t-1) for i in range(n_in, 0, -1): cols.append(df.shift(i)) names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)] # forecast sequence (t, t+1, ... t+n) for i in range(0, n_out): cols.append(df.shift(-i)) if i == 0: names += [('var%d(t)' % (j + 1)) for j in range(n_vars)] else: names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)] # put it all together agg = concat(cols, axis=1) agg.columns = names # drop rows with NaN values if dropnan: agg.dropna(inplace=True) return agg 1234time_cnt = list(train['cnt'].values)# nin 前看 nout后看 这个题目需要前看time2sup = series_to_supervised(data=time_cnt,n_in=276,dropnan=True) time2sup.head(10) var1(t-276) var1(t-275) var1(t-274) var1(t-273) … var1(t-4) var1(t-3) var1(t-2) var1(t-1) var1(t) 276 20 48 16 20 … 407 237 200 535 384 277 48 16 20 1411 … 237 200 535 384 303 278 16 20 1411 811 … 200 535 384 303 314 279 20 1411 811 1005 … 535 384 303 314 176 280 1411 811 1005 773 … 384 303 314 176 310 281 811 1005 773 1565 … 303 314 176 310 283 282 1005 773 1565 1176 … 314 176 310 283 261 283 773 1565 1176 824 … 176 310 283 261 342 284 1565 1176 824 802 … 310 283 261 342 171 285 1176 824 802 1057 … 283 261 342 171 242 12print train.shapeprint time2sup.shape (4773, 4)(4497, 277)4497+276=4773，最后一列var1(t)则为label,表示此刻的值]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[正负样本的定义问题]]></title>
    <url>%2F2018%2F05%2F05%2F%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E7%9A%84%E5%AE%9A%E4%B9%89%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[sklearn与spark ml对于正负样本的定义有区别 sklearn横行为预测值，纵行为真实值 0为负样本、1为正样本,计算精确率、召回率，是计算类别为1的样本 预测值 0 1 真实值 0 x x 1 x x spark ml1为负样本、0为正样本,计算精确率、召回率，是计算类别为0的样本]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[特征重要性计算]]></title>
    <url>%2F2018%2F04%2F20%2F%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[特征重要性可以用来做模型可解释性，这在风控等领域是非常重要的方面。 xgboostxgboost实现中Booster类get_score方法输出特征重要性，其中importance_type参数支持三种特征重要性的计算方法 1.importance_type=weight（默认值），特征重要性使用特征在所有树中作为划分属性的次数 2.importance_type=gain，特征重要性使用特征在作为划分属性时loss平均的降低量 3.importance_type=cover，特征重要性使用特征在作为划分属性时对样本的覆盖度]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征重要性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT生成特征]]></title>
    <url>%2F2018%2F03%2F14%2FGBDT%E7%94%9F%E6%88%90%E7%89%B9%E5%BE%81%2F</url>
    <content type="text"><![CDATA[原理通过GDBT模型训练出很多棵树，每棵树的节点值是每个样本在这棵树上的预测值。同时，将样本在每棵树的节点的位置保存起来，比如[1,3,5]，表示样本在第一棵树节点位置为1，同理，在第二棵树的节点位置为3。对每个节点位置onehot编码，比如第一棵树共有3个节点，则1为001,同理，比如第二棵树有4个节点，则3为0010，假设第三棵树为5个节点，则[1,3,5]为[0 0 1 0 0 1 0 0 0 0 0 1] 代码123456789from sklearn.datasets import make_classificationfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.preprocessing import OneHotEncoderX, y = make_classification(n_samples=10) print yprint X.shapeX 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[0 1 1 0 0 1 1 1 0 0](10L, 20L)array([[-1.46218297e+00, -4.18333502e-01, -3.59274601e-02, 4.51595239e-01, -1.51047633e+00, 5.07112394e-01, 7.38097000e-01, -3.02315469e-01, 4.26175119e-01, 4.01702307e-02, -6.60368519e-01, -1.00253339e+00, 4.55802942e-01, -4.31784177e-01, 5.61925740e-02, -9.87095232e-01, -3.60589437e-01, -8.20339651e-02, -1.51267169e+00, -1.75602670e+00], [-1.38065657e-01, 1.51125229e+00, -1.16786498e+00, 8.96428602e-01, 2.24770779e-02, 2.75832881e-01, 8.84555986e-01, 1.90990870e-01, 1.50974562e+00, 6.32858976e-01, 7.43984848e-01, 6.23124869e-01, 2.75529340e-01, 4.13913270e-01, -5.69452199e-01, 9.92345363e-01, -6.58511532e-01, -1.04902254e+00, 4.17471392e-01, -1.69380460e+00], [ 7.94578797e-01, 6.91286961e-01, -8.36134614e-01, 1.35232255e+00, 8.19800516e-01, -6.63009090e-01, -3.93857189e-01, 1.21970975e+00, 9.45892797e-01, 8.78534488e-01, -6.82525652e-01, -4.65135487e-01, 7.91431577e-01, 3.54382290e-01, 8.96197192e-02, -8.50444786e-01, 5.54008693e-01, -2.22658663e-01, 2.60990234e-01, 5.36795887e-01], [ 2.35619886e-03, 2.00775837e-01, 4.87206937e-02, -4.86582074e-02, -1.93014354e+00, 3.94423848e-01, 8.43146268e-01, -1.18764902e+00, 1.21507109e+00, -5.82489166e-02, 2.91817058e-01, 2.63882786e-01, -8.09303986e-01, 2.13007401e+00, 2.03539722e+00, 2.50978548e-01, -5.79211447e-01, -2.27030861e+00, -1.32198712e+00, 3.58828134e-01], [ 1.76208221e-01, -6.44503388e-01, -1.35005640e-01, 2.57927723e-01, -1.15227919e+00, -1.04384378e+00, -5.25599357e-01, 2.16430369e+00, 1.14631323e+00, -4.86738371e-01, 3.56177943e-01, 4.25940193e-01, -2.46271932e-01, 8.02966687e-01, -9.16757374e-01, 3.92150341e-01, 8.89340812e-01, 8.89122664e-01, -6.91321300e-01, 3.60691085e-01], [-9.17667327e-01, -2.19718742e-01, 3.08231099e-01, -1.20865300e+00, 9.52723078e-02, -2.13289317e+00, 8.23448746e-01, 1.84966961e-01, 2.35626420e+00, -5.90061668e-01, -9.91453561e-01, 2.62985532e-01, -9.96616193e-01, -3.90777590e-01, 3.89293149e-01, -1.31349623e+00, 2.72401261e-02, -2.54414046e-01, -4.60378536e-01, 1.23080237e+00], [-8.28192448e-01, 1.15639869e+00, 2.78606672e-02, 9.65889005e-02, 1.68886757e+00, -3.25775102e-01, 1.75717716e-01, 1.59902401e-01, -6.55490597e-01, 2.95479360e-01, 6.60409198e-01, -2.70659454e+00, 7.21371599e-01, 5.61757431e-01, 1.22378486e+00, 9.99869289e-01, 2.21082939e+00, 1.31353776e-01, 1.64938642e+00, 3.81579172e-02], [-1.18357533e+00, 1.47944412e+00, 2.82847558e-01, 1.01023062e+00, 1.54087073e+00, 1.88003713e-02, -2.54038797e-01, -1.24068833e+00, -3.31394808e-02, 1.10512704e+00, -7.21359030e-01, 5.02536950e-01, 4.86264732e-01, -2.48485472e+00, 7.36671531e-01, -8.50743359e-01, 1.42773431e+00, -3.85276831e-01, 7.92621022e-01, 8.04302650e-01], [ 5.00298505e-01, 4.99864304e-01, -4.43128177e-01, -6.80877761e-01, -2.30634433e+00, -4.18106882e-01, -8.34288871e-01, 1.68655058e+00, -2.32944124e+00, -3.10646642e-01, -3.88910822e-01, -9.13217680e-01, 1.01294499e+00, 1.13638795e-01, 3.46336445e-01, -6.82351046e-01, -3.25986042e-01, -1.87458285e-01, -1.97646950e+00, -1.29381917e+00], [ 2.39415433e-01, 1.07290006e+00, 1.15550474e+00, -1.15201307e+00, -5.78108383e-01, -3.80187570e-01, -1.28861486e+00, -1.32918430e+00, -5.28881163e-01, 1.96556881e-01, -8.61984192e-01, -2.17046046e+00, -8.82487188e-02, 5.60887804e-01, 1.31724634e+00, -1.18909996e+00, -1.67108757e+00, 8.37225262e-01, -9.06706808e-01, -6.15740923e-01]]) 123456X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)gbc = GradientBoostingClassifier(n_estimators=2)one_hot = OneHotEncoder()gbc.fit(X_train, y_train)X_train_new = one_hot.fit_transform(gbc.apply(X_train)[:, :, 0])print (X_train_new.todense()) 12345[[0. 1. 0. 1.] [0. 1. 0. 1.] [1. 0. 1. 0.] [1. 0. 1. 0.] [1. 0. 1. 0.]]]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
      <tags>
        <tag>GBDT生成特征</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程简述]]></title>
    <url>%2F2018%2F03%2F07%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%AE%80%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1 数据变换 1.1 简单函数变换:将不正态分布的数据变为正态分布，如平方、根号、log和差分等，log和差分有时可以将时间序列非平稳序列转化为平稳序列 1.2 规范化:归一化、标准化、范数化、排序、小数定标化等 1.3 连续变量离散化:等值划分、等量划分、聚类划分 1.4 构造特征:找到特征之间的关系，构造和label相关性更强的特征 1.5 小波变换，提取特征产生新的向量 2 特征选择 2.1 策略特征选择本质上是一个特征组合优化的问题，主要分三种策略，分别是完全搜索、启发式搜索和随机搜索等。 2.1.1 完全搜索，如广度优先搜索。 2.1.2 启发式搜索，如序列前向选择、序列后向选择、增L去R选择和双向搜索等等。 2.1.3 随机搜索:随机产生序列选择算法，即随机产生一个特征子集，然后在这个子集上执行SFS(sequential forward selection)和SBS算法;模拟退火算法，每次以一定概率接受比当前解更差的解，这个概率会随着时间逐渐降低;遗传算法，通过交叉、突变等操作繁殖出下一代特征子集，评分越高的特征子集被选中繁殖的概率越大。缺点，随机搜索算法依赖随机因素，有实验结果难重现。 2.2 类型特种选择算法有三种类型，分别是嵌入式特征选择、过滤式特征选择和封装式特征选择。 2.2.1 嵌入式特征选择，如决策树 2.2.2 过滤式特征选择，主要有四类，距离度量、信息度量、关联度度量以及一致性度量，具体来说有相关系数法、卡方分布，互信息法和信息增益等。 2.2.3 封装式特征选择，如LR、RF和XGBoost等 3 构造特征 特征组合(交叉)A特征需要做离散化处理，然后通过字符串拼接B特征，然后再做OneHot编码 GDBT构造特征通过GDBT模型训练出很多棵树，每棵树的节点值是每个样本在这棵树上的预测值。同时，将样本在每棵树的节点的位置保存起来，比如[1,3,5]，表示样本在第一棵树节点位置为1，同理，在第二棵树的节点位置为3。对每个节点位置onehot编码，比如第一棵树共有3个节点，则1为001,同理，比如第二棵树有4个节点，则3为0010，假设第三棵树为5个节点，则[1,3,5]为[0 0 1 0 0 1 0 0 0 0 0 1] 4 其他 不同模型对数据的依赖度LR适合拟合离散特征，GDBT适合拟合连续特征 连续性特征离散化等值划分(按照值域平分区间)和等量划分(按照样本个数每个区间样本数量一样)两种。 树模型不用归一化，标准化 特征监控对于重要的特征需要进行监控，主要看特征最大最小值是否发生变化、特征稳定性是否发生明显变化、特征的对于样本的覆盖率等等 PCA算法比如时速和秒速度，PCA可以去掉。通过求解协方差矩阵，然后分解，选择前p个特征向量，则进行了降维 VC维理论维度越高，可容许的复杂度越高。但是，一，容易过拟合;二，特征数量增加带来的训练、测试和存储开销很大;三，有些基于距离的算法，如KNN/Kmeans等，会影响算法精度;四，可视化分析的需要，一般二维三维展示]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python文本处理]]></title>
    <url>%2F2018%2F03%2F05%2Fpython%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B9%8Bgensim%2F</url>
    <content type="text"><![CDATA[LDA参考123456789101112131415161718192021222324252627282930313233from gensim import corpora,similarities,models import jieba # 训练样本 raw_documents = [ '0南京江心洲污泥偷排”等污泥偷排或处置不当而造成的污染问题，不断被媒体曝光', '1面对美国金融危机冲击与国内经济增速下滑形势，中国政府在2008年11月初快速推出“4万亿”投资十项措施', '2全国大面积出现的雾霾，使解决我国环境质量恶化问题的紧迫性得到全社会的广泛关注', '3大约是1962年的夏天吧，潘文突然出现在我们居住的安宁巷中，她旁边走着40号王孃孃家的大儿子，一看就知道，他们是一对恋人。那时候，潘文梳着一条长长的独辫', '4坐落在美国科罗拉多州的小镇蒙特苏马有一座4200平方英尺(约合390平方米)的房子，该建筑外表上与普通民居毫无区别，但其内在构造却别有洞天', '5据英国《每日邮报》报道，美国威斯康辛州的非营利组织“占领麦迪逊建筑公司”(OMBuild)在华盛顿和俄勒冈州打造了99平方英尺(约9平方米)的迷你房屋', '6长沙市公安局官方微博@长沙警事发布消息称，3月14日上午10时15分许，长沙市开福区伍家岭沙湖桥菜市场内，两名摊贩因纠纷引发互殴，其中一人被对方砍死', '7乌克兰克里米亚就留在乌克兰还是加入俄罗斯举行全民公投，全部选票的统计结果表明，96.6%的选民赞成克里米亚加入俄罗斯，但未获得乌克兰和国际社会的普遍承认', '8京津冀的大气污染，造成了巨大的综合负面效应，显性的是空气污染、水质变差、交通拥堵、食品不安全等，隐性的是各种恶性疾病的患者增加，生存环境越来越差', '9 1954年2月19日，苏联最高苏维埃主席团，在“兄弟的乌克兰与俄罗斯结盟300周年之际”通过决议，将俄罗斯联邦的克里米亚州，划归乌克兰加盟共和国', '10北京市昌平区一航空训练基地，演练人员身穿训练服，从机舱逃生门滑降到地面', '11腾讯入股京东的公告如期而至，与三周前的传闻吻合。毫无疑问，仅仅是传闻阶段的“联姻”，已经改变了京东赴美上市的舆论氛围', '12国防部网站消息，3月8日凌晨，马来西亚航空公司MH370航班起飞后与地面失去联系，西安卫星测控中心在第一时间启动应急机制，配合地面搜救人员开展对失联航班的搜索救援行动', '13新华社昆明3月2日电，记者从昆明市政府新闻办获悉，昆明“3·01”事件事发现场证据表明，这是一起由新疆分裂势力一手策划组织的严重暴力恐怖事件', '14在即将召开的全国“两会”上，中国政府将提出2014年GDP增长7.5%左右、CPI通胀率控制在3.5%的目标', '15中共中央总书记、国家主席、中央军委主席习近平看望出席全国政协十二届二次会议的委员并参加分组讨论时强调，团结稳定是福，分裂动乱是祸。全国各族人民都要珍惜民族大团结的政治局面，都要坚决反对一切危害各民族大团结的言行' ] corpora_documents = [] #分词处理 for item_text in raw_documents: item_seg = list(jieba.cut(item_text)) corpora_documents.append(item_seg) # 生成字典和向量语料 dictionary = corpora.Dictionary(corpora_documents)print type(dictionary)print(dictionary) 12&lt;class 'gensim.corpora.dictionary.Dictionary'&gt;Dictionary(384 unique tokens: [u'\u8981', u'CPI', u'\u5931\u8054', u'\u901a\u8fc7', u'\u73af\u5883\u8d28\u91cf']...) 12345# 稀疏表达方式，实际上产生的是16*384的词频矩阵，16是文档数目，384是词语数目# 通过下面一句得到语料中每一篇文档对应的稀疏向量（这里是bow向量） corpus = [dictionary.doc2bow(text) for text in corpora_documents] # 向量的每一个元素代表了一个word在这篇文档中出现的次数 print(corpus) 1[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)], [(1, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1)], [(13, 3), (18, 1), (19, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1)], [(13, 3), (19, 5), (32, 1), (34, 1), (50, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 2), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 2), (87, 1), (88, 2), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1)], [(13, 2), (19, 2), (23, 1), (27, 1), (32, 2), (42, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1)], [(1, 1), (13, 2), (19, 1), (24, 1), (32, 1), (42, 1), (97, 2), (98, 2), (110, 1), (111, 1), (112, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1)], [(16, 1), (19, 4), (63, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 2)], [(13, 3), (19, 3), (57, 1), (79, 1), (103, 1), (134, 1), (184, 1), (185, 1), (186, 1), (187, 3), (188, 2), (189, 2), (190, 1), (191, 1), (192, 1), (193, 2), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1)], [(13, 5), (14, 1), (17, 1), (19, 4), (86, 2), (129, 1), (206, 1), (207, 3), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1)], [(1, 1), (13, 2), (19, 4), (24, 1), (27, 1), (32, 1), (34, 1), (45, 1), (124, 1), (136, 1), (171, 1), (173, 1), (187, 2), (188, 1), (189, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1)], [(19, 2), (149, 1), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 1), (260, 2), (261, 1), (262, 1)], [(1, 1), (13, 4), (19, 3), (21, 1), (24, 1), (27, 1), (65, 1), (86, 1), (129, 1), (263, 1), (264, 1), (265, 2), (266, 1), (267, 2), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1), (280, 1), (281, 1)], [(13, 1), (19, 4), (27, 1), (32, 1), (63, 1), (171, 1), (173, 1), (176, 1), (206, 1), (248, 1), (252, 2), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), (295, 1), (296, 1), (297, 1), (298, 1), (299, 1), (300, 1), (301, 2), (302, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1)], [(1, 1), (13, 1), (19, 3), (24, 1), (45, 1), (63, 2), (143, 1), (173, 1), (249, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1), (313, 1), (314, 1), (315, 1), (316, 1), (317, 1), (318, 1), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (324, 2), (325, 1), (326, 1), (327, 1), (328, 1), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1), (334, 1)], [(1, 1), (13, 2), (19, 1), (24, 1), (28, 1), (32, 2), (34, 1), (48, 1), (102, 1), (150, 1), (207, 1), (243, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 1), (340, 1), (341, 1), (342, 1), (343, 1), (344, 1), (345, 1), (346, 1), (347, 1), (348, 1)], [(13, 3), (19, 3), (48, 1), (65, 1), (86, 2), (151, 1), (172, 1), (207, 2), (316, 1), (349, 1), (350, 1), (351, 1), (352, 2), (353, 1), (354, 1), (355, 1), (356, 1), (357, 1), (358, 1), (359, 1), (360, 1), (361, 1), (362, 1), (363, 1), (364, 1), (365, 1), (366, 1), (367, 1), (368, 1), (369, 2), (370, 1), (371, 1), (372, 1), (373, 1), (374, 1), (375, 2), (376, 1), (377, 1), (378, 1), (379, 1), (380, 1), (381, 2), (382, 1), (383, 2)]] 1234# corpus是一个返回bow向量的迭代器。下面代码将完成对corpus中出现的每一个特征的IDF值的统计工作 tfidf_model = models.TfidfModel(corpus) corpus_tfidf = tfidf_model[corpus] corpus_tfidf 1&lt;gensim.interfaces.TransformedCorpus at 0xaa15eb8&gt; 12345678similarity = similarities.Similarity('Similarity-tfidf-index', corpus_tfidf, num_features=600) test_data_1 = '北京雾霾红色预警' test_cut_raw_1 = list(jieba.cut(test_data_1)) # ['北京', '雾', '霾', '红色', '预警'] test_corpus_1 = dictionary.doc2bow(test_cut_raw_1) # [(51, 1), (59, 1)]，即在字典的56和60的地方出现重复的字段，这个值可能会变化 similarity.num_best = 5 test_corpus_tfidf_1=tfidf_model[test_corpus_1] # 根据之前训练生成的model，生成query的IFIDF值，然后进行相似度计算 # [(51, 0.7071067811865475), (59, 0.7071067811865475)] print(similarity[test_corpus_tfidf_1]) # 返回最相似的样本材料,(index_of_document, similarity) tuples 1[(2, 0.3595932722091675)] 123456test_data_2 = '长沙街头发生砍人事件致6人死亡' test_cut_raw_2 = list(jieba.cut(test_data_2)) test_corpus_2 = dictionary.doc2bow(test_cut_raw_2) test_corpus_tfidf_2=tfidf_model[test_corpus_2] similarity.num_best = 3 print(similarity[test_corpus_tfidf_2]) # 返回最相似的样本材料,(index_of_document, similarity) tuples 1[(6, 0.19451555609703064), (13, 0.10124017298221588)]]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>gensim</tag>
        <tag>jieba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark DataFrame 常用函数及常见问题]]></title>
    <url>%2F2018%2F02%2F26%2FSpark%20DataFrame%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[函数123456# python:df['age'].value_counts()df.groupBy("age").count().show()# python:df.describe()df.describe().show()# python:df.fillna(df.mean())# spark:https://stackoverflow.com/questions/40057563/replace-missing-values-with-mean-spark-dataframe 问题 parquet格式错误 12//需注意Hive版本需要低于2.1.0SparkSession().config("spark.sql.hive.convertMetastoreParquet","false") 列字段存在但找不到 1SparkSession().config("spark.sql.parquet.filterPushdown", "false") 表字段过多不显示 SparkSession().config(&quot;spark.debug.maxToStringFields&quot;, &quot;100&quot;) spark ml算法管道predict产生的df字段，其中label对应predictedLabel(int类型)，indexedLabel对应prediction(double类型) spark ml模型评估，和sklearn不同，需要将我们关注的样本正样本定义为0，负样本为1，这样在计算精确率和召回率时才正确，一般所说的正样本和sklearn一样（正样本为1，负样本为0）。 spark 批量读取DataFrame文件时，有时候会出错，此时可以一个个文件读入，然后合并，即使这样还不行，那也找到了出现问题的文件在哪里。 命令行spark-shell –conf spark.sql.hive.convertMetastoreParquet=false 命令行启动]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘项目经验总结]]></title>
    <url>%2F2018%2F02%2F03%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E9%A1%B9%E7%9B%AE%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[阿里音乐流行趋势预测赛题描述，对一定数量的歌手(比如1000个)，根据历史播放量等信息(比如最近6个月的每天播放量等信息)，预测每个歌手未来2个月每天的播放量 可以使用某种方式对歌手进行分类，然后分别对分类群体，画出目标变量播放量的趋势图，摸索出不同的分类群体的变化规律 画出播放总量趋势图，发现在假期和周末时数据不一样，此时可以通过修正系数修正数据 规则算法 进一步分析变化规律，看看有几种规律，判断哪些规律可预测，哪些规律不可预测 根据第一步的分析结果人工定义特征，然后寻找参数，构造线性函数，函数实际上拟合的是非线性问题(通过人工定义特征转化线性问题) a,可以取前60日中位数，前7日均值，前14日中位数，前30/60日最小值等 特征b，当天前3日播放量的均值相对于前60日最低值的增长量。经分析，当天前3日播放量的均值相对于前60日最低值的增长量不近似直线，不好回归，使用b1=后30天播放量的均值/前7天播放量的均值 特征c对应特征，当天播放量，即总体播放量的增长量 公式还得加上对每周周末的周期变化和节假日的修正系数 特征b主要起的作用是分类（在本案例中，将这50名歌手分为两类，来做预测，每一类最终得到的预测函数是一样的），通过设置不同的b的阈值来划分类别 当b取某值比如0.96，小于此值为1类，大于此值为另一个类，这样数据就有标签了。然后对待预测数据，不使用此规则来打标签，而是使用GDBT基于刚才产生的有标签数据来预测类别。最后将用户分为两类，然后使用线性回归求出a,b,c对应的系数，这样就得到了线性回归模型，就可以预测歌手未来两个月每天的播放量 将b设置为另外一个值，又到了一个线性回归模型和结果 ··· 最终将这几个线性回归模型，使用加权法来融合模型]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>项目经验总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将jar包打包到集群运行]]></title>
    <url>%2F2018%2F01%2F06%2F%E5%B0%86jar%E5%8C%85%E6%89%93%E5%8C%85%E5%88%B0%E9%9B%86%E7%BE%A4%E8%BF%90%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[规范化，一般有4个文件夹，分别为bin、conf、jars和logs 运行 ./run.sh，不能直接使用run.sh，需指定当前目录 bin job-env.sh 12SPARK_HOME=/opt/cloudera/parcels/sparkHADOOP_CONF_DIR=/etc/hadoop/conf run.sh 1234567891011121314151617181920212223#!/usr/bin/env bashsource /etc/profileif [ -z "$&#123;JOB_HOME&#125;" ]; then export JOB_HOME="$(cd "`dirname "$0"`"/; pwd)"ficd $JOB_HOMEcd ..source bin/job-env.shexport HADOOP_CONF_DIR=/etc/hadoop/confnohup $SPARK_HOME/bin/spark-submit --master yarn \--deploy-mode client \--conf spark.yarn.maxAppAttempts=1 \--conf spark.default.parallelism=30 \--num-executors 3 \--executor-memory 1G \--driver-memory 1G \--executor-cores 1 \--driver-class-path /etc/hive/conf \--files /etc/hive/conf/hive-site.xml,/etc/hadoop/conf/hdfs-site.xml,/etc/hadoop/conf/core-site.xml,conf/log4j.properties \--class ScgdUserLoss \jars/1-1.0-SNAPSHOT.jar \ &gt; logs/nohup.out 2&gt;&amp;1 &amp; conflog4j.properties，默认为 12345678910111213141516171819202122232425262728293031323334353637383940## Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the "License"); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.## Set everything to be logged to the consolelog4j.rootCategory=INFO, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n# Set the default spark-shell log level to WARN. When running the spark-shell, the# log level for this class is used to overwrite the root logger's log level, so that# the user can have different defaults for the shell and regular Spark apps.log4j.logger.org.apache.spark.repl.Main=WARN# Settings to quiet third party logs that are too verboselog4j.logger.org.spark_project.jetty=WARNlog4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERRORlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFOlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFOlog4j.logger.org.apache.parquet=ERRORlog4j.logger.parquet=ERROR# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive supportlog4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATALlog4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR jars自己提交的jar包 logs输出日志]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python连接hive/mysql]]></title>
    <url>%2F2017%2F12%2F14%2Fpython%E8%BF%9E%E6%8E%A5hive-mysql%2F</url>
    <content type="text"><![CDATA[注意 windows环境，sasl安装不上，报各种错误解决办法：在whl库中，找到所需要的whl中，安装即可 whl文件是什么？whl格式本质上是一个压缩包，里面包含了py文件，以及经过编译的pyd文件。使得可以在不具备编译环境的情况下，选择合适自己的python环境进行安装。 首先集群需要启动hiveserver2 1hive --service hiveserver2 ip映射，可以在xshell对应集群窗口属性中设置隧道，访问某个ip的端口映射到访问本地ip某个端口 hive 12345678910import pandas as pdimport pymysqlfrom hdfs3 import HDFileSystemfrom pyhive import hivecursor = hive.connect("192.168.90.44",port = 10000).cursor()cursor.execute('select day ,mediaid,aid ,clickthrough from ad.terminalclick_log where day between "20180409" and "20180523" ')columns = ["day","mediaid","aid","clickthrougt"]data = pd.DataFrame(data=cursor.fetchall(),columns=columns)data.head() day mediaid aid clickthrougt 0 20180409 agltb3B1Yi1 id1 starcorcom 1 20180409 agltb3B1Yi1 id1 starcorcom 2 20180409 agltb3B1Yi1 id1 starcorcom 3 20180409 agltb3B1Yi1 id1 starcorcom 4 20180409 agltb3B1Yi1 id1 starcorcom mysql123456conn= pymysql.connect(host="192.168.90.44",user="root", password="starcor",db="test",port=3306,charset='utf8')# 创建游标cursor = conn.cursor()cursor.execute("select * from item_index")item_index_df=pd.DataFrame(data=list(cursor.fetchall()))item_index_df.head() 0 1 2 3 4 5 6 7 8 0 (22-23) 0.833254 0.166746 0.131273 0.245796 0.281922 0.162764 0.11543 0.0629 1 (26-27) 0.83121 0.16879 0.113689 0.233952 0.280988 0.187385 0.0982592 0.0857 2 (28-29) 0.766236 0.233764 0.105216 0.228691 0.280427 0.235434 0.104194 0.0459 3 -14 0.828478 0.171522 0.0940754 0.146784 0.28686 0.258316 0.116409 0.0977 4 007大战皇家赌场 0.709068 0.290932 0.279817 0.150034 0.180251 0.204246 0.103626 0.0822]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
      <tags>
        <tag>python连接hive/mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[提交本地项目到github及更新项目]]></title>
    <url>%2F2017%2F11%2F26%2F%E6%8F%90%E4%BA%A4%E6%9C%AC%E5%9C%B0%E9%A1%B9%E7%9B%AE%E5%88%B0github%E5%8F%8A%E6%9B%B4%E6%96%B0%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[github在线上传通过git工具 上传 cd 项目文件夹 git init git add . git commit -m update github主页创建同名项目，不要勾选add README,并复制仓库地址 git remote add origin https://github.com/dcexist/study_4_userLoss.git git push -u origin master 更新 cd 项目文件夹123git add .git commit -m updategit push origin master]]></content>
      <categories>
        <category>github</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2017%2F11%2F23%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux命令 打包 1tar cvf data0.tar data0/ 解压 1tar xvf data0.tar ../data1 集群配置文件夹 1cd /etc/hive/conf 查看集群其他主机名和对应ip 1cat /etc/hosts 查找指定进程是否在运行 1ps aux | grep hiveser]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Shell命令]]></title>
    <url>%2F2017%2F11%2F13%2FHadoop%20Shell%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Hadoop命令 查询目录文件大小 1hdfs dfs -du -h / 查询某个文件夹大小 1hdfs dfs -dus -h / 从hdfs取文件到linux本地 1hdfs dfs -get /user/portrait/scgd/data0 ./output/ 上传文件到hdfs上 1hdfs dfs -put ./output/ /user/portrait/scgd/data0]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
      <tags>
        <tag>Hadoop命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地IDEA连接集群]]></title>
    <url>%2F2017%2F11%2F05%2F%E6%9C%AC%E5%9C%B0IDEA%E8%BF%9E%E6%8E%A5%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[读取hdfs数据 建立resource文件夹,将集群中 /etc/hive/conf 中的xml文件复制到这个文件夹,并将该文件夹设置为resource属性，同时将代码文件夹设置为source属性 将集群 /etc/hosts文件中的映射关系添加到c:/System32/drivers/etc/hosts文件 spark.read.format(“csv”).load(“/user/test/test2.csv”).show() 读取hive表数据 除了上述之外，还需配置pom.xml的依赖，比如spark-sql/spark-hive/tez引擎等 spark.sql(“select * from starcor_test.play_log where day=20180509 limit 20”).show() #表名需要是库名.表名 读取mysql表数据加入mysql-connector···依赖 val jdbcDF = spark.read .format(&quot;jdbc&quot;) .option(&quot;url&quot;, &quot;jdbc:mysql://master02:3306&quot;) .option(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;) .option(&quot;dbtable&quot;, &quot;test.item_index&quot;) .option(&quot;user&quot;, &quot;root&quot;) .option(&quot;password&quot;, &quot;starcor&quot;) .load() 本地文件 spark.read.format(“csv”).load(“file:/c:/user/test/test2.csv”).show()]]></content>
      <categories>
        <category>Hadoop/Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sql 常用操作]]></title>
    <url>%2F2017%2F10%2F20%2Fsql-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[用户行为记录表，计算相邻两次的差值(比如时间戳之差，作为用户观看时间) 1234spark.sql("select user_id,video_type,server_time,row_number() over(partition by user_id order by server_time) rank from playlog ").createOrReplaceTempView("a")spark.sql("select user_id,video_type,server_time,rank-1 as rank from a order by user_id,server_time").createOrReplaceTempView("b")spark.sql("select a.user_id,a.video_type,( b.server_time-a.server_time) watch_time from a left join b on a.user_id=b.user_id and a.rank=b.rank order by a.server_time").createOrReplaceTempView("c")spark.sql("select user_id,video_type, watch_time/60000.0 watch_time from c where watch_time is not null").createOrReplaceTempView("d")]]></content>
  </entry>
  <entry>
    <title><![CDATA[pandas 常用函数]]></title>
    <url>%2F2017%2F10%2F17%2Fpandas-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[保存文件,含header1data.to_csv('F:\\MyDocuments\\starcor\\one_day.csv',header=True,index=False,encoding='utf-8') 读取文件，含header 1data=pd.read_csv('F:\\MyDocuments\\starcor\\one_day.csv',encoding='utf-8') 用户行为记录，按照用户分组，计算分组后server_time.max-server_time.min 123456index user_id client_type video_id video_name playbill_length server_time0 8230003234837184 stb d216440bff6dadf9813b067d6c42bdc0 御姐归来 0.0 1.529424e+121 8230004001768545 stb 34acd1a54645f3a9761173ddc62154f0 宝宝巴士儿歌 0.0 1.529424e+122 8230004001768545 stb 34acd1a54645f3a9761173ddc62154f0 宝宝巴士儿歌 0.0 1.529424e+123 8230004001768545 stb 34acd1a54645f3a9761173ddc62154f0 宝宝巴士儿歌 0.0 1.529424e+124 8230002289053523 stb 5b1fa285380f7f1fbf3d68665be05d24 女医明妃传 DVD版 0.0 1.529424e+12 123temp=data.groupby('user_id',as_index=False)['server_time'].agg(&#123;'server_time_max':max,'server_time_min':min&#125;)temp['playbill_length1']=temp.server_time_max-temp.server_time_mintemp.head() 123456index user_id server_time_min server_time_max playbill_length10 8230002560236474 1.529508e+12 1.529510e+12 2773122.01 8230002560283518 1.529508e+12 1.529510e+12 1383261.02 8230002560346901 1.529508e+12 1.529510e+12 2516150.03 8230002560367956 1.529510e+12 1.529510e+12 1326.04 8230002560463268 1.529509e+12 1.529509e+12 1832.0 DataFrame有些字段，比如影片类型特征列，往往不止一个类型，有些情况下需要将影片类型拆分并新建特征列(类型1，类型2) 1234# 提取多值标签-影片类型len=data2.evs_content_type.map(lambda x:len(str(x).split('/'))).max()for i in range(len): data2['film_category'+str(i)]=data2['evs_content_type'].map(lambda x:str(str(x).split('/')[i]) if len(str(x).split('/')) &gt; i else '') 用户行为记录，记录用户相邻两次操作时间差 1234567891011121314151617181920# 原始表 user_id timestamp0 8230003138952154 01 8230003138952154 12 8230003138952154 53 8230003138952154 94 8230003138952154 95 8230003138952154 106 8230003138952154 117 8230003138952154 158 8230004134835377 79 9950000002391827 510 9950000002391827 611 9950000002391827 912 9950000002391827 1213 9950000002391827 1414 9950000002391827 1415 9950000002391827 1516 9950000002391827 1617 9950000002391827 17 12345df['pre'] = df['timestamp'].shift(1)df['uid'] = df['user_id'].shift(1)df['interval'] = (df['timestamp'] - df['pre'])df1 = df[(df['user_id'] == df['uid'])]df1[['user_id','timestamp','interval']] 1234567891011121314151617# 输出 user_id server_time_min interval1 8230003138952154 1 1.02 8230003138952154 5 4.03 8230003138952154 9 4.04 8230003138952154 9 0.05 8230003138952154 10 1.06 8230003138952154 11 1.07 8230003138952154 15 4.010 9950000002391827 6 1.011 9950000002391827 9 3.012 9950000002391827 12 3.013 9950000002391827 14 2.014 9950000002391827 14 0.015 9950000002391827 15 1.016 9950000002391827 16 1.017 9950000002391827 17 1.0]]></content>
      <categories>
        <category>pandas</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jupyter notebook 隐藏代码]]></title>
    <url>%2F2017%2F10%2F16%2Fjupyter-notebook-%E9%9A%90%E8%97%8F%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[代码如下：123456789101112131415from IPython.display import HTMLHTML('''&lt;script&gt;code_show=true; function code_toggle() &#123; if (code_show)&#123; $('div.input').hide(); &#125; else &#123; $('div.input').show(); &#125; code_show = !code_show&#125; $( document ).ready(code_toggle);&lt;/script&gt;&lt;form action="javascript:code_toggle()"&gt;&lt;input type="submit" value="Click here to toggle on/off the raw code."&gt;&lt;/form&gt;''')]]></content>
  </entry>
  <entry>
    <title><![CDATA[makedown 表格自动生成]]></title>
    <url>%2F2017%2F10%2F15%2Fmakedown-%E8%A1%A8%E6%A0%BC%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%2F</url>
    <content type="text"><![CDATA[参考链接1234567891011exceltk用例整个表格： exceltk.exe -t md -xls xxx.xls exceltk.exe -t md -xls xxx.xlsx指定sheet： exceltk.exe -t md -xls xx.xls -sheet sheetname exceltk.exe -t md -xls xx.xlsx -sheet sheetnameexceltk特性： 转换Excel表格到MarkDown表格 支持Excel单元格带超链接 如果Excel里有合并的跨行单元格，在转换后的MarkDown里是分开的单元格，这是因为MarkDown本身不支持跨行单元格 如果Excel表格右侧有大量的空列，则会被自动裁剪，算法是根据前100行来检测并计算 exceltk下载地址 1./exceltk.exe -t md -xls 1.xlsx 注意事项在执行命令时，需关闭要处理的xls文件窗口，不能在Excel打开]]></content>
      <categories>
        <category>makedown</category>
      </categories>
      <tags>
        <tag>makedown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最小值寻优算法推导]]></title>
    <url>%2F2017%2F10%2F14%2F%E6%9C%80%E5%B0%8F%E5%80%BC%E5%AF%BB%E4%BC%98%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[梯度下降法+最小二乘法+牛顿法+拟牛顿法 推导]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[L1&L2正则化介绍及推导]]></title>
    <url>%2F2017%2F10%2F13%2FL1-L2%E6%AD%A3%E5%88%99%E5%8C%96%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[介绍L1范数，表示向量X中元素的绝对值之和；L2范数，表示向量X中元素平方的和然后开根号。 推导]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[XGBoost原理及推导]]></title>
    <url>%2F2017%2F10%2F12%2FXGBoost%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[原理利用前向分布算法，在优化目标函数的时候，泰勒展开到二阶，然后优化求解。 推导图片拖到新标签页，可仔细查看图片。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost&GDBT原理及推导]]></title>
    <url>%2F2017%2F10%2F10%2FAdaBoost-GDBT%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[原理 AdaBoost对于存在强依赖关系的弱学习器，需要串行生成。先从初试训练集训练一个学习器，然后根据该学习器的表现对训练样本的分布进行调整，对于先前学习器犯错的样本给予更多的关注，继续生成学习器，重复上述步骤，直至训练了指定数目的学习器，然后将这些学习器进行加权结合，每个权重代表的是其对应分类器在上一轮迭代中的成功度。 GDBT是一种Boosting算法，但又与传统的boost算法思路不同，每次迭代不是对犯错的样本施以更高的权重。回归树在使用前项分步算法迭代中，下一轮是拟合上一轮的模型的负梯度在上一轮模型的值。 推导图片拖到新标签页，可仔细查看图片。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[决策树原理及推导]]></title>
    <url>%2F2017%2F10%2F09%2F%E5%86%B3%E7%AD%96%E6%A0%91%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[原理主要是利用树的形式，每次按照一定的准则选择特征，然后根据特征划分子树或者叶结点，直到满足停止条件位置。 推导图片拖到新标签页，可仔细查看图片。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯原理及推导]]></title>
    <url>%2F2017%2F10%2F08%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[原理判断某个记录属于哪一类，可以认为在该记录出现的情况下哪种类别概率最大，最大的就属于该类（比如判断一个黑人来自哪里，可以认为在黑人记录出现的情况下非洲概率最大，会判断他来自非洲）。但这个问题有时候不好直接得到答案P(C=0|X)，利用朴素贝叶斯的话就是求（P(C=0，X)）除以（训练集中该记录出现的概率强P(X)），分母一般不需要计算，分子用条件概率就可以求得，分子，即P(C=0，X)=P(X|C=0)P(C=0)=P(X1|C=0)*P(X2|C=0)P(C=0) 推导图片拖到新标签页，可仔细查看图片。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SVM原理及推导]]></title>
    <url>%2F2017%2F10%2F07%2FSVM%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[原理寻找一个超平面，使得距离超平面最近的点与超平面间隔最大化，然后通过对其对偶问题的求解求得模型参数。 推导图片拖到新标签页，可仔细查看图片。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LR原理及推导]]></title>
    <url>%2F2017%2F10%2F06%2FLR%E5%8E%9F%E7%90%86%E5%8F%8A%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[原理利用sigmoid函数对线性回归预测的结果进行映射，规定当sigmoid自变量&gt;0时，输出y为1代表正例，当sigmoid变量&lt;0时，输出y为0代表负例，利用极大似然法求解模型参数（极大似然法公式中也代表着使预测为正的概率最大和预测为负的概率最大）。 推导图片拖到新标签页，可仔细查看图片。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
</search>
